from typing import List, Literal, Tuple, Any

import nltk
import numpy as np
import onnxruntime as ort
from huggingface_hub import hf_hub_download
from importlib_resources import files
from transformers import AutoTokenizer

from subtitle_splitter_fr.preprocessing_utils import split_text_into_elements
from subtitle_splitter_fr.splitting_strategies import merge_strat, split_rec_strat, hybride_strat, cutting_into_2_line

import logging

logger = logging.getLogger(__name__)

# --- Configuration Hugging Face ---
HF_REPO_ID = "JulesGo/subtitle-splitter-fr"
HF_MODEL_FILENAME = "camembert_regression.onnx"

def get_onnx_model_path():
    """
    Downloads the ONNX model from Hugging Face Hub if it's not already cached locally.
    Returns the local path to the model file.
    """
    try:
        # hf_hub_download handles downloading and caching
        model_path = hf_hub_download(repo_id=HF_REPO_ID, filename=HF_MODEL_FILENAME)
        print(f"ONNX model available locally at: {model_path}")
        return model_path
    except Exception as e:
        print(f"Error downloading model from Hugging Face Hub: {e}")
        raise # Re-raise the exception if the download fails


class Splitter:
    _MODEL_SUBPATH = "models/camembert_regression.onnx"
    _TOKENIZER_SUBPATH = "models/tokenizer"
    SEP_TOKEN_NAME = "<sep_secable>"
    MAX_LENGTH = 512

    def __init__(self):
        try:
            nltk.data.find('tokenizers/punkt_tab')
        except Exception as e0:
            logger.info("NLTK 'punkt_tab' resource not found, attempting to download.")
            try:
                nltk.download('punkt_tab', quiet=True)
            except Exception as e:
                logger.error(f"Failed to download 'punkt_tab': {e}. Sentence tokenization might be affected.")

        providers = ['CPUExecutionProvider']

        try:
            model_local_path = get_onnx_model_path()

            try:
                self.session = ort.InferenceSession(str(model_local_path), providers=providers)
            except Exception as e:
                raise RuntimeError(f"Erreur lors du chargement du modÃ¨le ONNX depuis {model_local_path}: {e}") from e
            tokenizer_path_obj = files("subtitle_splitter_fr").joinpath(self._TOKENIZER_SUBPATH)

            if not tokenizer_path_obj.is_dir():
                logger.error(f"Tokenizer directory not found or invalid: {tokenizer_path_obj}")
                raise FileNotFoundError(f"Tokenizer directory not found or invalid: {tokenizer_path_obj}")

            self.tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path_obj))
            self.SEP_TOKEN_ID = self.tokenizer.convert_tokens_to_ids(self.SEP_TOKEN_NAME)

            if self.SEP_TOKEN_ID == self.tokenizer.unk_token_id:
                logger.warning(
                    f"Separator token '{self.SEP_TOKEN_NAME}' is unknown to the tokenizer. "
                    "This might lead to unexpected behavior."
                )

        except FileNotFoundError as e:
            raise
        except Exception as e:
            logger.error(f"Unexpected error during model or tokenizer loading: {e}")
            raise

    def split(self, text: str, length: int = 15, method: Literal["MERGE", "SPLIT", "HYBRIDE"] = "MERGE") -> List[str]:
        if not text or text.isspace():
            return []

        try:
            elements_scores_predicts = self._predict_scores(text)
        except Exception as e:
            logger.error(f"Error predicting scores for text: '{text[:50]}...'. Error: {e}")
            return [text]

        if not elements_scores_predicts:
            return [text] if text.strip() else []

        min_chars = max(7, int(length * 2 / 3))
        max_chars = 2 * min_chars

        try:
            if method == "MERGE":
                sub_titles = merge_strat(
                    elements_scores_predicts,
                    min_chars=min_chars,
                    max_chars=max_chars,
                )
            elif method == "SPLIT":
                sub_titles = split_rec_strat(
                    elements_scores_predicts,
                    min_chars=min_chars,
                    max_chars=max_chars,
                )
            elif method == "HYBRIDE":
                sub_titles = hybride_strat(
                    elements_scores_predicts,
                    min_chars=min_chars,
                    max_chars=max_chars,
                )
            else:
                logger.error(f"Unknown splitting method: {method}. Returning original text.")
                return [text]
        except Exception as e:
            logger.error(f"Error during splitting method '{method}': {e}. Returning original text.")
            return [text]

        return sub_titles
    def split_two_line(self, text: str, length: int = 15) -> list[str] | list[Any] | list[list[str]]:
        if not text or text.isspace():
            return []

        try:
            elements_scores_predicts = self._predict_scores(text)
        except Exception as e:
            logger.error(f"Error predicting scores for text: '{text[:50]}...'. Error: {e}")
            return [text]

        if not elements_scores_predicts:
            return [text] if text.strip() else []

        min_chars = max(7, int(length * 2 / 3))
        max_chars = 2 * min_chars

        sub_titles = cutting_into_2_line(
            elements_scores_predicts,
            min_chars_by_line=min_chars,
            max_chars_by_line=max_chars,
        )

        return sub_titles

    def _predict_scores(self, text: str) -> List[Tuple[str, float]]:
        try:
            sentences_proprocesses = preprocess_inference(text, self.tokenizer, self.MAX_LENGTH, self.SEP_TOKEN_ID,
                                                          split_text_into_elements)
        except Exception as e:
            logger.error(f"Error during preprocessing in _predict_scores for text: '{text[:50]}...'. Error: {e}")
            return []

        elements_with_scores_tot: List[Tuple[str, float]] = []

        if not sentences_proprocesses:
            return []

        for inputs, elements, sep_indices in sentences_proprocesses:
            if inputs is None or not elements:
                continue

            try:
                ort_outputs = self.session.run(["logits"], inputs)
                logits = ort_outputs[0]
            except Exception as e:
                logger.error(f"Error during ONNX inference: {e}")
                continue

            scores_numpy = 1 / (1 + np.exp(-logits))
            all_token_scores = scores_numpy.squeeze()

            if all_token_scores.ndim == 0:
                all_token_scores = np.array([all_token_scores.item()])
            elif all_token_scores.ndim > 1:
                logger.error(f"Unexpected shape for all_token_scores: {all_token_scores.shape}. Expected 1D or 0D.")
                continue

            predicted_scores = []
            if len(all_token_scores) > 0:
                for index in sep_indices:
                    if index < len(all_token_scores):
                        predicted_scores.append(all_token_scores[index])
                    else:
                        pass

            num_scores = len(predicted_scores)
            elements_with_scores_tot.extend(list(zip(elements[:num_scores], predicted_scores)))

        return elements_with_scores_tot


def preprocess_inference(text: str, tokenizer, max_length: int, sep_token_id: int, split_text_into_elements_func) -> \
List[Tuple[dict, List[str], List[int]]]:
    if not text or text.isspace():
        return []

    try:
        sentences = nltk.sent_tokenize(text)
    except Exception as e:
        logger.error(f"Error tokenizing sentences with NLTK for text: '{text[:50]}...'. Error: {e}")
        return []

    sentences_preprocess: List[Tuple[dict, List[str], List[int]]] = []
    sentences_elements: List[Tuple[List[int], List[str]]] = []

    for sentence in sentences:
        try:
            elements = split_text_into_elements_func(sentence)
        except Exception as e:
            logger.error(f"Error splitting elements for sentence: '{sentence[:50]}...'. Error: {e}")
            continue

        input_ids_sentence: List[int] = []
        elements_sentence: List[str] = []

        for element in elements:
            try:
                element_ids = tokenizer.encode(element, add_special_tokens=False)
            except Exception as e:
                logger.error(f"Error encoding element: '{element}'. Error: {e}")
                continue

            if input_ids_sentence and (len(input_ids_sentence) + len(element_ids) + 1 > max_length - 2):
                if input_ids_sentence:
                    sentences_elements.append((input_ids_sentence, elements_sentence))

                input_ids_sentence = []
                elements_sentence = []

            input_ids_sentence.extend(element_ids + [sep_token_id])
            elements_sentence.append(element)

        if input_ids_sentence:
            sentences_elements.append((input_ids_sentence, elements_sentence))

    if not sentences_elements:
        return []

    input_ids_current: List[int] = [tokenizer.cls_token_id]
    elements_currents: List[str] = []

    for input_ids_chunk, elements_chunk in sentences_elements:
        if input_ids_current and (len(input_ids_current) + len(input_ids_chunk) + 1 > max_length):
            if len(input_ids_current) > 1:
                input_ids_current.append(tokenizer.sep_token_id)

                padding_length = max_length - len(input_ids_current)
                attention_mask = [1] * len(input_ids_current)
                if padding_length < 0:
                    logger.error(
                        f"Negative padding calculated ({padding_length}) for batch. Current length: {len(input_ids_current)}, max_length: {max_length}")
                    padding_length = 0

                if padding_length > 0:
                    input_ids_current.extend([tokenizer.pad_token_id] * padding_length)
                    attention_mask.extend([0] * padding_length)

                input_ids_current = input_ids_current[:max_length]
                attention_mask = attention_mask[:max_length]

                if elements_currents or len(input_ids_current) > 2:
                    sentences_preprocess.append((
                        {
                            "input_ids": np.array([input_ids_current], dtype=np.int64),
                            "attention_mask": np.array([attention_mask], dtype=np.int64),
                        },
                        elements_currents,
                        [i for i, x in enumerate(input_ids_current) if x == sep_token_id],
                    ))

            input_ids_current = [tokenizer.cls_token_id] + input_ids_chunk
            elements_currents = elements_chunk
        else:
            input_ids_current.extend(input_ids_chunk)
            elements_currents.extend(elements_chunk)

    if len(input_ids_current) > 1:
        input_ids_current.append(tokenizer.sep_token_id)

        padding_length = max_length - len(input_ids_current)
        attention_mask = [1] * len(input_ids_current)
        if padding_length < 0:
            logger.error(
                f"Negative padding calculated ({padding_length}) for the last batch. Length: {len(input_ids_current)}, max_length: {max_length}")
            padding_length = 0

        if padding_length > 0:
            input_ids_current.extend([tokenizer.pad_token_id] * padding_length)
            attention_mask.extend([0] * padding_length)

        input_ids_current = input_ids_current[:max_length]
        attention_mask = attention_mask[:max_length]

        if elements_currents or len(input_ids_current) > 2:
            sentences_preprocess.append((
                {
                    "input_ids": np.array([input_ids_current], dtype=np.int64),
                    "attention_mask": np.array([attention_mask], dtype=np.int64)
                },
                elements_currents,
                [i for i, x in enumerate(input_ids_current) if x == sep_token_id],
            ))

    return sentences_preprocess

if __name__ == "__main__":
    splitter = Splitter()
    t="""Les Ã©vÃ©nements historiques de la grande chasse aux sorciÃ¨res ont toujours Ã©tÃ© mystÃ©rieux. La sorcellerie fascine depuis lâAntiquitÃ©, comme tÃ©moigne HÃ©cate, dÃ©esse de la sorcellerie dans la mythologie grecque. La grande Chasse est difficile Ã  apprÃ©hender pour nos intelligences rationalistes. Elle pose beaucoup de questions, dâabord, sur les raisons dâune telle tuerie, mais aussi sur son ampleur tant sur le plan de lâintensitÃ© que de la gÃ©ographie, sur lâidentitÃ© de ceux qui furent poursuivis et de ceux qui les jugÃ¨rent, et enfin sur son Ã©volution et sa fin aussi soudaine que son commencement. R. Mandrou, R. Muchembled, B.P. Levack, et G. Bechtel ont tous portÃ© un regard nouveau sur la chasse aux sorciÃ¨res dans leurs livres. Tous Ã©ditÃ©s dans les quarante derniÃ¨res annÃ©es, ils reviennent sur lâinterprÃ©tation de Michelet et nuancent le rÃ´le de lâÃglise.

La sorciÃ¨re qui lit lâavenir, fait tomber amoureux ou malade est crainte et dÃ©sapprouvÃ©e, mais tolÃ©rÃ©e. Une transformation de lâimage de la sorciÃ¨re et de celle du Diable intervient au XIIe siÃ¨cle. Elle et le Diable deviennent des conspirateurs qui cherchent Ã  empÃªcher le royaume de Dieu. La sorciÃ¨re est recherchÃ©e puis jugÃ©e avant dâÃªtre punie, dâabord lÃ©gÃ¨rement. Les conditions de vie sont dures et le Diable est tenu pour responsable. La RÃ©forme accÃ©lÃ¨re le phÃ©nomÃ¨ne. Plus lâÃglise catholique perd dâinfluence sur cette question, plus les bÃ»chers sâimposent Ã  lâissue des procÃ¨s. Autour de 1600, la justice pÃ©nale cherchant Ã  soustraire la sociÃ©tÃ© Ã  lâinfluence du Diable se montre impitoyable. Ã lâaube des LumiÃ¨res, les accusations provoquent des scandales et la dynamique de la Chasse sâÃ©puise. Les pouvoirs centraux Ã©mergeant mettent alors un terme aux poursuites. Les raisons profondes de cette Chasse restent une Ã©nigme : est-ce dÃ» Ã  lâÃ©mergence de pouvoirs centraux, au rejet des valeurs rurales, ou Ã  lâattitude des Ã©lites ? Chacun de ses Ã©lÃ©ments et dâautres ont sans doute jouÃ© un rÃ´le. Aucune cause unique ne peut Ãªtre mise en Ã©vidence. Mais lâanalyse rÃ©cente proposÃ©e par ces auteurs permet de se faire une meilleure idÃ©e des processus qui ont conduit Ã  la grande Chasse..."""
    texte_exemple = "Le chÃ¢teau se dressait sur une colline escarpÃ©e, dominant la vallÃ©e sinueuse oÃ¹ la riviÃ¨re serpentait lentement, reflÃ©tant les rayons du soleil couchant. Ã l'intÃ©rieur, de vastes salles rÃ©sonnaient du silence des siÃ¨cles passÃ©s, tandis que des tapisseriesComplexes ornaient les murs de pierre froide, racontant des histoires oubliÃ©es de chevaliers et de dames. Dehors, le vent murmurait Ã  travers les arbres centenaires, emportant avec lui les Ã©chos d'un temps rÃ©volu."
    sous_titres = splitter.split_two_line(t, length=10)

    print("\nSous-titres gÃ©nÃ©rÃ©s :")
    for i, subs in enumerate(sous_titres):
        f=", ".join([str(len(s)) for s in subs])

        print(f"{i + 1}: {subs} ({f} chars)")