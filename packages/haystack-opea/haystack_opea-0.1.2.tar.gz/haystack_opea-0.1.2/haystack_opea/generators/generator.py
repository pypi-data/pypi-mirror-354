# Copyright (C) 2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

import warnings
from typing import Any, Dict, List, Optional

from haystack import component

from haystack_opea.utils import OPEABackend, url_validation

_DEFAULT_API_URL = "http://localhost:9009/v1/chat/completions"


@component
class OPEAGenerator:
    """
    Generates text using generative models provided by
    [OPEA](https://opea.dev).

    Model ID only relevant for specific backends.

    ### Usage example

    ```python
    from haystack_opea.generators import OPEAGenerator

    generator = OPEAGenerator(
        "http://localhost:9009",
        "Intel/neural-chat-7b-v3-3",
        model_arguments={
            "temperature": 0.2,
            "top_p": 0.7,
            "max_tokens": 1024,
        },
    )
    generator.warm_up()

    result = generator.run(prompt="What is the answer?")
    print(result["replies"])
    print(result["meta"])
    print(result["meta"]["usage"])
    ```

    """

    def __init__(
        self,
        api_url: str = _DEFAULT_API_URL,
        model_id: Optional[str] = "",
        model_arguments: Optional[Dict[str, Any]] = None,
    ):
        """Create a OPEAGenerator component.

        :param api_url:
            Custom API URL for the OPEA Generator.
        :param model_arguments:
            Additional arguments to pass to the model provider. These arguments are
            specific to a model.
            Search your model in [OPEA](https://opea.dev)
            to find the arguments it accepts.
        """
        self._api_url = url_validation(api_url, _DEFAULT_API_URL, ["v1/chat/completions"])
        self._model_id = model_id or ""
        self._model_arguments = model_arguments or {}

        self._backend: Optional[Any] = None

    def warm_up(self):
        """Initializes the component."""
        if self._backend is not None:
            return

        self._backend = OPEABackend(
            api_url=self._api_url,
            model_id=self._model_id,
            model_kwargs=self._model_arguments,
        )

    @component.output_types(replies=List[str], meta=List[Dict[str, Any]])
    def run(self, prompt: str):
        """Queries the model with the provided prompt.

        :param prompt:
            Text to be sent to the generative model.
        :returns:
            A dictionary with the following keys:
            - `replies` - Replies generated by the model.
            - `meta` - Metadata for each reply.
        """
        if self._backend is None:
            msg = "The generation model has not been loaded. Call warm_up() before running."
            raise RuntimeError(msg)

        assert self._backend is not None
        replies, meta = self._backend.generate(prompt=prompt)

        return {"replies": replies, "meta": meta}
