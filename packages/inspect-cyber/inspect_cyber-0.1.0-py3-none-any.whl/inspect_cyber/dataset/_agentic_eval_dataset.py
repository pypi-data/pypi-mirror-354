import logging
import random
from collections import defaultdict
from collections.abc import Iterable
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Literal, overload

from inspect_ai.dataset import Dataset, Sample
from typing_extensions import override

from inspect_cyber.model import AgenticEvalMetadataKey

if TYPE_CHECKING:
    from _typeshed import SupportsRichComparison

logger = logging.getLogger(__name__)


class AgenticEvalDataset(Dataset):
    """A Dataset generated from samples created by inspect_cyber."""

    _root_dir: Path
    _samples: list[Sample]
    _name: str | None
    _shuffled: bool

    def __init__(
        self,
        root_dir: Path,
        samples: list[Sample],
        name: str | None = None,
        shuffled: bool = False,
    ) -> None:
        """A dataset of samples created by inspect_cyber.

        The samples are collected from "samples.yaml" files within root_dir and created
        according to the models specified in `inspect_cyber/model.py`.

        Args:
            root_dir (Path): Absolute path to directory used to discover evals.
            samples (list[Sample]): List of samples in the dataset.
            name (str | None): Optional name for the dataset.
            shuffled (bool): Whether the dataset was shuffled after reading.
        """
        self._root_dir = root_dir
        self._samples = samples
        self._name = name
        self._shuffled = shuffled

    @override
    @property
    def name(self) -> str | None:
        """Dataset name."""
        return self._name

    @override
    @property
    def location(self) -> str | None:
        """Dataset location."""
        return str(self._root_dir)

    @override
    @property
    def shuffled(self) -> bool:
        """Was the dataset shuffled."""
        return self._shuffled

    @overload
    def __getitem__(self, index: int) -> Sample: ...

    @overload
    def __getitem__(self, index: slice) -> Dataset: ...

    @override
    def __getitem__(self, index: int | slice) -> Sample | Dataset:
        if isinstance(index, int):
            return self._samples[index]
        else:
            return AgenticEvalDataset(
                self._root_dir, self._samples[index], self._name, self._shuffled
            )

    @override
    def __len__(self) -> int:
        return len(self._samples)

    @override
    def shuffle(self, seed: int | None = None) -> None:
        if seed:
            random.Random(seed).shuffle(self._samples)
        else:
            random.shuffle(self._samples)
        self._shuffled = True

    @override
    def shuffle_choices(self, seed: int | None = None) -> None:
        raise ValueError(
            "AgenticEvalsDataset entries do not have choices to be shuffled"
        )

    @override
    def sort(
        self,
        reverse: bool = False,
        key: Callable[[Sample], "SupportsRichComparison"] | None = None,
    ) -> None:
        if key is None:
            raise ValueError("A key function must be provided to sort the dataset.")

        self._shuffled = False
        self._samples.sort(key=key, reverse=reverse)

    @override
    def filter(
        self, predicate: Callable[[Sample], bool], name: str | None = None
    ) -> "AgenticEvalDataset":
        return AgenticEvalDataset(
            root_dir=self._root_dir,
            samples=[sample for sample in self if predicate(sample)],
            name=name or self.name,
            shuffled=self.shuffled,
        )

    def flat_map(
        self,
        mapper: Callable[[Sample], Iterable[Sample]],
        name: str | None = None,
    ) -> "AgenticEvalDataset":
        """
        Applies mapper(sample) to each sample.

        Args:
            mapper: method that processes a sample and returns 0 or more samples derived
                    from the original.
            name: Name for the mapped dataset(optional).

        Returns:
            Mapped dataset with a flat mapping of samples produced by calling the mapper
            method. Original samples will not be included, only values returned from the
            mapper method.
        """
        new_samples: list[Sample] = []

        for sample in self._samples:
            results = mapper(sample)
            if not isinstance(results, Iterable):
                raise TypeError(
                    f"Mapper method must return Iterable, not {type(results)}"
                )
            new_samples.extend(results)

        return AgenticEvalDataset(
            root_dir=self._root_dir,
            samples=new_samples,
            name=name or self.name,
            shuffled=False,
        )

    def filter_by_metadata_field(
        self,
        field_name: AgenticEvalMetadataKey,
        value: Any | Iterable[Any] | None,
        dataset_name: str | None = None,
    ) -> "AgenticEvalDataset":
        """
        Filter the dataset by a specific metadata field.

        This method allows filtering samples based on whether a specified metadata field
        matches any of the provided values. The field can be a top-level field or a
        nested field accessed using dot notation. If the field is not present
        in the sample metadata it is considered to be None and filtered out if
        the target value is not None.

        Args:
            field_name (AgenticEvalMetadataKey): The metadata field to filter by. For
                nested fields, use dot notation (e.g., "details.category.type"). The
                method traverses the nested structure to find the specified field.
            value (str | Iterable[str] | None): The value(s) to match against the
                specified field. Samples are included if their field value matches ANY
                of the provided values. If None, the original dataset is returned
                unchanged.
            dataset_name (str | None): Optional name for the resulting dataset. If None,
                the original dataset name is used.

        Returns:
            AgenticEvalsDataset: A new dataset containing only the samples where
                the specified metadata field matches one of the provided values.
        """
        # Type guards. Value should be coerced to be either a set of hashable values,
        # or a list of non-hashable values from Any or Iterable[Any].
        if value is None:
            return self

        if isinstance(value, str):
            values: set[Any] | list[Any] = {value}
        elif isinstance(value, dict):
            values = [value]
        else:
            if hasattr(value, "__iter__") and isinstance(value, Iterable):
                values = set(value)
            else:
                values = {value}

        def get_nested_value(d: dict[str, Any] | None, key_path: str) -> Any:
            """Get a value from a nested dictionary using dot notation."""
            if d is None:
                return None

            if "." not in key_path:
                return d.get(key_path)

            parts = key_path.split(".", 1)
            current_key, remaining_path = parts

            next_level = d.get(current_key)

            if not isinstance(next_level, dict) or next_level is None:
                return None

            return get_nested_value(next_level, remaining_path)

        def predicate(sample: Sample) -> bool:
            """Predicate function to check if the sample matches the filter."""
            field_value = get_nested_value(sample.metadata or {}, field_name)
            try:
                return field_value in values
            except TypeError:
                # Likely the field is not hashable. Iterate over the values
                # and check for equality.
                for v in values:
                    if field_value == v:
                        return True
            return False

        return self.filter(
            predicate,
            name=dataset_name,
        )

    def filter_by_metadata(
        self, metadata: dict[str, Any] | None, name: str | None = None
    ) -> "AgenticEvalDataset":
        """
        Filter the dataset based on metadata key-value pairs.

        Args:
            metadata (dict[str, Any] | None): A dictionary of metadata key-value pairs
                to filter by. Keys can use dot notation for nested fields. If None, no
                filtering is applied.
            name (str | None): Optional name for the resulting dataset. If None,
                the original dataset name is used.

        Returns:
            AgenticEvalsDataset: A new dataset containing only the samples that
                match the specified metadata.
        """
        if metadata is None:
            return self

        filtered_dataset = self
        for key, value in metadata.items():
            filtered_dataset = filtered_dataset.filter_by_metadata_field(key, value)

        if name is not None:
            filtered_dataset._name = name

        return filtered_dataset

    def filter_by_sandbox(
        self, sandbox_type: str | list[str] | None, name: str | None = None
    ) -> "AgenticEvalDataset":
        """
        Filter the dataset by a specific sandbox name.

        Args:
            sandbox_type (str | None): The name of the sandbox to filter by. If None,
                no filtering is applied.
            name (str | None): Optional name for the resulting dataset. If None,
                the original dataset name is used.

        Returns:
            AgenticEvalsDataset: A new dataset containing only the samples that
                belong to the specified sandbox.
        """
        if sandbox_type is None:
            return self

        def predicate(sample: Sample) -> bool:
            """Predicate function to check if the sample belongs to the sandbox."""
            if sample.sandbox is None:
                return False
            if isinstance(sandbox_type, list):
                return sample.sandbox.type in sandbox_type
            # sandbox_type is a str
            return sample.sandbox.type == sandbox_type

        return self.filter(
            predicate,
            name=name,
        )

    def group_by(
        self, key: Literal["all", "eval", "variant"]
    ) -> list["AgenticEvalDataset"]:
        """
        Group the dataset into subsets based on the specified key.

        Args:
            key (Literal["all", "eval", "variant"]): The grouping strategy:
                - "all": Returns the entire dataset as a single group.
                - "eval": Groups samples by evaluation names.
                - "variant": Groups samples by unique evaluation-variant pairs.

        Returns:
            list[AgenticEvalsDataset]: A list of dataset subsets, each containing
                samples grouped according to the specified key.
        """
        if key == "all":
            return [self]

        elif key == "eval":
            datasets = defaultdict(list)
            for sample in self._samples:
                eval_name = (
                    sample.metadata.get("eval_name", "Unknown Eval")
                    if sample.metadata
                    else "Unknown Eval"
                )
                datasets[eval_name].append(sample)
            return [
                AgenticEvalDataset(
                    root_dir=self._root_dir,
                    samples=samples,
                    name=eval_name,
                    shuffled=self.shuffled,
                )
                for eval_name, samples in datasets.items()
            ]

        else:  # key == "variant"
            return [
                AgenticEvalDataset(
                    root_dir=self._root_dir,
                    samples=[sample],
                    name=str(sample.id),
                    shuffled=self.shuffled,
                )
                for sample in self._samples
            ]
