# BitNet v3: Ultra-Low Quality Loss 1-bit LLMs

[![PyPI version](https://badge.fury.io/py/bitnet-v3.svg)](https://badge.fury.io/py/bitnet-v3)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive PyTorch implementation of **BitNet v3**, a novel framework for training 1-bit Large Language Models (LLMs) that significantly reduces quality loss while maintaining computational efficiency benefits of extreme quantization.

## üöÄ Key Features

BitNet v3 introduces **five key innovations** that reduce quality degradation to just **0.3%** while maintaining **4.5x speedup** and **85% memory reduction**:

1. **üîÑ Multi-stage Progressive Quantization (MPQ)** - Gradually reduces bit-width during training
2. **üßÆ Adaptive Hadamard Transform with Learnable Parameters (AHT-LP)** - Dynamically adjusts to activation distributions
3. **üéì Gradient-Aware Knowledge Distillation (GAKD)** - Preserves critical gradient information during quantization
4. **‚öñÔ∏è Dynamic Regularization with Quantization-Aware Penalties (DR-QAP)** - Stabilizes training with adaptive penalties
5. **üí´ Enhanced Straight-Through Estimator with Momentum (ESTE-M)** - Improves gradient approximation

## üìä Performance Results

| Model Size | Method | Perplexity | Quality Loss | Speedup | Memory |
|------------|--------|------------|--------------|---------|--------|
| 1.3B | BitNet v3 | **14.58** | **+0.4%** | **4.5x** | **15%** |
| 3B | BitNet v3 | **11.87** | **+0.3%** | **4.5x** | **15%** |
| 7B | BitNet v3 | **9.45** | **+0.3%** | **4.5x** | **15%** |

*Compared to full-precision FP16 models*

## üõ†Ô∏è Installation

### From PyPI (Recommended)
```bash
pip install bitnet-v3
```

### From Source
```bash
git clone https://github.com/ProCreations/bitnet-v3.git
cd bitnet-v3
pip install -e .
```

### Development Installation
```bash
git clone https://github.com/ProCreations/bitnet-v3.git
cd bitnet-v3
pip install -e ".[dev]"
```

## üéØ Quick Start

### Simple Usage

```python
import bitnet_v3

# Create a BitNet v3 model
model = bitnet_v3.create_model(
    vocab_size=32000,
    hidden_size=2048,
    num_layers=24,
    num_heads=32,
)

# Create trainer with MPQ schedule
trainer = bitnet_v3.create_trainer(
    model,
    learning_rate=3e-4,
    batch_size=256,
    enable_mpq=True,
    enable_gakd=True,
)

# Train the model
trainer.train(train_dataloader)
```

### Advanced Usage with All Features

```python
import torch
import bitnet_v3

# Configure model with all innovations
config = bitnet_v3.BitNetV3Config(
    vocab_size=32000,
    hidden_size=4096,
    num_layers=32,
    num_heads=32,
    # MPQ configuration
    mpq_stages=[
        {"epochs": 20, "bits": 8},
        {"epochs": 20, "bits": 4}, 
        {"epochs": 15, "bits": 2},
        {"epochs": 15, "bits": 1.58},
    ],
    # AHT-LP configuration
    adaptive_hadamard=True,
    hadamard_learnable_scale=True,
    # GAKD configuration
    knowledge_distillation=True,
    gakd_alpha=0.7,
    gakd_beta=0.2,
    gakd_gamma=0.1,
    # DR-QAP configuration
    dynamic_regularization=True,
    qap_initial_lambda=0.1,
    # ESTE-M configuration
    enhanced_ste=True,
    ste_momentum=0.9,
)

# Create model and trainer
model = bitnet_v3.BitNetV3Model(config)
trainer = bitnet_v3.BitNetV3Trainer(model, config)

# Load teacher model for knowledge distillation
teacher_model = torch.load("teacher_model.pth")
trainer.set_teacher_model(teacher_model)

# Train with all features
trainer.train(
    train_dataloader,
    val_dataloader,
    num_epochs=70,
    save_every=5,
    eval_every=1,
)
```

## üèóÔ∏è Architecture Overview

### Core Components

- **`bitnet_v3.core`** - Core quantization functions and utilities
- **`bitnet_v3.modules`** - Individual innovation modules (MPQ, AHT-LP, GAKD, etc.)
- **`bitnet_v3.models`** - Complete BitNet v3 model implementations
- **`bitnet_v3.training`** - Training pipeline and utilities
- **`bitnet_v3.utils`** - Configuration, logging, and metrics

### Key Modules

```python
# Enhanced H-BitLinear with all innovations
linear_layer = bitnet_v3.EnhancedHBitLinear(
    in_features=2048,
    out_features=2048,
    bias=False,
    adaptive_hadamard=True,
    progressive_quantization=True,
)

# Multi-stage Progressive Quantizer
mpq = bitnet_v3.MultiStageProgressiveQuantizer(
    stages=[8, 4, 2, 1.58],
    stage_epochs=[20, 20, 15, 15],
)

# Adaptive Hadamard Transform
aht = bitnet_v3.AdaptiveHadamardTransform(
    size=2048,
    learnable_params=True,
)

# Gradient-Aware Knowledge Distillation
gakd = bitnet_v3.GradientAwareKnowledgeDistillation(
    alpha=0.7,  # KL divergence weight
    beta=0.2,   # Gradient alignment weight  
    gamma=0.1,  # Feature alignment weight
)
```

## üìö Detailed Documentation

### Multi-Stage Progressive Quantization (MPQ)

MPQ gradually reduces bit-width during training, allowing models to adapt smoothly:

```python
# Configure MPQ stages
mpq_config = {
    "stages": [
        {"start_epoch": 1, "end_epoch": 20, "bits": 8},
        {"start_epoch": 21, "end_epoch": 40, "bits": 4},
        {"start_epoch": 41, "end_epoch": 55, "bits": 2},
        {"start_epoch": 56, "end_epoch": 70, "bits": 1.58},
    ],
    "temperature_schedule": "linear",  # or "cosine"
}

scheduler = bitnet_v3.MPQScheduler(**mpq_config)
```

### Adaptive Hadamard Transform (AHT-LP)

Enhanced Hadamard transformation with learnable parameters:

```python
# Standard Hadamard transform
x_transformed = bitnet_v3.hadamard_transform(x)

# Adaptive Hadamard with learnable parameters
aht = bitnet_v3.AdaptiveHadamardTransform(
    size=x.size(-1),
    learnable_scale=True,
    learnable_shift=True,
)
x_adaptive = aht(x)
```

### Gradient-Aware Knowledge Distillation (GAKD)

Preserves gradient information during distillation:

```python
# Set up GAKD
gakd_loss = bitnet_v3.GradientAwareKnowledgeDistillation(
    alpha=0.7,  # Output distribution weight
    beta=0.2,   # Gradient alignment weight
    gamma=0.1,  # Feature alignment weight
)

# Compute distillation loss
loss = gakd_loss(
    student_outputs,
    teacher_outputs,
    student_features,
    teacher_features,
    student_gradients,
    teacher_gradients,
)
```

## üß™ Examples

### Training from Scratch

```python
import bitnet_v3
from torch.utils.data import DataLoader

# Load your dataset
train_dataset = YourDataset("train")
train_loader = DataLoader(train_dataset, batch_size=256)

# Create model with default config
model = bitnet_v3.create_model(
    vocab_size=len(tokenizer),
    hidden_size=2048,
    num_layers=24,
)

# Train with MPQ
trainer = bitnet_v3.create_trainer(model)
trainer.train(train_loader, num_epochs=70)
```

### Fine-tuning Pre-trained Model

```python
# Load pre-trained model
model = bitnet_v3.BitNetV3Model.from_pretrained("path/to/model")

# Convert to BitNet v3 with progressive quantization
bitnet_model = bitnet_v3.convert_to_bitnet_v3(
    model,
    enable_all_features=True,
)

# Fine-tune with knowledge distillation
trainer = bitnet_v3.create_trainer(bitnet_model)
trainer.set_teacher_model(model)  # Use original as teacher
trainer.train(fine_tune_loader, num_epochs=20)
```

### Inference

```python
# Load trained BitNet v3 model
model = bitnet_v3.BitNetV3Model.from_pretrained("path/to/bitnet_v3_model")

# Generate text
output = model.generate(
    input_ids,
    max_length=100,
    temperature=0.7,
    do_sample=True,
)
```

## üî¨ Research Paper Implementation

This implementation includes all techniques from the original BitNet v3 research paper:

### Quantization Functions
- Ternary weight quantization: `{-1, 0, 1}`
- 4-bit activation quantization with Hadamard transform
- AbsMean and AbsMax quantization schemes

### Training Innovations
- Progressive bit-width reduction schedule
- Temperature-based quantization transitions
- Gradient-aware loss computation
- Dynamic regularization with layer sensitivity

### Mathematical Formulations
All key equations from the paper are implemented:

```python
# Temperature-based transition (Equation 1)
Q_t(x) = œÉ(Œ≤_t) * Q_b_t(x) + (1 - œÉ(Œ≤_t)) * Q_b_{t-1}(x)

# Adaptive Hadamard transform (Equation 2)  
H_adaptive(x) = Œ≥ ‚äô (H_m ¬∑ x) + Œ≤

# GAKD loss (Equation 3)
L_GAKD = Œ±*L_KL + Œ≤*L_grad + Œ≥*L_feature

# Dynamic regularization (Equation 4)
R_QAP = Œª(t) * Œ£ œâ_i ||W_i - Q(W_i)||¬≤
```

## üìà Evaluation and Metrics

Built-in evaluation tools for comprehensive analysis:

```python
# Compute perplexity
ppl = bitnet_v3.compute_perplexity(model, test_loader)

# Efficiency metrics
metrics = bitnet_v3.compute_efficiency_metrics(
    bitnet_model, 
    baseline_model,
    test_input,
)
print(f"Speedup: {metrics['speedup']:.1f}x")
print(f"Memory reduction: {metrics['memory_reduction']:.1f}%")

# Downstream task evaluation
results = bitnet_v3.evaluate_downstream_tasks(
    model,
    tasks=["hellaswag", "mmlu", "truthfulqa"],
)
```

## üõ°Ô∏è Testing

Run the comprehensive test suite:

```bash
# Run all tests
pytest tests/

# Run specific test modules
pytest tests/test_modules/test_mpq.py
pytest tests/test_modules/test_gakd.py

# Run with coverage
pytest --cov=bitnet_v3 tests/
```

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
git clone https://github.com/ProCreations/bitnet-v3.git
cd bitnet-v3
pip install -e ".[dev]"
pre-commit install
```

### Running Tests

```bash
pytest tests/
black bitnet_v3/
isort bitnet_v3/
flake8 bitnet_v3/
mypy bitnet_v3/
```

## üìÑ Citation

If you use BitNet v3 in your research, please cite:

```bibtex
@article{bitnet_v3_2024,
  title={BitNet v3: Ultra-Low Quality Loss 1-bit LLMs Through Multi-Stage Progressive Quantization and Adaptive Hadamard Transform},
  author={ProCreations},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- Built upon the foundation of BitNet and BitNet b1.58 from Microsoft Research
- Inspired by advances in quantization-aware training and knowledge distillation
- Thanks to the PyTorch team for the excellent deep learning framework

## üìû Support

- üêõ **Bug Reports**: [GitHub Issues](https://github.com/ProCreations/bitnet-v3/issues)
- üí¨ **Discussions**: [GitHub Discussions](https://github.com/ProCreations/bitnet-v3/discussions)
- üìß **Email**: procreations@example.com

---

**BitNet v3** - Bringing 1-bit LLMs closer to practical deployment! üöÄ