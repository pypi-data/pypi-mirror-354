import json
import os
from evalbench.runtime_setup.runtime import get_config

# Global flag to suppress printing inside decorated functions
_SUPPRESS_PRINTING = False

def suppress_printing():
    global _SUPPRESS_PRINTING
    _SUPPRESS_PRINTING = True

def enable_printing():
    global _SUPPRESS_PRINTING
    _SUPPRESS_PRINTING = False

def is_printing_suppressed():
    return _SUPPRESS_PRINTING

def print_results(name, input_data, results, error_message=None):
    print(f'\n{name.upper()}:')
    if error_message:
        print(json.dumps({'error': error_message}))
        return

    if not results:
        print(json.dumps({'warning': 'No results'}))
        return

    # Case: results is a list of floats
    if isinstance(results, list) and all(isinstance(r, (float, int)) for r in results):
        for i, score in enumerate(results):
            # Match inputs to corresponding score if inputs are batched
            record = {
                'input': {
                    key: (value[i] if isinstance(value, list) and len(value) == len(results) else value)
                    for key, value in input_data.items()
                },
                'output': score
            }
            print(json.dumps(record))

    # Case: results is a list of dicts
    elif isinstance(results, list) and all(isinstance(r, dict) for r in results):
        for i, res_dict in enumerate(results):
            record = {
                'input': {
                    key: (value[i] if isinstance(value, list) and len(value) == len(results) else value)
                    for key, value in input_data.items()
                },
                'output': res_dict
            }
            print(json.dumps(record))

    # Case: single score or object
    else:
        print(json.dumps({'input': input_data, 'output': results}))

def save_results(name, input_data, result, error_message):
    cfg = get_config()
    output_path = cfg.output_filepath or f'./outputs/{name}.jsonl'
    directory = os.path.dirname(output_path)

    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

    with open(output_path, 'a') as f:
        if error_message:
            record = {
                'metric': name,
                'error': error_message
            }
            f.write(json.dumps(record) + '\n')

        # Case: batched numeric results
        if isinstance(result, list) and all(isinstance(r, (float, int)) for r in result):
            for i, r in enumerate(result):
                record = {
                    'metric': name,
                    'input': {
                        key: (value[i] if isinstance(value, list) and len(value) == len(result) else value)
                        for key, value in input_data.items()
                    },
                    'output': r
                }
                f.write(json.dumps(record) + '\n')

        # Case: batched dict results
        elif isinstance(result, list) and all(isinstance(r, dict) for r in result):
            for i, r in enumerate(result):
                record = {
                    'metric': name,
                    'input': {
                        key: (value[i] if isinstance(value, list) and len(value) == len(result) else value)
                        for key, value in input_data.items()
                    },
                    'output': r
                }
                f.write(json.dumps(record) + '\n')

        # Case: single result
        else:
            record = {
                'metric': name,
                'input': input_data,
                'output': result
            }
            f.write(json.dumps(record) + '\n')

def generate_report(request):
    instruction = request.get('instruction', 'N/A')
    task = request.get('task', 'Unknown')
    data = json.dumps(request.get('data', {}), indent=2) if request.get('data') else 'N/A'
    results = request.get('results', {})
    interpretation = request.get('interpretation', '')
    recommendations = request.get('recommendations', '')

    report = [
        '---',
        'LLM Evaluation Report\n',
        f'Instruction:\n{instruction}\n',
        f'Inferred Task:\n{task}\n',
        f'Input Data:\n{data}\n',
    ]

    if results:
        report.append('Evaluation Results:')
        if isinstance(results, (dict, list)):
            report.append(json.dumps(results, indent=2))
        else:
            report.append(str(results))  # fallback
        report.append('')

    if interpretation:
        report.append('Interpretation:')
        if isinstance(interpretation, list):
            report.append('\n'.join(interpretation))
        else:
            report.append(interpretation)
        report.append('')

    if recommendations:
        report.append('Recommendation:')
        report.append(recommendations)
        report.append('')

    report.append('---')
    report.append('This report was generated by EvalBench.')

    return '\n'.join(report)