{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Agent State Machine (ASM) - Quick Start Guide\n",
                "\n",
                "An **agent** is a wrapper around a finite state machine designed to accomplish a specific task and will be referred to as ASM.\n",
                "\n",
                "A **StateMachineBuilder** is used to build the ASM from a manifest and a state diagram.\n",
                "\n",
                "```mermaid\n",
                "flowchart TD\n",
                "    A[\"state_diagram\"] -->|Input| B[\"StateMachineBuilder\"]\n",
                "    A2[\"state_manifest\"] -->|Input| C[\"StateModel\"]\n",
                "    C -->|Build| B\n",
                "    B --> D[\"fsm\"]\n",
                "```\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: Standard Assistant\n",
                "\n",
                "In this example, we will demonstrate how to create a simple **Assistant** agent using a state diagram and state manifest.\n",
                "\n",
                "### a) Define State Diagram\n",
                "\n",
                "The following is a simple example of an **Assistant** agent using 3 states:\n",
                "\n",
                "* **INIT:** Collect initial input.\n",
                "* **GENERATE:** The state where the LLM generates a response.\n",
                "* **FINAL:** Return final output.\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> GENERATE: next / action\n",
                "GENERATE --> FINAL: next / action\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_DIAGRAM = \"\"\"\n",
                "    INIT --> GENERATE\n",
                "    GENERATE --> FINAL\n",
                "    \"\"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### b) Define State Manifest\n",
                "\n",
                "The state manifest is a dictionary. \n",
                "Each state in the manifest corresponds to a state in the state diagram.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_MANIFEST_V1 = {\n",
                "        \"INIT\": {\n",
                "            \"input_data\": {\n",
                "                \"name\": \"Sara\",\n",
                "                \"user_message\": \"Write a one sentence story\"\n",
                "            }\n",
                "        },\n",
                "        \"GENERATE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"PureActionState\",\n",
                "            \"title\": \"GENERATE\",\n",
                "            \"action\": \"generate\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\n",
                "                    \"type\": \"getter\",\n",
                "                    \"dependency\": \"get_llm_config\"\n",
                "                },\n",
                "            },\n",
                "            \"output_data\": [\n",
                "                \"streamer\",\n",
                "                \"get_assistant_message\"\n",
                "            ]\n",
                "        },\n",
                "        \"FINAL\": {\n",
                "            \"output_data\": [\"monologue\"]\n",
                "        }\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Create state action"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.asm import AsyncStateMachine\n",
                "from gai.chat.openai import AsyncOpenAI\n",
                "\n",
                "async def generate_action(state):\n",
                "    \n",
                "    llm_config = state.machine.state_bag[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    user_message = state.machine.state_bag.get(\"user_message\", \"If you are seeing this, that means I have forgotten to add a user message. Remind me.\")\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=[{\n",
                "            \"role\":\"user\",\n",
                "            \"content\":user_message\n",
                "            }],\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Build State Machine"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gai.asm import AsyncStateMachine\n",
                "\n",
                "with AsyncStateMachine.StateMachineBuilder(STATE_DIAGRAM) as builder:\n",
                "    fsm = builder.build(\n",
                "        STATE_MANIFEST_V1,\n",
                "        get_llm_config=lambda state: {\n",
                "            \"client_type\": \"gai\",\n",
                "            \"name\": \"dolphin_llama:exl2\",\n",
                "            \"model\": \"ttt\",\n",
                "            \"url\": \"http://gai-chat-svr:12031/gen/v1/chat/completions\"\n",
                "        },\n",
                "        generate=generate_action\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Run State Machine (INIT->GENERATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "As the first rays of sunlight touched the dew-covered grass, a lone figure emerged from the darkness, their footsteps silent and purposeful, leaving behind only the faintest hint of mystery.\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Continue (GENERATE->FINAL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State History:\n",
                        "State: INIT\n",
                        "- input: {'name': 'Sara', 'user_message': 'Write a one sentence story'}\n",
                        "- output: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x744099580fd0>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 13, 43, 8, 25139)}\n",
                        "--------------------\n",
                        "State: GENERATE\n",
                        "- input: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x744099580fd0>, 'step': 1, 'time': datetime.datetime(2025, 6, 11, 13, 43, 8, 25451), 'llm_config': {'client_type': 'gai', 'name': 'dolphin_llama:exl2', 'model': 'ttt', 'url': 'http://gai-chat-svr:12031/gen/v1/chat/completions'}}\n",
                        "- output: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x744082e68a40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x744082e639a0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x744082e3fca0>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 13, 43, 8, 601058)}\n",
                        "--------------------\n",
                        "State: FINAL\n",
                        "- input: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x744082e68a40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x744082e639a0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x744082e3fca0>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 13, 43, 8, 601058)}\n",
                        "- output: {'monologue': <gai.asm.monologue.Monologue object at 0x744099580fd0>}\n",
                        "--------------------\n",
                        "Assistant Message:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'As the first rays of sunlight touched the dew-covered grass, a lone figure emerged from the darkness, their footsteps silent and purposeful, leaving behind only the faintest hint of mystery.'"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "print(\"State History:\")\n",
                "for state in fsm.state_history:\n",
                "    print(f\"State: {state['state']}\")\n",
                "    print(f\"- input: {state['input']}\")\n",
                "    print(f\"- output: {state['output']}\")\n",
                "    print(\"-\" * 20)\n",
                "print(\"Assistant Message:\")\n",
                "fsm.state_bag[\"get_assistant_message\"]()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Example: Standard Assistant (Part 2)\n",
                "\n",
                "Same example but with an additional state to demonstrate context management by monologue messages.\n",
                "\n",
                "```mermaid\n",
                "stateDiagram-v2\n",
                "direction LR\n",
                "INIT --> GENERATE\n",
                "GENERATE --> CONTINUE\n",
                "CONTINUE --> FINAL\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "STATE_DIAGRAM = \"\"\"\n",
                "    INIT --> GENERATE\n",
                "    GENERATE --> CONTINUE\n",
                "    CONTINUE --> FINAL\n",
                "    \"\"\"\n",
                "    \n",
                "STATE_MANIFEST_V1 = {\n",
                "        \"INIT\": {\n",
                "            \"input_data\": {\n",
                "                \"name\": \"Sara\",\n",
                "                \"user_message\": \"Write a one sentence story\"\n",
                "            }\n",
                "        },\n",
                "        \"GENERATE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"PureActionState\",\n",
                "            \"title\": \"GENERATE\",\n",
                "            \"action\": \"generate_action\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\n",
                "                    \"type\": \"getter\",\n",
                "                    \"dependency\": \"get_llm_config\"\n",
                "                },\n",
                "            },\n",
                "            \"output_data\": [\n",
                "                \"streamer\",\n",
                "                \"get_assistant_message\"\n",
                "            ]\n",
                "        },\n",
                "        \"CONTINUE\": {\n",
                "            \"module_path\": \"gai.asm.states\",\n",
                "            \"class_name\": \"PureActionState\",\n",
                "            \"title\": \"CONTINUE\",\n",
                "            \"action\": \"continue_action\",\n",
                "            \"input_data\": {\n",
                "                \"llm_config\": {\n",
                "                    \"type\": \"getter\",\n",
                "                    \"dependency\": \"get_llm_config\"\n",
                "                },\n",
                "            },\n",
                "            \"output_data\": [\n",
                "                \"streamer\",\n",
                "                \"get_assistant_message\"\n",
                "            ]\n",
                "        },\n",
                "        \"FINAL\": {\n",
                "            \"output_data\": [\"monologue\"]\n",
                "        }\n",
                "    }\n",
                "\n",
                "from gai.chat.openai import AsyncOpenAI\n",
                "\n",
                "async def generate_action(state):\n",
                "    \n",
                "    llm_config = state.machine.state_bag[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    user_message = state.machine.state_bag.get(\"user_message\", \"If you are seeing this, that means I have forgotten to add a user message. Remind me.\")\n",
                "    monologue = state.machine.state_bag[\"monologue\"]\n",
                "    monologue.add_user_message(state=state,content=user_message)\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=monologue.list_chat_messages(),\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "        monologue.add_assistant_message(state=state,content=assistant_message)\n",
                "    state.machine.state_bag[\"monologue\"] = monologue\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()\n",
                "\n",
                "async def continue_action(state):\n",
                "    \n",
                "    llm_config = state.machine.state_bag[\"llm_config\"]\n",
                "    client = AsyncOpenAI(llm_config)\n",
                "    \n",
                "    # Import data from state_bag\n",
                "    monologue = state.machine.state_bag[\"monologue\"]\n",
                "    monologue.add_user_message(state=state,content=\"Please continue.\")\n",
                "    \n",
                "    # Execute\n",
                "    \n",
                "    response = await client.chat.completions.create(\n",
                "        model=llm_config[\"model\"],\n",
                "        messages=monologue.list_chat_messages(),\n",
                "        max_tokens=50,\n",
                "        stream=True\n",
                "    )\n",
                "    \n",
                "    assistant_message = \"\"\n",
                "    async def streamer():\n",
                "        nonlocal assistant_message\n",
                "        async for chunk in response:\n",
                "            chunk = chunk.choices[0].delta.content\n",
                "            if isinstance(chunk,str) and chunk:\n",
                "                assistant_message += chunk\n",
                "                yield chunk\n",
                "        monologue.add_assistant_message(state=state,content=assistant_message)\n",
                "    state.machine.state_bag[\"monologue\"] = monologue\n",
                "    state.machine.state_bag[\"get_assistant_message\"] = lambda: assistant_message\n",
                "    state.machine.state_bag[\"streamer\"] = streamer()\n",
                "    \n",
                "from gai.asm import AsyncStateMachine\n",
                "\n",
                "with AsyncStateMachine.StateMachineBuilder(STATE_DIAGRAM) as builder:\n",
                "    fsm = builder.build(\n",
                "        STATE_MANIFEST_V1,\n",
                "        get_llm_config=lambda state: {\n",
                "            \"client_type\": \"gai\",\n",
                "            \"name\": \"dolphin_llama:exl2\",\n",
                "            \"model\": \"ttt\",\n",
                "            \"url\": \"http://gai-chat-svr:12031/gen/v1/chat/completions\"\n",
                "        },\n",
                "        generate_action=generate_action,\n",
                "        continue_action=continue_action\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### d) Run State Machine (INIT->GENERATE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Once upon a time, in a land far away, there was a beautiful princess who lived in a magnificent castle with tall towers and grand halls.\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### c) Run State Machine (GENERATE->CONTINUE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The princess had everything she could ever want, from fine clothes to elegant feasts, but she was not happy because she longed for true love and companionship. One day, a handsome prince arrived in the kingdom, and the princess fell in love with\n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "async for chunk in fsm.state_bag[\"streamer\"]:\n",
                "    print(chunk,end='',flush=True)\n",
                "print(\"\\n\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### e) END (GENERATE->FINAL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "State History:\n",
                        "State: INIT\n",
                        "- input: {'name': 'Sara', 'user_message': 'Write a one sentence story'}\n",
                        "- output: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x79c806c23910>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 14, 6, 6, 318)}\n",
                        "--------------------\n",
                        "State: GENERATE\n",
                        "- input: {'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x79c806c23910>, 'step': 1, 'time': datetime.datetime(2025, 6, 11, 14, 6, 6, 523), 'llm_config': {'client_type': 'gai', 'name': 'dolphin_llama:exl2', 'model': 'ttt', 'url': 'http://gai-chat-svr:12031/gen/v1/chat/completions'}}\n",
                        "- output: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x79c80631cd40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x79c806327ac0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x79c82425b610>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 14, 6, 6, 603711)}\n",
                        "--------------------\n",
                        "State: CONTINUE\n",
                        "- input: {'streamer': <async_generator object generate_action.<locals>.streamer at 0x79c80631cd40>, 'get_assistant_message': <function generate_action.<locals>.<lambda> at 0x79c806327ac0>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x79c82425b610>, 'step': 1, 'time': datetime.datetime(2025, 6, 11, 14, 6, 41, 733242), 'llm_config': {'client_type': 'gai', 'name': 'dolphin_llama:exl2', 'model': 'ttt', 'url': 'http://gai-chat-svr:12031/gen/v1/chat/completions'}}\n",
                        "- output: {'streamer': <async_generator object continue_action.<locals>.streamer at 0x79c80631dd40>, 'get_assistant_message': <function continue_action.<locals>.<lambda> at 0x79c8243c3010>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x79c8061952a0>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 14, 6, 41, 752806)}\n",
                        "--------------------\n",
                        "State: FINAL\n",
                        "- input: {'streamer': <async_generator object continue_action.<locals>.streamer at 0x79c80631dd40>, 'get_assistant_message': <function continue_action.<locals>.<lambda> at 0x79c8243c3010>, 'name': 'Assistant', 'monologue': <gai.asm.monologue.Monologue object at 0x79c8061952a0>, 'step': 0, 'time': datetime.datetime(2025, 6, 11, 14, 6, 41, 752806)}\n",
                        "- output: {'monologue': <gai.asm.monologue.Monologue object at 0x79c806c23910>}\n",
                        "--------------------\n",
                        "Assistant Message:\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'The princess had everything she could ever want, from fine clothes to elegant feasts, but she was not happy because she longed for true love and companionship. One day, a handsome prince arrived in the kingdom, and the princess fell in love with'"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "await fsm.run_async()\n",
                "print(\"State History:\")\n",
                "for state in fsm.state_history:\n",
                "    print(f\"State: {state['state']}\")\n",
                "    print(f\"- input: {state['input']}\")\n",
                "    print(f\"- output: {state['output']}\")\n",
                "    print(\"-\" * 20)\n",
                "print(\"Assistant Message:\")\n",
                "fsm.state_bag[\"get_assistant_message\"]()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
