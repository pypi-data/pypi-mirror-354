"""
–§–æ–Ω–æ–≤—ã–π –≤–æ—Ä–∫–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤.
"""
import asyncio
import time
from typing import Dict, List, Optional, Set
from datetime import datetime

from mindbank_poc.common.logging import get_logger
from mindbank_poc.core.config.settings import settings
from mindbank_poc.core.knowledge_store import get_knowledge_store
from mindbank_poc.core.knowledge_store.base import KnowledgeStore
from mindbank_poc.api.normalizers.manager import get_normalization_manager
from mindbank_poc.api.schemas import AggregateInput

logger = get_logger(__name__)


class NormalizationWorker:
    """
    –§–æ–Ω–æ–≤—ã–π –≤–æ—Ä–∫–µ—Ä –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É NormalizationManager.
    """
    
    def __init__(self, knowledge_store: Optional[KnowledgeStore] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–æ—Ä–∫–µ—Ä–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
        
        Args:
            knowledge_store: –•—Ä–∞–Ω–∏–ª–∏—â–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü
        """
        self.knowledge_store = knowledge_store or get_knowledge_store()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.enabled = getattr(settings.enrichment, 'normalization_enabled', True)
        self.min_pending = getattr(settings.enrichment, 'normalization_min_pending', 5)
        self.timeout_sec = getattr(settings.enrichment, 'normalization_timeout_sec', 1800)  # 30 –º–∏–Ω
        self.check_interval = settings.enrichment.check_interval_seconds
        
        # –í—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—É—Å–∫–∞
        self.last_run_timestamp = 0
        
        # –í–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self._running = False
        self._task: Optional[asyncio.Task] = None
        
        logger.info(
            f"NormalizationWorker initialized: "
            f"enabled={self.enabled}, min_pending={self.min_pending}, "
            f"timeout={self.timeout_sec}s, check_interval={self.check_interval}s"
        )
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π –≤–æ—Ä–∫–µ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏."""
        if not self.enabled:
            logger.info("NormalizationWorker is disabled in settings")
            return
            
        if self._running:
            logger.warning("NormalizationWorker is already running")
            return
        
        self._running = True
        self._task = asyncio.create_task(self._run_worker())
        logger.info("NormalizationWorker started")
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ–æ–Ω–æ–≤—ã–π –≤–æ—Ä–∫–µ—Ä –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏."""
        if not self._running:
            return
            
        self._running = False
        
        if self._task:
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        
        logger.info("NormalizationWorker stopped")
    
    async def _run_worker(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –≤–æ—Ä–∫–µ—Ä–∞."""
        logger.info("NormalizationWorker main loop started")
        
        while self._running:
            try:
                await self._check_and_process()
                await asyncio.sleep(self.check_interval)
            except asyncio.CancelledError:
                logger.info("NormalizationWorker loop cancelled")
                break
            except Exception as e:
                logger.error(f"Error in NormalizationWorker loop: {e}", exc_info=True)
                await asyncio.sleep(self.check_interval)
    
    async def _check_and_process(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ç–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –≤ –æ—á–µ—Ä–µ–¥—å."""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç—ã
            unprocessed_aggregates = await self._get_unprocessed_aggregates()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –∑–∞–ø—É—Å–∫–∞
            if not self._should_run_normalization(len(unprocessed_aggregates)):
                logger.debug(f"Normalization skipped: {len(unprocessed_aggregates)} pending aggregates")
                return
            
            if not unprocessed_aggregates:
                self.last_run_timestamp = time.time()
                logger.debug("No unprocessed aggregates found")
                return
            
            logger.info(f"üöÄ Starting normalization of {len(unprocessed_aggregates)} unprocessed aggregates")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–≥—Ä–µ–≥–∞—Ç—ã –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –æ—á–µ—Ä–µ–¥—å —á–µ—Ä–µ–∑ NormalizationManager
            normalization_manager = await get_normalization_manager()
            
            processed_count = 0
            for aggregate in unprocessed_aggregates:
                try:
                    aggregate_dict = aggregate.model_dump()
                    success = await normalization_manager.add_aggregate(aggregate_dict)
                    if success:
                        processed_count += 1
                    else:
                        logger.warning(f"Failed to add aggregate {aggregate.id} to normalization queue")
                except Exception as e:
                    logger.error(f"Error adding aggregate {aggregate.id} to queue: {e}")
            
            self.last_run_timestamp = time.time()
            logger.info(f"‚úÖ Added {processed_count}/{len(unprocessed_aggregates)} aggregates to normalization queue")
            
        except Exception as e:
            logger.error(f"Error in check_and_process: {e}", exc_info=True)
    
    async def _get_unprocessed_aggregates(self) -> List[AggregateInput]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∞–≥—Ä–µ–≥–∞—Ç—ã –±–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö NormalizedUnit."""
        from mindbank_poc.api.backends import jsonl_backend  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π backend
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∞–≥—Ä–µ–≥–∞—Ç—ã
        all_aggregates = await jsonl_backend.load_all_aggregates()
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ aggregate_id
        processed_ids = await self._get_processed_aggregate_ids()
        
        # 3. –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        unprocessed = [agg for agg in all_aggregates if agg.id not in processed_ids]
        
        logger.debug(f"Found {len(unprocessed)} unprocessed aggregates out of {len(all_aggregates)} total")
        return unprocessed
    
    async def _get_processed_aggregate_ids(self) -> Set[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö aggregate_id –∏–∑ NormalizedUnits."""
        all_units = await self.knowledge_store.list_all()
        processed_ids = {unit.aggregate_id for unit in all_units if unit.aggregate_id}
        logger.debug(f"Found {len(processed_ids)} processed aggregate IDs")
        return processed_ids
    
    def _should_run_normalization(self, pending_count: int) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é."""
        current_time = time.time()
        time_since_last_run = current_time - self.last_run_timestamp
        
        min_count_reached = pending_count >= self.min_pending
        timeout_reached = time_since_last_run >= self.timeout_sec
        
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ª–æ–≤–∏–π
        logger.debug(f"üìä Normalization conditions check:")
        logger.debug(f"  ‚Ä¢ Pending aggregates: {pending_count} (min required: {self.min_pending}) ‚Üí {'‚úÖ' if min_count_reached else '‚ùå'}")
        logger.debug(f"  ‚Ä¢ Time since last run: {time_since_last_run:.0f}s (timeout: {self.timeout_sec}s) ‚Üí {'‚úÖ' if timeout_reached else '‚ùå'}")
        logger.debug(f"  ‚Ä¢ Last run timestamp: {self.last_run_timestamp} ({datetime.fromtimestamp(self.last_run_timestamp) if self.last_run_timestamp > 0 else 'Never'})")
        
        should_run = min_count_reached or timeout_reached
        if should_run:
            logger.info(f"üöÄ WILL RUN normalization: pending={pending_count}, timeout_reached={timeout_reached}")
        
        return should_run
    
    def get_stats(self) -> Dict[str, any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –≤–æ—Ä–∫–µ—Ä–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        """
        return {
            "running": self._running,
            "enabled": self.enabled,
            "min_pending": self.min_pending,
            "timeout_sec": self.timeout_sec,
            "check_interval": self.check_interval,
            "last_run_timestamp": self.last_run_timestamp
        } 