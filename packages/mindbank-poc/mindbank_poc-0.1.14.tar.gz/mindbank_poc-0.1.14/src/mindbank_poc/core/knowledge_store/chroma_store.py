"""
–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ ChromaDB.
"""
import os
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, List, Tuple, Union, Set
import uuid
import asyncio
import aiofiles

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

from mindbank_poc.common.logging import get_logger
from mindbank_poc.core.config.settings import settings
from .base import BaseKnowledgeStore
from ..normalizer.models import NormalizedUnit
from ..enrichment.models import SegmentModel, ClusterModel

# –ü–æ–ª—É—á–∞–µ–º –ª–æ–≥–≥–µ—Ä
logger = get_logger(__name__)


class ChromaKnowledgeStore(BaseKnowledgeStore):
    """
    –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–Ω–∞–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ ChromaDB.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç ChromaDB –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —é–Ω–∏—Ç–æ–≤.
    –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å JSONL:
    - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ/–∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –≤ –ø–∞–º—è—Ç–∏
    - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    - –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Args:
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞, –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
                   - data_dir: –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
                   - collection_name: –∏–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ ChromaDB
        """
        super().__init__(config or {})
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        self.data_dir = Path(self.config.get("data_dir", settings.storage.knowledge_dir))
        self.data_dir.mkdir(exist_ok=True, parents=True)
        
        # –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        self.collection_name = self.config.get("collection_name", "normalized_units")
        
        # –ü—É—Ç—å –∫ ChromaDB
        self.chroma_path = self.data_dir / "chroma_db"
        self.chroma_path.mkdir(exist_ok=True, parents=True)
        
        # –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        logger.info(f"ChromaKnowledgeStore initialized. Data directory: {self.data_dir.resolve()}")
        logger.info(f"ChromaDB path: {self.chroma_path.resolve()}")
        logger.info(f"Collection name: {self.collection_name}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ ChromaDB (persistent)
        self.client = chromadb.PersistentClient(
            path=str(self.chroma_path.resolve()),
            settings=Settings(
                anonymized_telemetry=False
            )
        )
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        try:
            # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ None, —á—Ç–æ–±—ã –æ—Ç–∫–ª—é—á–∏—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            # –≠—Ç–æ –≤–∞–∂–Ω–æ, —Ç–∞–∫ –∫–∞–∫ –º—ã —Ö–æ—Ç–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞—à–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç OpenAI
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={"description": "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –∑–Ω–∞–Ω–∏–π"},
                embedding_function=None  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            )
            logger.info(f"ChromaKnowledgeStore: Collection '{self.collection_name}' ready")
        except Exception as e:
            logger.error(f"Error initializing ChromaDB collection: {e}", exc_info=True)
            raise
    
    async def store(self, unit: NormalizedUnit) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Args:
            unit: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã (unit.id)
        """
        try:
            # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è ChromaDB (–∏—Å–ø–æ–ª—å–∑—É–µ–º unit.id)
            doc_id = unit.id
            
            # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
            document = unit.text_repr
            
            # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            embedding = unit.vector_repr
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            metadata = {
                "unit_id": unit.id,
                "aggregate_id": unit.aggregate_id,
                "group_id": unit.group_id,
                "normalized_at": unit.normalized_at.isoformat(),
                "archetype": unit.archetype,  # üéØ –î–û–ë–ê–í–õ–Ø–ï–ú –ê–†–•–ï–¢–ò–ü!
                # –ö–æ–ø–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                **{f"class_{k}": str(v) for k, v in unit.classification.items()},
                # –ö–æ–ø–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —é–Ω–∏—Ç–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ)
                **{k: str(v) if not isinstance(v, (int, float, bool, str)) else v 
                   for k, v in unit.metadata.items() 
                   if v is not None}
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –∫–∞–∫ JSON –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            full_unit_json = unit.model_dump_json()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º ID
            try:
                existing = self.collection.get(ids=[doc_id])
                if existing and existing['ids']:
                    # –ï—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –æ–±–Ω–æ–≤–ª—è–µ–º
                    logger.info(f"Updating existing unit with ID {doc_id}")
                    self.collection.update(
                        ids=[doc_id],
                        embeddings=[embedding] if embedding else None,
                        metadatas=[metadata],
                        documents=[full_unit_json]
                    )
                else:
                    # –ï—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –¥–æ–±–∞–≤–ª—è–µ–º
                    logger.info(f"Adding new unit with ID {doc_id}")
                    self.collection.add(
                        ids=[doc_id],
                        embeddings=[embedding] if embedding else None,
                        metadatas=[metadata],
                        documents=[full_unit_json]
                    )
            except Exception as e:
                # –ï—Å–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–ª–ª–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞), –¥–æ–±–∞–≤–ª—è–µ–º
                logger.warning(f"Error checking existence, adding as new: {e}")
                self.collection.add(
                    ids=[doc_id],
                    embeddings=[embedding] if embedding else None,
                    metadatas=[metadata],
                    documents=[full_unit_json]
                )
            
            logger.info(f"Stored unit with ID {doc_id} in ChromaDB")
            return doc_id
        
        except Exception as e:
            logger.error(f"Error storing unit in ChromaDB: {e}", exc_info=True)
            raise
    
    async def get(self, unit_id: str) -> Optional[NormalizedUnit]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É.
        
        Args:
            unit_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –µ–¥–∏–Ω–∏—Ü—ã (unit.id)
            
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –µ–¥–∏–Ω–∏—Ü–∞ –∏–ª–∏ None, –µ—Å–ª–∏ –µ–¥–∏–Ω–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
        """
        try:
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID
            result = self.collection.get(ids=[unit_id], include=["documents"])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞–π–¥–µ–Ω –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç
            if not result or not result['documents'] or not result['documents'][0]:
                logger.debug(f"Unit with ID {unit_id} not found in ChromaDB")
                return None
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º JSON –≤ –æ–±—ä–µ–∫—Ç NormalizedUnit
            try:
                unit_json = result['documents'][0]
                unit = NormalizedUnit.model_validate_json(unit_json)
                return unit
            except Exception as e:
                logger.error(f"Error deserializing unit from ChromaDB: {e}", exc_info=True)
                return None
        
        except Exception as e:
            logger.error(f"Error retrieving unit from ChromaDB: {e}", exc_info=True)
            return None
    
    async def load_all(self) -> List[NormalizedUnit]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —é–Ω–∏—Ç—ã –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —é–Ω–∏—Ç–æ–≤
        """
        try:
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            result = self.collection.get(include=["documents", "metadatas"])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if not result or not result['documents']:
                logger.warning("No units found in ChromaDB")
                return []
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –∫–∞–∂–¥—ã–π JSON –≤ –æ–±—ä–µ–∫—Ç NormalizedUnit
            units = []
            metadatas = result.get('metadatas', [])
            
            for i, unit_json in enumerate(result['documents']):
                try:
                    if unit_json:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ JSON –Ω–µ –ø—É—Å—Ç–æ–π
                        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        metadata = metadatas[i] if i < len(metadatas) else {}
                        doc_type = metadata.get('doc_type', 'unit')  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º unit
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ units, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º segments –∏ –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã
                        if doc_type == 'unit' or doc_type is None:
                            unit = NormalizedUnit.model_validate_json(unit_json)
                            units.append(unit)
                        else:
                            logger.debug(f"Skipping document with doc_type '{doc_type}' in load_all")
                except Exception as e:
                    logger.error(f"Error deserializing unit from ChromaDB: {e}", exc_info=True)
            
            logger.info(f"Loaded {len(units)} units from ChromaDB")
            return units
        
        except Exception as e:
            logger.error(f"Error loading all units from ChromaDB: {e}", exc_info=True)
            return []
    
    async def delete(self, unit_id: str) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –ø–æ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É.
        
        Args:
            unit_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –µ–¥–∏–Ω–∏—Ü—ã (unit.id)
            
        Returns:
            True, –µ—Å–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
        """
        try:
            # –£–¥–∞–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ ID
            self.collection.delete(ids=[unit_id])
            logger.info(f"Deleted unit with ID {unit_id} from ChromaDB")
            return True
        
        except Exception as e:
            logger.error(f"Error deleting unit from ChromaDB: {e}", exc_info=True)
            return False
    
    async def delete_all(self) -> bool:
        """
        –£–¥–∞–ª—è–µ—Ç –≤—Å–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Returns:
            True, –µ—Å–ª–∏ —É–¥–∞–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –∏–Ω–∞—á–µ False
        """
        try:
            # –£–¥–∞–ª—è–µ–º –≤—Å—é –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏ —Å–æ–∑–¥–∞–µ–º –∑–∞–Ω–æ–≤–æ
            self.client.delete_collection(name=self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –∑–Ω–∞–Ω–∏–π"}
            )
            logger.info(f"Deleted all units from ChromaDB collection '{self.collection_name}'")
            return True
        
        except Exception as e:
            logger.error(f"Error deleting all units from ChromaDB: {e}", exc_info=True)
            return False
    
    async def search(
        self, 
        query_text: Optional[str] = None,
        where_filters: Optional[Dict[str, Any]] = None,
        limit: int = 10
    ) -> List[Tuple[NormalizedUnit, float]]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–∏—Å–∫ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø–æ —Ç–µ–∫—Å—Ç—É –∏/–∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
        
        Args:
            query_text: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞  
            where_filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (unit, score) —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏ –∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫—Ä–∏—Ç–µ—Ä–∏–π –ø–æ–∏—Å–∫–∞ (—Ç–µ–∫—Å—Ç –∏–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã)
            if not query_text and not where_filters:
                logger.warning("Search requires either query_text or where_filters")
                return []
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞, –µ—Å–ª–∏ –µ—Å—Ç—å query_text
            query_vector = None
            if query_text:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º embed_provider –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞
                # –ù—É–∂–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                from mindbank_poc.api.normalizers.config import load_config
                from mindbank_poc.core.normalizer.normalizer import Normalizer
                
                normalizer_config = load_config()
                normalizer = Normalizer(normalizer_config)
                embed_response = await normalizer.embed_provider.embed_text(query_text)
                
                # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –∫–∞–∫ .embedding, —Ç–∞–∫ –∏ .vector)
                if hasattr(embed_response, 'embedding'):
                    query_vector = embed_response.embedding
                elif hasattr(embed_response, 'vector'):
                    query_vector = embed_response.vector
                else:
                    # –ï—Å–ª–∏ response —Å–∞–º —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º
                    query_vector = embed_response if isinstance(embed_response, list) else None
                
                if query_vector is None:
                    logger.error("Failed to extract query vector from embed response")
                    return []
                    
                logger.info(f"Generated query vector with {len(query_vector)} dimensions")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ ChromaDB
            chroma_filter = None
            if where_filters:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç ChromaDB
                filter_conditions = []
                for key, value in where_filters.items():
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                    if key.startswith("metadata."):
                        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å metadata. –¥–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                        filter_key = key[9:]  # —É–¥–∞–ª—è–µ–º "metadata."
                    elif key in ["type", "topic", "category"]:
                        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–ª—è –∏–º–µ—é—Ç –ø—Ä–µ—Ñ–∏–∫—Å class_
                        filter_key = f"class_{key}"
                    else:
                        filter_key = key
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —É—Å–ª–æ–≤–∏–µ —Ñ–∏–ª—å—Ç—Ä–∞
                    filter_conditions.append({filter_key: value})
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∏–ª—å—Ç—Ä–∞ –¥–ª—è ChromaDB
                if len(filter_conditions) == 1:
                    # –û–¥–Ω–æ —É—Å–ª–æ–≤–∏–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
                    chroma_filter = filter_conditions[0]
                elif len(filter_conditions) > 1:
                    # –ù–µ—Å–∫–æ–ª—å–∫–æ —É—Å–ª–æ–≤–∏–π - –∏—Å–ø–æ–ª—å–∑—É–µ–º $and –æ–ø–µ—Ä–∞—Ç–æ—Ä
                    chroma_filter = {"$and": filter_conditions}
                else:
                    chroma_filter = None
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            if query_vector:
                # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä—É
                result = self.collection.query(
                    query_embeddings=[query_vector],
                    where=chroma_filter,
                    n_results=limit,
                    include=["documents", "distances", "metadatas"] # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∏—Å—Ç–∞–Ω—Ü–∏–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                )
            elif where_filters: # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–µ—Ç, –Ω–æ –µ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã
                # –¢–æ–ª—å–∫–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
                result = self.collection.get(
                    where=chroma_filter,
                    limit=limit,
                    include=["documents", "metadatas"] # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                )
            else: # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –∫–æ–Ω—Ç—Ä–æ–ª—å –≤—ã—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç
                return []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            # –î–ª—è query —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ result["documents"][0], –¥–ª—è get –≤ result["documents"]
            documents = result.get('documents')
            if not documents or (isinstance(documents, list) and not documents[0]):
                logger.warning("No results found in ChromaDB query")
                return []
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results_list = []
            docs_to_process = documents[0] if query_vector else documents
            distances_list = result.get('distances')[0] if query_vector and result.get('distances') else None
            metadatas_list = result.get('metadatas')[0] if query_vector and result.get('metadatas') else result.get('metadatas', [])
            
            for i, unit_json in enumerate(docs_to_process):
                try:
                    if unit_json:
                        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        metadata = metadatas_list[i] if i < len(metadatas_list) else {}
                        doc_type = metadata.get('doc_type', 'unit')
                        
                        if doc_type in ['segment', 'cluster']:
                            # –î–ª—è –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ–∑–¥–∞–µ–º "–ø—Å–µ–≤–¥–æ-unit"
                            unit = self._create_migrated_unit_wrapper(unit_json, metadata, doc_type)
                        else:
                            # –û–±—ã—á–Ω–∞—è –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–∏—Ö units
                            unit = NormalizedUnit.model_validate_json(unit_json)
                        
                        score = 0.99 # –°–∫–æ—Ä –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è get()
                        
                        if distances_list:
                            distance = float(distances_list[i])
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –¥–∏—Å—Ç–∞–Ω—Ü–∏—é [0, 2] –≤ —Å–∫–æ—Ä [0.99, ~0]
                            # similarity = 1.0 - (distance / 2.0) # –°—Ö–æ–¥—Å—Ç–≤–æ [0, 1]
                            # score = 0.99 * (similarity ** 0.8) # –ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                            score = max(0.01, 0.99 * (1.0 - (distance / 2.0))) # –õ–∏–Ω–µ–π–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
                            score = round(score, 2)
                            
                        results_list.append((unit, score))
                except Exception as e:
                    logger.error(f"Error deserializing search result from ChromaDB: {e}", exc_info=True)
            
            logger.info(f"Found {len(results_list)} results in ChromaDB")
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∫–æ—Ä—É, –µ—Å–ª–∏ –±—ã–ª –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            if query_vector:
                results_list.sort(key=lambda item: item[1], reverse=True)
                
            return results_list
        
        except Exception as e:
            logger.error(f"Error searching in ChromaDB: {str(e)}", exc_info=True)
            return []
    
    def _create_migrated_unit_wrapper(self, content: str, metadata: Dict[str, Any], doc_type: str) -> NormalizedUnit:
        """
        –°–æ–∑–¥–∞–µ—Ç –æ–±–µ—Ä—Ç–∫—É NormalizedUnit –¥–ª—è –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.
        
        Args:
            content: –¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–°–µ–≥–º–µ–Ω—Ç: ...")
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type: –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ ('segment' –∏–ª–∏ 'cluster')
            
        Returns:
            NormalizedUnit –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if doc_type == 'segment':
            unit_id = metadata.get('segment_id', f"migrated_segment_{id(content)}")
        elif doc_type == 'cluster':
            unit_id = metadata.get('cluster_id', f"migrated_cluster_{id(content)}")
        else:
            unit_id = f"migrated_{doc_type}_{id(content)}"
        
        # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é NormalizedUnit –æ–±–µ—Ä—Ç–∫—É
        from datetime import datetime
        
        unit_data = {
            "id": unit_id,
            "aggregate_id": f"migrated_{doc_type}_{unit_id}",
            "group_id": metadata.get('group_id', 'migrated'),
            "text_repr": content,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–∞–∫ text_repr
            "created_at": metadata.get('created_at', datetime.utcnow().isoformat()),
            "entity_metadata": {},
            "classifications": [],
            "archetype": doc_type,  # –ú–∞—Ä–∫–∏—Ä—É–µ–º –∫–∞–∫ segment –∏–ª–∏ cluster
            "source": metadata.get('source', 'migrated'),
            "importance_score": 0.5,
            "custom_metadata": {
                "migrated_type": doc_type,
                "original_content": content[:100] + "..." if len(content) > 100 else content
            }
        }
        
        return NormalizedUnit(**unit_data)

    async def list_group_ids(self) -> Set[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –≤—Å–µ—Ö group_id –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Returns:
            Set[str]: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö group_id
        """
        group_ids = set()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            result = self.collection.get(
                include=["metadatas"]
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º group_id –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if result and "metadatas" in result:
                for metadata in result["metadatas"]:
                    if metadata and "group_id" in metadata:
                        group_ids.add(metadata["group_id"])
            
            logger.debug(f"Found {len(group_ids)} unique group IDs in ChromaDB")
            return group_ids
            
        except Exception as e:
            logger.error(f"Failed to list group IDs from ChromaDB: {e}", exc_info=True)
            return set()
    
    async def list_by_group(self, group_id: str) -> List[NormalizedUnit]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –≥—Ä—É–ø–ø—ã.
        
        Args:
            group_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≥—Ä—É–ø–ø—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –µ–¥–∏–Ω–∏—Ü –≥—Ä—É–ø–ø—ã
        """
        try:
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≥—Ä—É–ø–ø—ã
            results = self.collection.get(
                where={"group_id": group_id},
                include=["documents", "metadatas"]
            )
            
            units = []
            if results and results['documents']:
                metadatas = results.get('metadatas', [])
                for i, doc_json in enumerate(results['documents']):
                    if doc_json:
                        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        metadata = metadatas[i] if i < len(metadatas) else {}
                        doc_type = metadata.get('doc_type', 'unit')  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º unit
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ units, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º segments –∏ –¥—Ä—É–≥–∏–µ —Ç–∏–ø—ã
                        if doc_type == 'unit' or doc_type is None:
                            try:
                                unit = NormalizedUnit.model_validate_json(doc_json)
                                units.append(unit)
                            except Exception as e:
                                logger.error(f"Error deserializing unit from group {group_id}: {e}")
                                # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                                continue
                        else:
                            logger.debug(f"Skipping document with doc_type '{doc_type}' in group {group_id}")
            
            logger.info(f"Found {len(units)} units for group {group_id} in ChromaDB")
            return units
            
        except Exception as e:
            logger.error(f"Error listing units by group from ChromaDB: {e}", exc_info=True)
            return []
    
    async def list_unprocessed_groups(self, min_units: int = 10) -> List[str]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≥—Ä—É–ø–ø —Å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —é–Ω–∏—Ç–∞–º–∏ (–±–µ–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤).
        
        Args:
            min_units: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —é–Ω–∏—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –≥—Ä—É–ø–ø
        """
        try:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ group_id –∏ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —é–Ω–∏—Ç—ã
            group_ids = await self.list_group_ids()
            qualified_groups = []
            
            for group_id in group_ids:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —é–Ω–∏—Ç–æ–≤ –≤ –≥—Ä—É–ø–ø–µ
                units = await self.list_by_group(group_id)
                if len(units) >= min_units:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã
                    segments = await self.list_segments_by_group(group_id)
                    if not segments:
                        qualified_groups.append(group_id)
            
            logger.info(f"Found {len(qualified_groups)} groups with >= {min_units} units and no segments")
            return qualified_groups
            
        except Exception as e:
            logger.error(f"Failed to list unprocessed groups: {e}", exc_info=True)
            return []
    
    async def store_aggregate(self, aggregate_id: str, aggregate_data: Dict[str, Any]) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–≥—Ä–µ–≥–∞—Ç –≤ ChromaDB.
        
        Args:
            aggregate_id: ID –∞–≥—Ä–µ–≥–∞—Ç–∞
            aggregate_data: –î–∞–Ω–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ç–∞ (—Å–ª–æ–≤–∞—Ä—å)
            
        Returns:
            ID —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∞–≥—Ä–µ–≥–∞—Ç–∞
        """
        try:
            # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è ChromaDB
            doc_id = f"aggregate_{aggregate_id}"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ JSON –¥–æ–∫—É–º–µ–Ω—Ç
            import json
            document = json.dumps(aggregate_data, ensure_ascii=False)
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            metadata = {
                "doc_type": "aggregate",
                "aggregate_id": aggregate_id,
                "group_id": aggregate_data.get("group_id", "unknown"),
                "created_at": aggregate_data.get("aggregated_at", ""),
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª–ª–µ–∫—Ü–∏—é –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (–∞–≥—Ä–µ–≥–∞—Ç—ã –Ω–µ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø–æ–∏—Å–∫–µ)
            self.collection.add(
                ids=[doc_id],
                documents=[document],
                metadatas=[metadata]
            )
            
            logger.debug(f"Stored aggregate {aggregate_id} in ChromaDB")
            return aggregate_id
            
        except Exception as e:
            logger.error(f"Error storing aggregate in ChromaDB: {e}", exc_info=True)
            raise

    async def get_aggregate(self, aggregate_id: str) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∞–≥—Ä–µ–≥–∞—Ç –∏–∑ ChromaDB.
        
        Args:
            aggregate_id: ID –∞–≥—Ä–µ–≥–∞—Ç–∞
            
        Returns:
            –ê–≥—Ä–µ–≥–∞—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        try:
            doc_id = f"aggregate_{aggregate_id}"
            result = self.collection.get(ids=[doc_id], include=["documents"])
            
            if not result or not result['documents'] or not result['documents'][0]:
                logger.debug(f"Aggregate with ID {aggregate_id} not found in ChromaDB")
                return None
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º JSON
            try:
                import json
                aggregate_json = result['documents'][0]
                aggregate_data = json.loads(aggregate_json)
                return aggregate_data
            except Exception as e:
                logger.error(f"Error deserializing aggregate from ChromaDB: {e}", exc_info=True)
                return None
                
        except Exception as e:
            logger.error(f"Error retrieving aggregate from ChromaDB: {e}", exc_info=True)
            return None

    async def get_original_aggregate(self, unit_id: str) -> Optional[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã.
        
        Args:
            unit_id: ID –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π –µ–¥–∏–Ω–∏—Ü—ã (unit.id)
            
        Returns:
            –ê–≥—Ä–µ–≥–∞—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é –µ–¥–∏–Ω–∏—Ü—É
            unit = await self.get(unit_id)
            if not unit:
                logger.debug(f"Unit {unit_id} not found (normal for migrated segments/clusters)")
                return None
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∞–≥—Ä–µ–≥–∞—Ç –∏–∑ ChromaDB
            aggregate_data = await self.get_aggregate(unit.aggregate_id)
            if aggregate_data:
                return aggregate_data
            
            # –§–æ–ª–±—ç–∫ –∫ JSONL backend –µ—Å–ª–∏ –∞–≥—Ä–µ–≥–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ ChromaDB
            logger.debug(f"Aggregate {unit.aggregate_id} not found in ChromaDB, trying JSONL backend")
            from mindbank_poc.api.backends import jsonl_backend
            aggregate = await jsonl_backend.load_aggregate_by_id(unit.aggregate_id)
            
            if aggregate:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–æ–¥–µ–ª—å AggregateInput –≤ —Å–ª–æ–≤–∞—Ä—å
                return aggregate.model_dump(mode="json")
            else:
                logger.warning(f"Original aggregate {unit.aggregate_id} not found for unit {unit_id}")
                return None
        except Exception as e:
            logger.error(f"Error loading original aggregate for unit {unit_id}: {e}")
            return None
    
    # –ú–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Å–µ–≥–º–µ–Ω—Ç–∞–º–∏
    async def store_segment(self, segment: SegmentModel) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Args:
            segment: –°–µ–≥–º–µ–Ω—Ç –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        """
        try:
            # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è ChromaDB
            doc_id = f"segment_{segment.id}"
            
            # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + —Ä–µ–∑—é–º–µ)
            document = f"{segment.title}\n\n{segment.summary}"
            
            # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            embedding = segment.vector_repr
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            metadata = {
                "doc_type": "segment",  # –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                "segment_id": segment.id,
                "group_id": segment.group_id,
                "created_at": segment.created_at.isoformat(),
                "entity_count": len(segment.entities),
                "unit_count": len(segment.raw_unit_ids),
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                **{f"entity_{i}": entity for i, entity in enumerate(segment.entities[:10])}
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –∫–∞–∫ JSON
            full_segment_json = segment.model_dump_json()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º upsert –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if embedding:
                self.collection.upsert(
                    ids=[doc_id],
                    embeddings=[embedding],
                    documents=[full_segment_json],
                    metadatas=[metadata]
                )
            else:
                self.collection.upsert(
                    ids=[doc_id],
                    documents=[full_segment_json],
                    metadatas=[metadata]
                )
            
            logger.info(f"Stored segment {segment.id} in ChromaDB")
            return segment.id
            
        except Exception as e:
            logger.error(f"Error storing segment in ChromaDB: {e}", exc_info=True)
            raise
    
    async def get_segment(self, segment_id: str) -> Optional[SegmentModel]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Args:
            segment_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞
            
        Returns:
            –°–µ–≥–º–µ–Ω—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        try:
            doc_id = f"segment_{segment_id}"
            result = self.collection.get(ids=[doc_id], include=["documents"])
            
            if not result or not result['documents'] or not result['documents'][0]:
                logger.warning(f"Segment with ID {segment_id} not found in ChromaDB")
                return None
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º JSON –≤ –æ–±—ä–µ–∫—Ç SegmentModel
            try:
                segment_json = result['documents'][0]
                segment = SegmentModel.model_validate_json(segment_json)
                return segment
            except Exception as e:
                logger.error(f"Error deserializing segment from ChromaDB: {e}", exc_info=True)
                return None
                
        except Exception as e:
            logger.error(f"Error retrieving segment from ChromaDB: {e}", exc_info=True)
            return None
    
    async def list_segments_by_group(self, group_id: str) -> List[SegmentModel]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –≥—Ä—É–ø–ø—ã.
        
        Args:
            group_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≥—Ä—É–ø–ø—ã
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –≥—Ä—É–ø–ø—ã
        """
        try:
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã –≥—Ä—É–ø–ø—ã
            results = self.collection.get(
                where={
                    "$and": [
                        {"doc_type": "segment"},
                        {"group_id": group_id}
                    ]
                },
                include=["documents"]
            )
            
            segments = []
            if results and results['documents']:
                for doc_json in results['documents']:
                    if doc_json:
                        try:
                            segment = SegmentModel.model_validate_json(doc_json)
                            segments.append(segment)
                        except Exception as e:
                            logger.error(f"Error deserializing segment: {e}")
            
            logger.info(f"Found {len(segments)} segments for group {group_id} in ChromaDB")
            return segments
            
        except Exception as e:
            logger.error(f"Error listing segments by group from ChromaDB: {e}", exc_info=True)
            return []
    
    async def get_segments_for_unit(self, unit_id: str) -> List[SegmentModel]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã, –≤ –∫–æ—Ç–æ—Ä—ã–µ –≤—Ö–æ–¥–∏—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —é–Ω–∏—Ç.
        
        Args:
            unit_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —é–Ω–∏—Ç–∞ (unit.id)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–µ–≥–º–µ–Ω—Ç–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –¥–∞–Ω–Ω—ã–π —é–Ω–∏—Ç
        """
        try:
            # –í ChromaDB –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ –∏—Å–∫–∞—Ç—å –ø–æ –º–∞—Å—Å–∏–≤—É raw_unit_ids,
            # –ø–æ—ç—Ç–æ–º—É –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            results = self.collection.get(
                where={"doc_type": "segment"},
                include=["documents"]
            )
            
            segments = []
            if results and results['documents']:
                for doc_json in results['documents']:
                    if doc_json:
                        try:
                            segment = SegmentModel.model_validate_json(doc_json)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å–µ–≥–º–µ–Ω—Ç –¥–∞–Ω–Ω—ã–π —é–Ω–∏—Ç
                            if unit_id in segment.raw_unit_ids:
                                segments.append(segment)
                        except Exception as e:
                            logger.error(f"Error deserializing segment: {e}")
            
            logger.debug(f"Found {len(segments)} segments containing unit {unit_id}")
            return segments
            
        except Exception as e:
            logger.error(f"Error getting segments for unit from ChromaDB: {e}", exc_info=True)
            return []

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
    async def store_cluster(self, cluster: ClusterModel) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ.
        
        Args:
            cluster: –ö–ª–∞—Å—Ç–µ—Ä –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        """
        try:
            # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è ChromaDB
            doc_id = f"cluster_{cluster.id}"
            
            # –¢–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ (–∑–∞–≥–æ–ª–æ–≤–æ–∫ + —Ä–µ–∑—é–º–µ + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
            keywords_text = ", ".join(cluster.keywords) if cluster.keywords else ""
            document = f"{cluster.title}\n\n{cluster.summary}\n\n–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {keywords_text}"
            
            # –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (—Ü–µ–Ω—Ç—Ä–æ–∏–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞)
            embedding = cluster.centroid if cluster.centroid else None
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            metadata = {
                "doc_type": "cluster",  # –¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                "cluster_id": cluster.id,
                "cluster_size": cluster.cluster_size,
                "created_at": cluster.created_at.isoformat(),
                "keyword_count": len(cluster.keywords),
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–µ 10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                **{f"keyword_{i}": keyword for i, keyword in enumerate(cluster.keywords[:10])}
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ–±—ä–µ–∫—Ç –∫–∞–∫ JSON
            full_cluster_json = cluster.model_dump_json()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º upsert –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if embedding:
                self.collection.upsert(
                    ids=[doc_id],
                    embeddings=[embedding],
                    documents=[full_cluster_json],
                    metadatas=[metadata]
                )
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –µ—Å–ª–∏ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –Ω–µ—Ç
                self.collection.upsert(
                    ids=[doc_id],
                    documents=[full_cluster_json],
                    metadatas=[metadata]
                )
            
            logger.info(f"Stored cluster {cluster.id} with {cluster.cluster_size} segments in ChromaDB")
            return cluster.id
            
        except Exception as e:
            logger.error(f"Error storing cluster in ChromaDB: {e}", exc_info=True)
            raise
    
    async def get_cluster(self, cluster_id: str) -> Optional[ClusterModel]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Args:
            cluster_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
            
        Returns:
            –ö–ª–∞—Å—Ç–µ—Ä –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        try:
            doc_id = f"cluster_{cluster_id}"
            result = self.collection.get(ids=[doc_id], include=["documents", "metadatas"])
            
            if not result or not result['documents'] or not result['documents'][0]:
                logger.warning(f"Cluster with ID {cluster_id} not found in ChromaDB")
                return None
            
            # –î–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º JSON –≤ –æ–±—ä–µ–∫—Ç ClusterModel
            try:
                cluster_json = result['documents'][0]
                cluster = ClusterModel.model_validate_json(cluster_json)
                return cluster
            except Exception as json_error:
                # –ï—Å–ª–∏ JSON –Ω–µ –ø–æ–ª—É—á–∏–ª—Å—è, —ç—Ç–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
                # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π ClusterModel –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                try:
                    metadata = result['metadatas'][0] if result['metadatas'] else {}
                    
                    cluster = ClusterModel(
                        id=cluster_id,
                        title=metadata.get('title', 'Migrated Cluster'),
                        summary=metadata.get('summary', ''),
                        keywords=metadata.get('keywords', '').split(',') if metadata.get('keywords') else [],
                        segment_ids=[],  # –ù–µ –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ —Å—Ç—Ä–æ–∫–∏
                        cluster_size=int(metadata.get('cluster_size', 0)),
                        centroid=[],  # –ù–µ –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ —Å—Ç—Ä–æ–∫–∏
                        created_at=metadata.get('created_at', ''),
                        updated_at=metadata.get('created_at', '')
                    )
                    logger.debug(f"Recovered migrated cluster {cluster_id} from metadata")
                    return cluster
                except Exception as e:
                    logger.error(f"Could not recover migrated cluster {cluster_id}: {e}")
                    return None
                
        except Exception as e:
            logger.error(f"Error retrieving cluster from ChromaDB: {e}", exc_info=True)
            return None
    
    async def list_clusters(self) -> List[ClusterModel]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏–∑ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        """
        try:
            # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            results = self.collection.get(
                where={"doc_type": "cluster"},
                include=["documents", "metadatas"]
            )
            
            clusters = []
            if results and results['documents']:
                for i, doc_content in enumerate(results['documents']):
                    if doc_content:
                        try:
                            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –¥–µ—Å–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–∞–∫ JSON
                            cluster = ClusterModel.model_validate_json(doc_content)
                            clusters.append(cluster)
                        except Exception as json_error:
                            # –ï—Å–ª–∏ JSON –Ω–µ –ø–æ–ª—É—á–∏–ª—Å—è, —ç—Ç–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
                            # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π ClusterModel –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                            try:
                                metadata = results['metadatas'][i] if results['metadatas'] and i < len(results['metadatas']) else {}
                                
                                # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π ClusterModel –¥–ª—è –º–∏–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                                cluster_id = metadata.get('cluster_id', f'unknown_{i}')
                                cluster = ClusterModel(
                                    id=cluster_id,
                                    title=metadata.get('title', 'Migrated Cluster'),
                                    summary=metadata.get('summary', ''),
                                    keywords=metadata.get('keywords', '').split(',') if metadata.get('keywords') else [],
                                    segment_ids=[],  # –ù–µ –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ —Å—Ç—Ä–æ–∫–∏
                                    cluster_size=int(metadata.get('cluster_size', 0)),
                                    centroid=[],  # –ù–µ –º–æ–∂–µ–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏–∑ —Å—Ç—Ä–æ–∫–∏
                                    created_at=metadata.get('created_at', ''),
                                    updated_at=metadata.get('created_at', '')
                                )
                                clusters.append(cluster)
                                logger.debug(f"Recovered migrated cluster {cluster_id} from metadata")
                            except Exception as e:
                                logger.warning(f"Could not recover migrated cluster {i}: {e}")
            
            logger.info(f"Found {len(clusters)} clusters in ChromaDB")
            return clusters
            
        except Exception as e:
            logger.error(f"Error listing clusters from ChromaDB: {e}", exc_info=True)
            return []
    
    async def get_clusters_for_segment(self, segment_id: str) -> List[ClusterModel]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã, –≤ –∫–æ—Ç–æ—Ä—ã–µ –≤—Ö–æ–¥–∏—Ç —É–∫–∞–∑–∞–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç.
        
        Args:
            segment_id: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–µ–≥–º–µ–Ω—Ç–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö –¥–∞–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
        """
        try:
            # –í ChromaDB –Ω–µ—Ç –ø—Ä—è–º–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ –∏—Å–∫–∞—Ç—å –ø–æ –º–∞—Å—Å–∏–≤—É segment_ids,
            # –ø–æ—ç—Ç–æ–º—É –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º
            results = self.collection.get(
                where={"doc_type": "cluster"},
                include=["documents"]
            )
            
            clusters = []
            if results and results['documents']:
                for doc_json in results['documents']:
                    if doc_json:
                        try:
                            cluster = ClusterModel.model_validate_json(doc_json)
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–ª–∞—Å—Ç–µ—Ä –¥–∞–Ω–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç
                            if segment_id in cluster.segment_ids:
                                clusters.append(cluster)
                        except Exception as e:
                            logger.error(f"Error deserializing cluster: {e}")
            
            logger.debug(f"Found {len(clusters)} clusters containing segment {segment_id}")
            return clusters
            
        except Exception as e:
            logger.error(f"Error getting clusters for segment from ChromaDB: {e}", exc_info=True)
            return []

    # New method: segments for cluster
    async def list_segments_by_cluster(self, cluster_id: str) -> List[SegmentModel]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –∫–ª–∞—Å—Ç–µ—Ä—É."""
        try:
            cluster = await self.get_cluster(cluster_id)
            if not cluster or not cluster.segment_ids:
                return []

            segment_ids_set = set(cluster.segment_ids)

            # –ë—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å: –µ—Å–ª–∏ segment_ids –Ω–µ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ, –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ id
            segments: List[SegmentModel] = []
            for sid in segment_ids_set:
                seg = await self.get_segment(sid)
                if seg:
                    segments.append(seg)
            return segments
        except Exception as e:
            logger.error(f"Error listing segments by cluster from ChromaDB: {e}", exc_info=True)
            return []

    def _group_meta_file(self):
        return self.data_dir / "group_segmentation_meta.json"

    async def get_group_segmentation_meta(self, group_id: str) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã (–∏–ª–∏ –ø—É—Å—Ç–æ–π dict)."""
        meta_file = self._group_meta_file()
        if not meta_file.exists():
            return {}
        async with aiofiles.open(meta_file, 'r', encoding='utf-8') as f:
            content = await f.read()
            if not content.strip():
                return {}
            try:
                meta = json.loads(content)
                return meta.get(group_id, {})
            except Exception:
                return {}

    async def set_group_segmentation_meta(self, group_id: str, last_segmented_at: str, last_segmented_unit_id: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –≥—Ä—É–ø–ø—ã."""
        meta_file = self._group_meta_file()
        meta = {}
        if meta_file.exists():
            async with aiofiles.open(meta_file, 'r', encoding='utf-8') as f:
                content = await f.read()
                if content.strip():
                    try:
                        meta = json.loads(content)
                    except Exception:
                        meta = {}
        meta[group_id] = {
            "last_segmented_at": last_segmented_at,
            "last_segmented_unit_id": last_segmented_unit_id
        }
        async with aiofiles.open(meta_file, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(meta, ensure_ascii=False, indent=2))

    async def get_groups_with_new_units_for_segmentation(self, min_units: int = 10) -> list:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥—Ä—É–ø–ø—ã, –≥–¥–µ –µ—Å—Ç—å –Ω–æ–≤—ã–µ —é–Ω–∏—Ç—ã –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏."""
        group_ids = await self.list_group_ids()
        result = []
        for group_id in group_ids:
            units = await self.list_by_group(group_id)
            if not units:
                continue
            units_sorted = sorted(units, key=lambda u: (u.normalized_at or getattr(u, 'stored_at', "")))
            last_unit = units_sorted[-1]
            meta = await self.get_group_segmentation_meta(group_id)
            last_segmented_unit_id = meta.get("last_segmented_unit_id")
            # –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –±—ã–ª–æ –∏–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ —é–Ω–∏—Ç—ã
            if not last_segmented_unit_id or any(u.id > last_segmented_unit_id for u in units_sorted):
                if len(units) >= min_units:
                    result.append(group_id)
        return result 

    async def filter_segments(
        self,
        group_id: Optional[str] = None,
        source: Optional[str] = None,
        source_name: Optional[str] = None,
        title_contains: Optional[str] = None,
        summary_contains: Optional[str] = None,
        entity_contains: Optional[str] = None,
        min_unit_count: Optional[int] = None,
        max_unit_count: Optional[int] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
        limit: int = 50,
        sort_by: str = "created_at",
        sort_order: str = "desc"
    ) -> List[SegmentModel]:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —Å–µ–≥–º–µ–Ω—Ç—ã –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º.
        
        Args:
            group_id: –§–∏–ª—å—Ç—Ä –ø–æ –≥—Ä—É–ø–ø–µ
            source: –§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É –¥–∞–Ω–Ω—ã—Ö (source_metadata.source)
            source_name: –§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (source_metadata.source_name)
            title_contains: –ü–æ–∏—Å–∫ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
            summary_contains: –ü–æ–∏—Å–∫ –≤ —Ä–µ–∑—é–º–µ (—á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
            entity_contains: –§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–ª–∏—á–∏—é —Å—É—â–Ω–æ—Å—Ç–∏
            min_unit_count: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —é–Ω–∏—Ç–æ–≤
            max_unit_count: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —é–Ω–∏—Ç–æ–≤
            date_from: –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç (ISO —Å—Ç—Ä–æ–∫–∞)
            date_to: –î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ (ISO —Å—Ç—Ä–æ–∫–∞)
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            sort_by: –ü–æ–ª–µ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
            sort_order: –ü–æ—Ä—è–¥–æ–∫ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ (asc/desc)
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –∑–∞–≥—Ä—É–∑–∫—É –≤—Å–µ—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
            # –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            if group_id:
                # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –≥—Ä—É–ø–ø–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥
                segments = await self.list_segments_by_group(group_id)
            else:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Å–µ–≥–º–µ–Ω—Ç—ã
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ç–∏–ø–æ–º segment
                    results = self.collection.get(include=["documents", "metadatas"])
                    
                    segments = []
                    if results and results['documents']:
                        metadatas = results.get('metadatas', [])
                        
                        for i, doc_json in enumerate(results['documents']):
                            if doc_json:
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                                metadata = metadatas[i] if i < len(metadatas) else {}
                                doc_type = metadata.get('doc_type', 'unit')
                                
                                if doc_type == 'segment':
                                    try:
                                        segment = SegmentModel.model_validate_json(doc_json)
                                        segments.append(segment)
                                    except Exception as e:
                                        logger.error(f"Error deserializing segment: {e}")
                                        continue
                except Exception as e:
                    logger.error(f"Error loading segments from ChromaDB: {e}")
                    return []
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            filtered_segments = []
            for segment in segments:
                # –§–∏–ª—å—Ç—Ä –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫—É
                if source:
                    segment_source = segment.metadata.get('source_metadata', {}).get('source', '')
                    if source.lower() not in segment_source.lower():
                        continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                if source_name:
                    segment_source_name = segment.metadata.get('source_metadata', {}).get('source_name', '')
                    if source_name.lower() not in segment_source_name.lower():
                        continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                if title_contains and title_contains.lower() not in segment.title.lower():
                    continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–∑—é–º–µ
                if summary_contains and summary_contains.lower() not in segment.summary.lower():
                    continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ —Å—É—â–Ω–æ—Å—Ç–∏
                if entity_contains:
                    if not any(entity_contains.lower() in entity.lower() for entity in segment.entities):
                        continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —é–Ω–∏—Ç–æ–≤
                unit_count = len(segment.raw_unit_ids)
                if min_unit_count is not None and unit_count < min_unit_count:
                    continue
                if max_unit_count is not None and unit_count > max_unit_count:
                    continue
                
                # –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ
                if date_from:
                    try:
                        from datetime import datetime
                        date_from_dt = datetime.fromisoformat(date_from.replace('Z', '+00:00'))
                        if segment.created_at < date_from_dt:
                            continue
                    except ValueError:
                        logger.warning(f"Invalid date_from format: {date_from}")
                
                if date_to:
                    try:
                        from datetime import datetime
                        date_to_dt = datetime.fromisoformat(date_to.replace('Z', '+00:00'))
                        if segment.created_at > date_to_dt:
                            continue
                    except ValueError:
                        logger.warning(f"Invalid date_to format: {date_to}")
                
                filtered_segments.append(segment)
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            if sort_by == "created_at":
                filtered_segments.sort(key=lambda s: s.created_at, reverse=(sort_order == "desc"))
            elif sort_by == "title":
                filtered_segments.sort(key=lambda s: s.title.lower(), reverse=(sort_order == "desc"))
            elif sort_by == "unit_count":
                filtered_segments.sort(key=lambda s: len(s.raw_unit_ids), reverse=(sort_order == "desc"))
            else:
                # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                filtered_segments.sort(key=lambda s: s.created_at, reverse=(sort_order == "desc"))
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            result = filtered_segments[:limit]
            
            logger.info(f"Filtered segments: found {len(result)} of {len(segments)} total")
            return result
            
        except Exception as e:
            logger.error(f"Error filtering segments: {e}", exc_info=True)
            return [] 