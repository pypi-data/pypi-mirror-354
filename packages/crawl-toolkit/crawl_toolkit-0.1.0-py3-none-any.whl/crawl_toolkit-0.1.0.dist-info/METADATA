Metadata-Version: 2.4
Name: crawl-toolkit
Version: 0.1.0
Summary: Universal package for BrightData web scraping and crawling. And making keyword research easier.
Author-email: Patryk Samulewicz <p.samulewicz@senuto.com>
License-Expression: MIT
Requires-Python: >=3.8
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: beautifulsoup4>=4.12.0
Requires-Dist: lxml>=5.1.0
Requires-Dist: openai>=1.12.0
Requires-Dist: pydantic>=2.6.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: requests>=2.31.0
Provides-Extra: dev
Requires-Dist: black>=24.1.0; extra == 'dev'
Requires-Dist: isort>=5.13.0; extra == 'dev'
Requires-Dist: mypy>=1.8.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.23.0; extra == 'dev'
Requires-Dist: pytest>=8.0.0; extra == 'dev'
Description-Content-Type: text/markdown

# Python Crawl Toolkit

Universal package for BrightData web scraping and crawling. And making keyword research easier.

## Installation

```bash
pip install crawl-toolkit
```

## Features

- Web scraping with BrightData integration
- Keyword research tools
- OpenAI integration for content analysis
- Asynchronous crawling support
- BeautifulSoup4 and lxml for HTML parsing
- Type hints and validation with Pydantic

## Requirements

- Python 3.8+
- BrightData account and credentials
- OpenAI API key

## Usage

```python
from crawl_toolkit import CrawlToolkit

# Initialize the toolkit
toolkit = CrawlToolkit(
    brightdata_username="your_username",
    brightdata_password="your_password",
    openai_api_key="your_openai_key"
)

# Crawl a website
result = await toolkit.crawl(
    url="https://example.com",
    max_pages=10
)

# Analyze content
analysis = await toolkit.analyze_content(result.content)
```

## Development

1. Clone the repository
2. Install development dependencies:
```bash
pip install -e ".[dev]"
```
3. Run tests:
```bash
pytest
```

## License

MIT License 