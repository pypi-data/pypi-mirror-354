{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Downscale Skill\n",
    "\n",
    "This example shows how to evaluate Salient's daily downscaled forecasts and calculate meaningful metrics. It demonstrates [validation best practices](https://salientpredictions.notion.site/Validation-0220c48b9460429fa86f577914ea5248) such as:\n",
    "\n",
    "- Proper scoring using the Ensemble Continuous Ranked Probability Score (CRPS)\n",
    "  - Considers the full forecast distribution to reward both accuracy and precision\n",
    "  - Less sensitive to climatology decisions than metrics like Anomaly Correlation\n",
    "- A long backtesting period (2015-2022)\n",
    "  - Short evaluation periods are subject to noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7fe499027f90>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "# Prevent wrapping on tables for readability\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "sk.set_file_destination(\"validate_ensemble_example\")\n",
    "sk.login(\"username\", \"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize The Validation\n",
    "\n",
    "This notebook is written flexibly so you have the option of validating Salient and other forecasts multiple ways. These variables will control what, when, and how the validation proceeds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast dates to test:\n",
      "['2021-04-21', '2021-05-19', '2021-06-16', '2021-07-21']\n"
     ]
    }
   ],
   "source": [
    "# 1. The meteorological variable that we'll be evaluating:\n",
    "vars = [\"temp\", \"precip\"]  # wspd, tsi\n",
    "\n",
    "# 2. Number of days in the downscale:\n",
    "length = 35  # fast 35-day evaluation vs gefs (also 35 days)\n",
    "# length = 366  # comprehensive full-year evaluation\n",
    "\n",
    "# 3. Debias temp, precip, and wind\n",
    "debias = False  # Evaluate vs ERA5\n",
    "# debias = True  # Evaluate vs observations from GHCNd\n",
    "\n",
    "# 4. Set the date range to test over.\n",
    "(start_date, end_date) = (\"2021-04-01\", \"2021-07-31\")  # fast \"sample\" dataset\n",
    "# (start_date, end_date) = (\"2015-01-01\", \"2022-12-31\")  # out-of-sample \"test\" set\n",
    "# (start_date, end_date) = (\"2000-01-01\", \"2022-12-31\")  # comprehensive \"all-history\" set\n",
    "\n",
    "# 5. The reference model to compare Salient blend to\n",
    "# ref_model = \"none\"  # skip the reference model comparison\n",
    "ref_model = \"noaa_gefs\"  # good for daily-frequency 35-day comparisons\n",
    "# ref_model = \"noaa_gfs\"  # hourly-frequency comparisons\n",
    "\n",
    "# 6. Variable to focus on for plots\n",
    "plot_var = \"temp\"\n",
    "# plot_var = \"precip\"\n",
    "assert plot_var in vars\n",
    "\n",
    "\n",
    "# ===== Additional shared variables ==========================\n",
    "# Not recommended to change these.\n",
    "\n",
    "# For specialized use only, Salient can manually provide a \"bulk downscale\" zarr\n",
    "# bulk = os.path.join(sk.get_file_destination(), \"bulk_downscale\")  # zarr directory\n",
    "bulk = None  # don't use bulk downscale (default)\n",
    "\n",
    "# Temporal resolution of the downscaled & historical timeseries:\n",
    "freq = \"daily\"\n",
    "# freq = \"hourly\"\n",
    "\n",
    "# Caching strategy:\n",
    "force = False  # Cache data to save on repeat API calls\n",
    "# force = True  # Repeat API calls, even if data exists\n",
    "\n",
    "# Make all figures have a consistent size:\n",
    "figsize = (8, 5)\n",
    "\n",
    "# Determine which dates for which to request forecasts:\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "# GEFS is unavailable for 2020 January-September\n",
    "if \"gefs\" in ref_model:\n",
    "    date_range = date_range[~((date_range.year == 2020) & (date_range.month <= 9))]\n",
    "\n",
    "# Find first Wednesday on or after 16th of each month\n",
    "date_range = (\n",
    "    date_range[(date_range.dayofweek == 2) & (date_range.day >= 16)]\n",
    "    .to_series()\n",
    "    .groupby([lambda x: x.year, lambda x: x.month])\n",
    "    .first()\n",
    "    .dt.strftime(\"%Y-%m-%d\")\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"Forecast dates to test:\")\n",
    "print(date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Area of Interest\n",
    "\n",
    "The Salient SDK uses a \"Location\" object to specify the geographic bounds of a request. In this case, we will be validating against the vector of airport locations that are used to settle the Chicago Mercantile Exchange's Cooling and Heating Degree Day contracts. With `load_location_file` we can see that the file contains:\n",
    "\n",
    "- `lat` / `lon`: latitude and longitude of the met station, standard for a `location_file`\n",
    "- `name`: the 3-letter IATA airport code of the location, also `location_file` standard\n",
    "- `ghcnd`: the global climate network ID of the station, used to validate against observations. To customize this analysis for any set of observation stations, use the NCEI [stations list](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt).\n",
    "- `cme`: the CME code for the location used to create CDD/HDD strip codes.\n",
    "- `description`: full name of the airport\n",
    "\n",
    "If you have a list of locations already defined in a separate CSV file, you can use [`upload_file`](https://sdk.salientpredictions.com/api/#salientsdk.upload_file) to upload the file directly without building it in code via `upload_location_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lat        lon name        ghcnd cme                description                     geometry\n",
      "0   33.62972  -84.44224  ATL  USW00013874   1         Atlanta Hartsfield   POINT (-84.44224 33.62972)\n",
      "1   42.36057  -71.00975  BOS  USW00014739   W               Boston Logan   POINT (-71.00975 42.36057)\n",
      "2   34.19966 -118.36543  BUR  USW00023152   P  Burbank-Glendale-Pasadena  POINT (-118.36543 34.19966)\n",
      "3   41.96017  -87.93164  ORD  USW00094846   2             Chicago O'Hare   POINT (-87.93164 41.96017)\n",
      "4   39.04443  -84.67241  CVG  USW00093814   3     Cincinnati (Covington)   POINT (-84.67241 39.04443)\n",
      "5   32.89744  -97.02196  DFW  USW00003927   5          Dallas-Fort Worth   POINT (-97.02196 32.89744)\n",
      "6   29.98438  -95.36072  IAH  USW00012960   R        Houston-George Bush   POINT (-95.36072 29.98438)\n",
      "7   36.07190 -115.16343  LAS  USW00023169   0         Las Vegas McCarran   POINT (-115.16343 36.0719)\n",
      "8   44.88523  -93.23133  MSP  USW00014922   Q         Minneapolis-StPaul   POINT (-93.23133 44.88523)\n",
      "9   40.77945  -73.88027  LGA  USW00014732   4        New York La Guardia   POINT (-73.88027 40.77945)\n",
      "10  39.87326  -75.22681  PHL  USW00013739   6               Philadelphia   POINT (-75.22681 39.87326)\n",
      "11  45.59578 -122.60919  PDX  USW00024229   7                   Portland  POINT (-122.60919 45.59578)\n",
      "12  38.50659 -121.49604  SAC  USW00023232   S            Sacramento Exec  POINT (-121.49604 38.50659)\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "loc = sk.Location(location_file=sk.upload_location_file(\n",
    "    lats =[33.62972     ,      42.36057,      34.19966,      41.96017,      39.04443,      32.89744,      29.98438,      36.07190,      44.88523,      40.77945,      39.87326,      45.59578,      38.50659],\n",
    "    lons =[-84.44224    ,     -71.00975,    -118.36543,     -87.93164,     -84.67241,     -97.02196,     -95.36072,    -115.16343,     -93.23133,     -73.88027,     -75.22681,    -122.60919,    -121.49604],\n",
    "    names=[\"ATL\"        ,         \"BOS\",         \"BUR\",         \"ORD\",         \"CVG\",         \"DFW\",         \"IAH\",         \"LAS\",         \"MSP\",         \"LGA\",         \"PHL\",         \"PDX\",         \"SAC\"],\n",
    "    ghcnd=[\"USW00013874\", \"USW00014739\", \"USW00023152\", \"USW00094846\", \"USW00093814\", \"USW00003927\", \"USW00012960\", \"USW00023169\", \"USW00014922\", \"USW00014732\", \"USW00013739\", \"USW00024229\", \"USW00023232\"],\n",
    "    cme  =[\"1\"          ,           \"W\",           \"P\",           \"2\",           \"3\",           \"5\",           \"R\",           \"0\",           \"Q\",           \"4\",           \"6\",           \"7\",           \"S\"],\n",
    "    geoname=\"cmeus\",\n",
    "    force=force,\n",
    "    description=[\"Atlanta Hartsfield\", \"Boston Logan\", \"Burbank-Glendale-Pasadena\", \"Chicago O'Hare\", \"Cincinnati (Covington)\",\"Dallas-Fort Worth\", \"Houston-George Bush\", \"Las Vegas McCarran\", \"Minneapolis-StPaul\", \"New York La Guardia\",\"Philadelphia\", \"Portland\", \"Sacramento Exec\"],\n",
    "))\n",
    "# fmt: on\n",
    "stations = loc.load_location_file()\n",
    "print(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Data\n",
    "\n",
    "To calculate forecast skill, we will want to compare forecasts made in the past with actuals. There are two flavors of actual data: (1) The ERA5 reanalysis dataset and (2) point weather station observations.\n",
    "\n",
    "Salient's forecast natively predicts (1) ERA5, but contains a debiasing function to remove bias between ERA5 and (2) station observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical ERA5 or Observed Data\n",
    "\n",
    "Download daily or hourly historical values from [`data_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.data_timeseries) or `get_ghcnd` and then aggregate to match the forecasts, so that we can ensure that all forecasts use the same dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility function: get_ghcnd\n",
    "\n",
    "When validating against observations you can use any observation source that you have access to. For this example, we'll pull observation station data from NCEI. The main requirement is that the data be tabular, daily in frequency, and contain a column with the same name as `var`.\n",
    "\n",
    "Salient's `debias` forecasts are debiased vs GHCN and other public data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_ghcnd(\n",
    "    ghcnd_id: str | Iterable[str],\n",
    "    start_date: str = \"2000-01-01\",\n",
    "    xdd: float = (65 - 32) * 5 / 9,\n",
    "    destination: str = \"-default\",\n",
    "    force: bool = False,\n",
    ") -> pd.DataFrame | list[pd.DataFrame]:\n",
    "    \"\"\"Get GHCNd observed data timeseries for a single station.\n",
    "\n",
    "    Global Historical Climatology Network - Daily.\n",
    "\n",
    "    Args:\n",
    "        ghcnd_id (str | list[str]): GHCND station ID or list of IDs\n",
    "        start_date (str): omit data before this date\n",
    "        xdd (float): base temperature for heating/cooling degree days, in degC.\n",
    "        destination (str): The directory to download the data to\n",
    "        force (bool): if True (default False), force update of observed data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | list[pd.DataFrame]: observed data timeseries with\n",
    "        columns `time`, `precip`, `wspd`, `tmin`, `tmax`, `tavg`, `hdd` and `cdd`\n",
    "        in units `degC`.  Also meta-data columns for `ghcn_id`, `lat`, `lon`, `elev`, and `name`.\n",
    "        Will return a list of DataFrames if wban_id is iterable.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> obs_single = get_ghcnd(\"USW00013874\")\n",
    "        >>> obs_vector = get_ghcnd([\"USW00013874\", \"USW00014739\"])\n",
    "    \"\"\"\n",
    "    if isinstance(ghcnd_id, Iterable) and not isinstance(ghcnd_id, str):\n",
    "        return [get_ghcnd(single_id, start_date, xdd, force) for single_id in ghcnd_id]\n",
    "\n",
    "    file_name = os.path.join(sk.get_file_destination(), f\"{ghcnd_id}.csv\")\n",
    "    if force or not os.path.exists(file_name):\n",
    "        GHCND_ROOT = \"https://www.ncei.noaa.gov\"\n",
    "        GHCND_DIR = \"data/global-historical-climatology-network-daily/access\"\n",
    "        ghcnd_url = f\"{GHCND_ROOT}/{GHCND_DIR}/{ghcnd_id}.csv\"\n",
    "        r = requests.get(ghcnd_url)\n",
    "        r.raise_for_status()\n",
    "        with open(file_name, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "    # Gusts: WSF1, WSF2, WSF5 are fastest 1-min, 2-min, 5-sec wind speed\n",
    "    # Humidity: RHAV, RHMN, RHMX\n",
    "    keep_cols = {\n",
    "        \"STATION\": \"ghcnd_id\",\n",
    "        \"DATE\": \"time\",\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"ELEVATION\": \"elev\",  # meters\n",
    "        \"NAME\": \"name\",\n",
    "        \"TMAX\": \"tmax\",\n",
    "        \"TMIN\": \"tmin\",\n",
    "        \"PRCP\": \"precip\",\n",
    "        \"TAVG\": \"temp\",\n",
    "        \"AWND\": \"wspd\",\n",
    "    }\n",
    "\n",
    "    obs = pd.read_csv(file_name, usecols=keep_cols.keys())\n",
    "    obs.rename(columns=keep_cols, inplace=True)\n",
    "\n",
    "    # ncei uses 9999 for missing data\n",
    "    INVALID_NUMBER = 9999\n",
    "    obs.replace(INVALID_NUMBER, pd.NA, inplace=True)\n",
    "\n",
    "    # ncei uses 10ths of values\n",
    "    columns_to_decimalize = [\"precip\", \"tmax\", \"tmin\", \"wspd\", \"temp\"]\n",
    "    for col in columns_to_decimalize:\n",
    "        if col in obs.columns:\n",
    "            obs[col] = obs[col] / 10.0\n",
    "\n",
    "    obs = obs[obs[\"time\"] >= start_date]\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"])\n",
    "\n",
    "    # Only calculate degree days if both tmin and tmax are available\n",
    "    if \"tmin\" in obs.columns and \"tmax\" in obs.columns:\n",
    "        # Note that these long_names are identical to what data_timeseries returns:\n",
    "        obs[\"tmax\"].attrs[\"units\"] = \"degC\"\n",
    "        obs[\"tmax\"].attrs[\"long_name\"] = \"2 metre daily temperature\"\n",
    "\n",
    "        obs[\"tmin\"].attrs[\"units\"] = \"degC\"\n",
    "        obs[\"tmin\"].attrs[\"long_name\"] = \"2 metre daily temperature\"\n",
    "\n",
    "        # HDD & CDD settle off the mean of tmin/tmax, not the daily mean\n",
    "        temp = pd.concat([obs[\"tmin\"], obs[\"tmax\"]], axis=1).mean(axis=1)\n",
    "        obs[\"cdd\"] = (temp - xdd).clip(lower=0).round(1)\n",
    "        obs[\"hdd\"] = (xdd - temp).clip(lower=0).round(1)\n",
    "\n",
    "        # Some stations such as USW00023152 don't record temp but do have tmin/tmax.\n",
    "        # Replace missing temp with mean(tmin,tmax) as an approximation.\n",
    "        temp_na_mask = obs[\"temp\"].isna()\n",
    "        obs.loc[temp_na_mask, \"temp\"] = temp[temp_na_mask]\n",
    "\n",
    "        obs[\"hdd\"].attrs[\"units\"] = \"HDD day-1 (degC)\"\n",
    "        obs[\"hdd\"].attrs[\"long_name\"] = \"Heating Degree Days\"\n",
    "\n",
    "        obs[\"cdd\"].attrs[\"units\"] = \"CDD day-1 (degC)\"\n",
    "        obs[\"cdd\"].attrs[\"long_name\"] = \"Cooling Degree Days\"\n",
    "\n",
    "    obs[\"temp\"].attrs[\"units\"] = \"degC\"\n",
    "    obs[\"temp\"].attrs[\"long_name\"] = \"2 metre temperature\"\n",
    "\n",
    "    obs[\"precip\"].attrs[\"units\"] = \"mm day-1\"\n",
    "    obs[\"precip\"].attrs[\"long_name\"] = \"Total precipitation\"\n",
    "\n",
    "    obs[\"wspd\"].attrs[\"units\"] = \"m s**-1\"\n",
    "    obs[\"wspd\"].attrs[\"long_name\"] = \"Wind Speed\"\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request Historical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 36kB\n",
      "Dimensions:   (time: 163, location: 13)\n",
      "Coordinates:\n",
      "  * time      (time) datetime64[ns] 1kB 2021-03-27 2021-03-28 ... 2021-09-05\n",
      "  * location  (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat       (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon       (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "Data variables:\n",
      "    temp      (time, location) float64 17kB 21.77 10.66 14.5 ... 21.44 24.16\n",
      "    precip    (time, location) float64 17kB 4.033 0.0 0.0 ... 3.343 0.01152 0.0\n"
     ]
    }
   ],
   "source": [
    "if debias:\n",
    "    # Validate vs observations\n",
    "\n",
    "    # Use get_ghcnd, or access your own proprietary observations:\n",
    "    assert freq == \"daily\", \"ghcnd only available for daily temporal frequency\"\n",
    "    obs_df = get_ghcnd(stations.ghcnd, start_date=start_date, force=force)\n",
    "\n",
    "    # Convert observation tables into a dataset that looks like data_timeseries:\n",
    "    hist = sk.observed.make_observed_ds(\n",
    "        obs_df=obs_df,  # a DataFrame or vector of DataFrames\n",
    "        name=stations.name,  # this will populate the location coordinate\n",
    "        variable=vars,  # make sure that the DataFrame(s) in obs_df contain this column name\n",
    "        time_col=\"time\",  # the name of the column in obs_df containing the date\n",
    "    )\n",
    "elif bulk:\n",
    "    # Use a Salient-provided bulk downscale zarr\n",
    "    hist = xr.open_zarr(bulk)\n",
    "    truth_vars = [var for var in hist.data_vars if \"truth\" in var]\n",
    "    hist = hist[truth_vars].rename({var: var.replace(\"_truth\", \"\") for var in truth_vars})\n",
    "else:\n",
    "    # Validate vs ERA5\n",
    "    hist = sk.load_multihistory(\n",
    "        sk.data_timeseries(\n",
    "            loc=loc,\n",
    "            variable=vars,\n",
    "            field=\"vals\",\n",
    "            start=np.datetime64(start_date) - np.timedelta64(5, \"D\"),\n",
    "            end=np.datetime64(end_date) + np.timedelta64(length + 1, \"D\"),\n",
    "            frequency=freq,\n",
    "            verbose=False,\n",
    "            force=force,\n",
    "        )\n",
    "    )\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downscale the Salient Forecast\n",
    "\n",
    "The [`downscale`](https://sdk.salientpredictions.com/api/#salientsdk.downscale) API endpoint and SDK function converts Salient's native weekly/monthly/quarterly probabilistic forecasts into a daily or hourly ensemble timeseries.\n",
    "\n",
    "This is the most heavyweight call in the notebook, since it's getting multiple historical forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name        date\n",
      "0  validate_ensemble_example/downscale_d1f38fc0e6...  2021-04-21\n",
      "1  validate_ensemble_example/downscale_47d128c2a7...  2021-05-19\n",
      "2  validate_ensemble_example/downscale_3156cfdf5f...  2021-06-16\n",
      "3  validate_ensemble_example/downscale_f83414a94d...  2021-07-21\n"
     ]
    }
   ],
   "source": [
    "if bulk:\n",
    "    # Use a Salient-provided bulk downscale zarr\n",
    "    fcst = xr.open_zarr(bulk)\n",
    "    fcst_vars = [var for var in hist.data_vars if \"truth\" not in var]\n",
    "    fcst = fcst[fcst_vars]\n",
    "else:\n",
    "    fcst = sk.downscale(\n",
    "        loc=loc,\n",
    "        variables=vars,\n",
    "        debias=debias,\n",
    "        date=date_range,\n",
    "        frequency=freq,\n",
    "        length=length,\n",
    "        verbose=False,\n",
    "        force=force,\n",
    "        strict=False,  # if one downscale call fails, proceed with others\n",
    "    )\n",
    "    # Check to see if there are any missing forecasts:\n",
    "    fcst_na = fcst[fcst[\"file_name\"].isna()]\n",
    "    if not fcst_na.empty:\n",
    "        print(\"Missing forecast dates:\")\n",
    "        print(fcst_na)\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_source = (\n",
    "    None\n",
    "    if ref_model == \"none\"  # skip reference comparisons\n",
    "    else sk.forecast_timeseries(\n",
    "        loc=loc,\n",
    "        variable=vars,\n",
    "        date=date_range,\n",
    "        field=\"vals_ens\",\n",
    "        model=ref_model,\n",
    "        timescale=freq,\n",
    "        strict=False,\n",
    "        force=force,\n",
    "        verbose=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name        date\n",
      "0  validate_ensemble_example/forecast_timeseries_...  2021-04-21\n",
      "1  validate_ensemble_example/forecast_timeseries_...  2021-05-19\n",
      "2  validate_ensemble_example/forecast_timeseries_...  2021-06-16\n",
      "3  validate_ensemble_example/forecast_timeseries_...  2021-07-21\n"
     ]
    }
   ],
   "source": [
    "def format_as_downscale(ref: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reformat single-variable forecast_timeseries to match downscale's format.\n",
    "\n",
    "    Changes:\n",
    "    - Renames 'lead' dimension to 'forecast_day'\n",
    "    - Reorders dimensions to (ensemble, forecast_day, location)\n",
    "    - Converts forecast_day values to datetime using forecast_date + lead\n",
    "    \"\"\"\n",
    "    if ref is None:\n",
    "        return None\n",
    "\n",
    "    def process_date(date, group):\n",
    "        out_file = os.path.join(sk.get_file_destination(), f\"forecast_timeseries_ref_{date}.nc\")\n",
    "\n",
    "        # Load first dataset to get forecast_date\n",
    "        first_ds = xr.load_dataset(group.iloc[0].file_name)\n",
    "        forecast_date = first_ds.forecast_date\n",
    "\n",
    "        # Create dataset with reordered dimensions\n",
    "        ds = xr.Dataset(\n",
    "            {\n",
    "                row.variable: xr.load_dataset(row.file_name)\n",
    "                .vals_ens.transpose(\"ensemble\", \"lead\", \"location\")\n",
    "                .assign_attrs(xr.load_dataset(row.file_name).attrs)\n",
    "                for _, row in group.iterrows()\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Convert lead to forecast_day\n",
    "        ds = ds.rename({\"lead\": \"forecast_day\"})\n",
    "        ds[\"forecast_day\"] = forecast_date + ds.forecast_day\n",
    "\n",
    "        ds.to_netcdf(out_file, encoding={\"location\": {\"dtype\": str}})\n",
    "        return {\"file_name\": out_file, \"date\": date}\n",
    "\n",
    "    result_files = [process_date(date, group) for date, group in ref.groupby(\"date\")]\n",
    "    return pd.DataFrame(result_files)\n",
    "\n",
    "\n",
    "ref = format_as_downscale(ref_source)\n",
    "print(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Skill Metrics\n",
    "\n",
    "Compare the forecast and ERA5 datasets to see how well they match. Here we will calculate the same \"Continuous Ranked Probability Score\" that resulted from the call to `hindcast_summary` earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 37kB\n",
      "Dimensions:          (forecast_date: 4, lead: 35, location: 13)\n",
      "Coordinates:\n",
      "  * location         (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PDX' 'SAC'\n",
      "    lat              (location) float64 104B 33.63 42.36 34.2 ... 45.6 38.51\n",
      "    lon              (location) float64 104B -84.44 -71.01 ... -122.6 -121.5\n",
      "  * lead             (lead) timedelta64[ns] 280B 0 days 1 days ... 34 days\n",
      "  * forecast_date    (forecast_date) datetime64[ns] 32B 2021-04-21 ... 2021-0...\n",
      "Data variables:\n",
      "    crps_temp_all    (forecast_date, lead, location) float64 15kB 0.7161 ... ...\n",
      "    crps_precip_all  (forecast_date, lead, location) float64 15kB 0.000271 .....\n",
      "    crps_temp        (lead, location) float64 4kB 0.588 0.2581 ... 1.466 1.346\n",
      "    crps_precip      (lead, location) float64 4kB 0.4079 1.316 ... 0.001919\n",
      "Attributes:\n",
      "    short_name:  crps\n",
      "    long_name:   CRPS\n"
     ]
    }
   ],
   "source": [
    "skill = sk.skill.crps_ensemble(observations=hist, forecasts=fcst)\n",
    "print(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "skill[f\"crps_{plot_var}\"].plot.line(x=\"lead\", hue=\"location\", add_legend=True, alpha=0.5, ax=ax)\n",
    "skill[f\"crps_{plot_var}\"].mean(\"location\").plot(ax=ax, color=\"black\", linewidth=2, label=\"Mean\")\n",
    "ax.set_xlabel(\"Lead Time (days)\")\n",
    "plot_name = skill[f\"crps_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())\n",
    "plot_units = (\n",
    "    f'[{skill[f\"crps_{plot_var}\"].attrs[\"units\"]}]'\n",
    "    if \"units\" in skill[f\"crps_{plot_var}\"].attrs\n",
    "    else \"\"\n",
    ")\n",
    "ax.set_ylabel(f\"CRPS {plot_name} {plot_units}\".strip())\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Skill Relative to Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 36kB\n",
      "Dimensions:           (location: 13, lead: 34, forecast_date: 4)\n",
      "Coordinates:\n",
      "  * location          (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PDX' 'SAC'\n",
      "  * lead              (lead) timedelta64[ns] 272B 1 days 2 days ... 34 days\n",
      "  * forecast_date     (forecast_date) datetime64[ns] 32B 2021-04-21 ... 2021-...\n",
      "    lat               (location) float64 104B 33.63 42.36 34.2 ... 45.6 38.51\n",
      "    lon               (location) float64 104B -84.44 -71.01 ... -122.6 -121.5\n",
      "Data variables:\n",
      "    crpss_temp_all    (forecast_date, lead, location) float64 14kB 0.7998 ......\n",
      "    crpss_precip_all  (forecast_date, lead, location) float64 14kB 1.0 ... 0....\n",
      "    crpss_temp        (lead, location) float64 4kB 0.5489 0.8111 ... 0.3044\n",
      "    crpss_precip      (lead, location) float64 4kB 0.08609 0.9442 ... 0.7187\n",
      "Attributes:\n",
      "    short_name:  crpss\n",
      "    long_name:   CRPSS\n"
     ]
    }
   ],
   "source": [
    "if ref is None:\n",
    "    print(\"No reference model, skipping relative comparison\")\n",
    "    skill_ref = None\n",
    "    skill_rel = None\n",
    "else:\n",
    "    # Skill of the reference model:\n",
    "    skill_ref = sk.skill.crps_ensemble(observations=hist, forecasts=ref)\n",
    "\n",
    "    # Relative skills score of Salient downscale vs the reference model:\n",
    "    skill_rel = sk.skill.crpss(forecast=skill, reference=skill_ref)\n",
    "    print(skill_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skill_ref is None:\n",
    "    print(\"Skipping relative skill plotting\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    skill_ref[f\"crps_{plot_var}\"].mean(\"location\").plot(\n",
    "        ax=ax,\n",
    "        color=\"#FF7F00\",\n",
    "        linewidth=2,\n",
    "        label=ref_model.replace(\"_\", \" \").upper(),\n",
    "    )\n",
    "    skill[f\"crps_{plot_var}\"].mean(\"location\").plot(\n",
    "        ax=ax,\n",
    "        color=\"dodgerblue\",\n",
    "        linewidth=2,\n",
    "        label=\"Salient downscale\",\n",
    "    )\n",
    "\n",
    "    ax.xaxis.set_major_formatter(lambda x, pos: f\"{x/1e9/86400:.0f}\")\n",
    "    ax.set_xlabel(\"Lead Time (days)\")\n",
    "    ax.set_ylabel(\n",
    "        f'CRPS {skill[f\"crps_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())} [{skill[f\"crps_{plot_var}\"].attrs.get(\"units\", \"\")}]'\n",
    "    )\n",
    "    ax.set_title(\"All-locations Mean Error (lower is better)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skill_rel is None:\n",
    "    print(\"Skipping relative skill timeseries plot\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    skill_rel[f\"crpss_{plot_var}\"].plot.line(\n",
    "        x=\"lead\", hue=\"location\", add_legend=True, alpha=0.5, ax=ax\n",
    "    )\n",
    "    skill_rel[f\"crpss_{plot_var}\"].mean(\"location\").plot(\n",
    "        ax=ax, color=\"black\", linewidth=2, label=\"Mean\"\n",
    "    )\n",
    "    ax.xaxis.set_major_formatter(lambda x, pos: f\"{x/1e9/86400:.0f}\")\n",
    "    ax.axhline(y=0, color=\"grey\", linestyle=\":\", zorder=0)\n",
    "    ax.set_title(f\"Relative Skill Salient downscale vs {ref_model} (higher is better)\")\n",
    "    ax.set_xlabel(\"Lead Time (days)\")\n",
    "    ax.set_ylabel(\n",
    "        f'CRPSS ({skill_rel[f\"crpss_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())})'\n",
    "    )\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_ylim(max(-0.7, ymin), min(0.7, ymax))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skill_rel is None:\n",
    "    print(\"Skipping relative skill boxplot\")\n",
    "else:\n",
    "    medians = skill_rel[f\"crpss_{plot_var}\"].median(\"lead\")\n",
    "    sorted_locations = medians.sortby(medians, ascending=False).location.values\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    df = skill_rel[f\"crpss_{plot_var}\"].to_pandas().melt(ignore_index=False)\n",
    "    ax.boxplot(\n",
    "        [df[df[\"location\"] == loc][\"value\"] for loc in sorted_locations],\n",
    "        tick_labels=sorted_locations,  # Updated parameter name\n",
    "        patch_artist=True,\n",
    "        showfliers=False,\n",
    "        medianprops=dict(color=\"black\"),\n",
    "        boxprops=dict(facecolor=\"dodgerblue\"),\n",
    "    )\n",
    "    ax.axhline(y=0, color=\"grey\", linestyle=\":\", zorder=0)\n",
    "    ax.set_xlabel(\"Location\")\n",
    "    ax.set_ylabel(\n",
    "        f'CRPSS ({skill_rel[f\"crpss_{plot_var}\"].attrs.get(\"long_name\", plot_var.title())})'\n",
    "    )\n",
    "    ax.set_title(f\"Relative Skill Salient downscale vs {ref_model} (higher is better)\")\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    ax.set_ylim(max(-1, ymin), min(1, ymax))\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salientsdk-54A4kIpb-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
