{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Forecast Skill vs. ERA5 or Observations\n",
    "\n",
    "This example shows how to evaluate Salient's probabilistic forecasts against observations and calculate meaningful metrics. It demonstrates [validation best practices](https://salientpredictions.notion.site/Validation-0220c48b9460429fa86f577914ea5248) such as:\n",
    "\n",
    "- Proper scoring using the Continuous Ranked Probability Score (CRPS)\n",
    "  - Considers the full forecast distribution to reward both accuracy and precision\n",
    "  - Less sensitive to climatology decisions than metrics like Anomaly Correlation\n",
    "- A long backtesting period (2015-2022)\n",
    "  - Short evaluation periods are subject to noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<requests.sessions.Session at 0x7ff4aa7d9e10>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment:\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import salientsdk as sk\n",
    "except ModuleNotFoundError as e:\n",
    "    if os.path.exists(\"../salientsdk\"):\n",
    "        sys.path.append(os.path.abspath(\"..\"))\n",
    "        import salientsdk as sk\n",
    "    else:\n",
    "        raise ModuleNotFoundError(\"Install salient SDK with: pip install salientsdk\")\n",
    "\n",
    "# Prevent wrapping on tables for readability\n",
    "pd.set_option(\"display.width\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "sk.set_file_destination(\"validation_example\")\n",
    "sk.login(\"username\", \"password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize The Validation\n",
    "\n",
    "This notebook is written flexibly so you have the option of validating Salient and other forecasts multiple ways. These variables will control what, when, and how the validation proceeds.\n",
    "\n",
    "The `split_set` variable controls the amount of data to request via the `start_date` and `end_date` variables.\n",
    "\n",
    "- `sample` - a single season of data, good for quickly making sure that the mechanics of the process work.\n",
    "- `test` - gets data from 2015-2022, which is completely out-of-sample from model training. This requests a medium amount of data and is recommended for most validation processes.\n",
    "- `all` - gets data from 2000-2022, representing the full historical evaluation record. This will download quite a bit of data and is not recommended for most applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set the meteorological variable that we'll be evaluating:\n",
    "var = \"temp\"  # temperature\n",
    "# var = \"precip\"  # precipitation\n",
    "# var = \"wspd\" # wind\n",
    "# var = \"tsi\" # solar\n",
    "\n",
    "# 2. Set the forecast look-ahead amount:\n",
    "timescale = \"sub-seasonal\"  # weeks 1-5\n",
    "# timescale = \"seasonal\"  # months 1-3\n",
    "# timescale = \"long-range\" # quarters 1-4\n",
    "\n",
    "# 3. Set the number of hindcasts to download for validation:\n",
    "split_set = \"sample\"  # fast demonstration of mechanics\n",
    "# split_set = \"test\"  # recommended to validate out-of-sample with hindcast_summary\n",
    "# split_set = \"all\"  # download every hindcast since 2000\n",
    "\n",
    "# 4. Set the reference model to compare Salient blend to\n",
    "ref_model = \"clim\"  # Climatology.  Works across all timescale values.\n",
    "# ref_model = \"noaa_gefs\"  # Valid for the sub-seasonal timescale\n",
    "# ref_model = \"ecmwf_ens\" # Valid for sub-seasonal timescale\n",
    "# ref_model = \"ecmwf_seas5\" # Valid for seasonal and long-range timescales\n",
    "\n",
    "# 5. Use meteorological station observations in validation:\n",
    "validate_obs = True  # Calculate skill of debiased forecast vs met stations\n",
    "# validate_obs = False  # Calculate skill of undebiased forecast vs ERA5\n",
    "\n",
    "# ===== Additional shared variables ==========================\n",
    "fld = \"vals\"  # Evaluate in absolute space, not anomaly\n",
    "model = \"blend\"  # Validate the primary Salient blend model\n",
    "force = False  # If \"False\", cache data calls.  Set to \"True\" to overwrite caches\n",
    "\n",
    "frequency = {\"sub-seasonal\": \"weekly\", \"seasonal\": \"monthly\", \"long-range\": \"quarterly\"}[timescale]\n",
    "(start_date, end_date) = {\n",
    "    \"sample\": (\"2021-04-01\", \"2021-08-31\"),\n",
    "    \"test\": (\"2015-01-01\", \"2022-12-31\"),\n",
    "    \"all\": (\"2000-01-01\", \"2022-12-31\"),\n",
    "}[split_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Area of Interest\n",
    "\n",
    "The Salient SDK uses a \"Location\" object to specify the geographic bounds of a request. In this case, we will be validating against the vector of airport locations that are used to settle the Chicago Mercantile Exchange's Cooling and Heating Degree Day contracts. With `load_location_file` we can see that the file contains:\n",
    "\n",
    "- `lat` / `lon`: latitude and longitude of the met station, standard for a `location_file`\n",
    "- `name`: the 3-letter IATA airport code of the location, also `location_file` standard\n",
    "- `ghcnd`: the global climate network ID of the station, used to validate against observations. To customize this analysis for any set of observation stations, use the NCEI [stations list](https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt).\n",
    "- `cme`: the CME code for the location used to create CDD/HDD strip codes.\n",
    "- `description`: full name of the airport\n",
    "\n",
    "If you have a list of locations already defined in a separate CSV file, you can use [`upload_file`](https://sdk.salientpredictions.com/api/#salientsdk.upload_file) to upload the file directly without building it in code via `upload_location_file`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         lat        lon name        ghcnd cme                description                     geometry\n",
      "0   33.62972  -84.44224  ATL  USW00013874   1         Atlanta Hartsfield   POINT (-84.44224 33.62972)\n",
      "1   42.36057  -71.00975  BOS  USW00014739   W               Boston Logan   POINT (-71.00975 42.36057)\n",
      "2   34.19966 -118.36543  BUR  USW00023152   P  Burbank-Glendale-Pasadena  POINT (-118.36543 34.19966)\n",
      "3   41.96017  -87.93164  ORD  USW00094846   2             Chicago O'Hare   POINT (-87.93164 41.96017)\n",
      "4   39.04443  -84.67241  CVG  USW00093814   3     Cincinnati (Covington)   POINT (-84.67241 39.04443)\n",
      "5   32.89744  -97.02196  DFW  USW00003927   5          Dallas-Fort Worth   POINT (-97.02196 32.89744)\n",
      "6   29.98438  -95.36072  IAH  USW00012960   R        Houston-George Bush   POINT (-95.36072 29.98438)\n",
      "7   36.07190 -115.16343  LAS  USW00023169   0         Las Vegas McCarran   POINT (-115.16343 36.0719)\n",
      "8   44.88523  -93.23133  MSP  USW00014922   Q         Minneapolis-StPaul   POINT (-93.23133 44.88523)\n",
      "9   40.77945  -73.88027  LGA  USW00014732   4        New York La Guardia   POINT (-73.88027 40.77945)\n",
      "10  39.87326  -75.22681  PHL  USW00013739   6               Philadelphia   POINT (-75.22681 39.87326)\n",
      "11  45.59578 -122.60919  PDX  USW00024229   7                   Portland  POINT (-122.60919 45.59578)\n",
      "12  38.50659 -121.49604  SAC  USW00023232   S            Sacramento Exec  POINT (-121.49604 38.50659)\n"
     ]
    }
   ],
   "source": [
    "# fmt: off\n",
    "loc = sk.Location(location_file=sk.upload_location_file(\n",
    "    lats =[33.62972     ,      42.36057,      34.19966,      41.96017,      39.04443,      32.89744,      29.98438,      36.07190,      44.88523,      40.77945,      39.87326,      45.59578,      38.50659],\n",
    "    lons =[-84.44224    ,     -71.00975,    -118.36543,     -87.93164,     -84.67241,     -97.02196,     -95.36072,    -115.16343,     -93.23133,     -73.88027,     -75.22681,    -122.60919,    -121.49604],\n",
    "    names=[\"ATL\"        ,         \"BOS\",         \"BUR\",         \"ORD\",         \"CVG\",         \"DFW\",         \"IAH\",         \"LAS\",         \"MSP\",         \"LGA\",         \"PHL\",         \"PDX\",         \"SAC\"],\n",
    "    ghcnd=[\"USW00013874\", \"USW00014739\", \"USW00023152\", \"USW00094846\", \"USW00093814\", \"USW00003927\", \"USW00012960\", \"USW00023169\", \"USW00014922\", \"USW00014732\", \"USW00013739\", \"USW00024229\", \"USW00023232\"],\n",
    "    cme  =[\"1\"          ,           \"W\",           \"P\",           \"2\",           \"3\",           \"5\",           \"R\",           \"0\",           \"Q\",           \"4\",           \"6\",           \"7\",           \"S\"],\n",
    "    geoname=\"cmeus\",\n",
    "    force=force,\n",
    "    description=[\"Atlanta Hartsfield\", \"Boston Logan\", \"Burbank-Glendale-Pasadena\", \"Chicago O'Hare\", \"Cincinnati (Covington)\",\"Dallas-Fort Worth\", \"Houston-George Bush\", \"Las Vegas McCarran\", \"Minneapolis-StPaul\", \"New York La Guardia\",\"Philadelphia\", \"Portland\", \"Sacramento Exec\"],\n",
    "))\n",
    "# fmt: on\n",
    "stations = loc.load_location_file()\n",
    "print(stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull precomputed skill\n",
    "\n",
    "Salient pre-calculates skill metrics as part of our internal validation and model improvement process. Use the `hindcast_summary` api endpoint to request pre-calculated skill scores. This is the \"easy\" way to validate Salient's forecasts: we've already done all the work for you.\n",
    "\n",
    "The remainder of this notebook will show you how to reproduce this skill calculation by requesting historical forecasts, historical data, and calculating a skill score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Reference CRPS  Salient CRPS  Salient CRPS Skill Score (%)\n",
      "Location Lead                                                              \n",
      "ATL      Week 1           1.522         0.476                        68.705\n",
      "         Week 2           1.521         1.063                        30.122\n",
      "         Week 3           1.528         1.361                        10.951\n",
      "         Week 4           1.525         1.410                         7.512\n",
      "         Week 5           1.522         1.408                         7.504\n",
      "...                         ...           ...                           ...\n",
      "SAC      Week 1           1.218         0.487                        59.950\n",
      "         Week 2           1.225         0.889                        27.396\n",
      "         Week 3           1.225         1.087                        11.314\n",
      "         Week 4           1.228         1.119                         8.881\n",
      "         Week 5           1.231         1.118                         9.194\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "skill_summ = (\n",
    "    pd.read_csv(\n",
    "        sk.hindcast_summary(\n",
    "            loc=loc,\n",
    "            interp_method=\"linear\",\n",
    "            metric=\"crps\",\n",
    "            variable=var,\n",
    "            timescale=timescale,\n",
    "            reference=ref_model,\n",
    "            split_set=\"test\" if split_set == \"sample\" else split_set,\n",
    "            verbose=False,\n",
    "            force=force,\n",
    "        )\n",
    "    )\n",
    "    .round(decimals=3)\n",
    "    .drop(columns=[\"Reference Model\"])\n",
    "    .set_index([\"Location\", \"Lead\"])\n",
    "    .sort_index(level=[0, 1])\n",
    ")\n",
    "print(skill_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skill(\n",
    "    df: pd.DataFrame, col: str = \"Salient CRPS Skill Score (%)\", title: str = \"Skill\"\n",
    ") -> None:\n",
    "    \"\"\"Plot skill scores in a table.\"\"\"\n",
    "    df = df.reset_index()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for location in df[\"Location\"].unique():\n",
    "        subset = df[df[\"Location\"] == location]\n",
    "        plt.plot(subset[\"Lead\"], subset[col], label=location)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Lead\")\n",
    "    plt.ylabel(col)\n",
    "    plt.legend(title=\"Location\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_skill(skill_summ, title=f\"hindcast_summary crps {model} vs {ref_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Data\n",
    "\n",
    "To calculate forecast skill, we will want to compare forecasts made in the past with actuals. There are two flavors of actual data: (1) The ERA5 reanalysis dataset and (2) point weather station observations.\n",
    "\n",
    "Salient's forecast natively predicts (1) ERA5, but contains a debiasing function to remove bias between ERA5 and (2) station observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical ERA5 Data\n",
    "\n",
    "Download daily historical values from [`data_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.data_timeseries) and then aggregate to match the forecasts, so that we can ensure that all forecasts use the same dates.\n",
    "\n",
    "Also, get observed weather station data in the same format by downloading directly from NCEI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 333kB\n",
      "Dimensions:   (time: 2967, location: 13)\n",
      "Coordinates:\n",
      "  * time      (time) datetime64[ns] 24kB 2014-12-27 2014-12-28 ... 2023-02-09\n",
      "  * location  (location) object 104B 'ATL' 'BOS' 'BUR' ... 'PHL' 'PDX' 'SAC'\n",
      "    lat       (location) float64 104B 33.63 42.36 34.2 ... 39.87 45.6 38.51\n",
      "    lon       (location) float64 104B -84.44 -71.01 -118.4 ... -122.6 -121.5\n",
      "Data variables:\n",
      "    vals      (time, location) float64 309kB 7.563 5.248 8.52 ... 6.154 10.1\n",
      "Attributes:\n",
      "    long_name:   2 metre temperature\n",
      "    units:       degC\n",
      "    clim_start:  1990-01-01\n",
      "    clim_end:    2019-12-31\n"
     ]
    }
   ],
   "source": [
    "# Get additional historical data beyond end_date to make sure we have enough\n",
    "# observed days to compare with the final forecast.\n",
    "duration = {\"sub-seasonal\": 8 * 5, \"seasonal\": 31 * 3, \"long-range\": 95 * 4}[timescale]\n",
    "hist = sk.data_timeseries(\n",
    "    loc=loc,\n",
    "    variable=var,\n",
    "    field=fld,\n",
    "    start=np.datetime64(start_date) - np.timedelta64(5, \"D\"),\n",
    "    end=np.datetime64(end_date) + np.timedelta64(duration, \"D\"),\n",
    "    frequency=\"daily\",\n",
    "    # reference_clim=\"30_yr\",  implicitly uses 30 yr climatology\n",
    "    verbose=False,\n",
    "    force=force,\n",
    ")\n",
    "print(xr.load_dataset(hist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast\n",
    "\n",
    "The [`forecast_timeseries`](https://sdk.salientpredictions.com/api/#salientsdk.forecast_timeseries) API endpoint and SDK function returns Salient's native temporally granular weekly/monthly/quarterly forecasts.\n",
    "\n",
    "This is the most heavyweight call in the notebook, since it's getting multiple historical forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              file_name        date  model\n",
      "0     validation_example/forecast_timeseries_6bfd97e...  2015-01-01  blend\n",
      "1     validation_example/forecast_timeseries_f0a103d...  2015-01-01   clim\n",
      "2     validation_example/forecast_timeseries_2bf8d56...  2015-01-04  blend\n",
      "3     validation_example/forecast_timeseries_d8754c3...  2015-01-04   clim\n",
      "4     validation_example/forecast_timeseries_5065cf0...  2015-01-07  blend\n",
      "...                                                 ...         ...    ...\n",
      "2171  validation_example/forecast_timeseries_0c62df7...  2022-12-21   clim\n",
      "2172  validation_example/forecast_timeseries_d0f3523...  2022-12-25  blend\n",
      "2173  validation_example/forecast_timeseries_6b1bd58...  2022-12-25   clim\n",
      "2174  validation_example/forecast_timeseries_ace47ee...  2022-12-28  blend\n",
      "2175  validation_example/forecast_timeseries_5b17f55...  2022-12-28   clim\n",
      "\n",
      "[2176 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# get_hindcast_dates is a utility that returns all valid hindcast initializations.\n",
    "date_range = sk.get_hindcast_dates(start_date=start_date, end_date=end_date, timescale=timescale)\n",
    "\n",
    "fcst = sk.forecast_timeseries(\n",
    "    loc=loc,\n",
    "    variable=var,\n",
    "    field=fld,\n",
    "    date=date_range,\n",
    "    timescale=timescale,\n",
    "    model=[model, ref_model],\n",
    "    reference_clim=\"30_yr\",  # this is the same climatology used by data_timeseries\n",
    "    debias=False,\n",
    "    verbose=False,\n",
    "    force=force,\n",
    "    strict=False,  # There is missing data in 2020.  Work around it.\n",
    ")\n",
    "print(fcst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 13kB\n",
      "Dimensions:                 (quantiles: 23, location: 13, lead_weekly: 5,\n",
      "                             nbnds: 2)\n",
      "Coordinates:\n",
      "  * quantiles               (quantiles) float64 184B 0.01 0.025 ... 0.975 0.99\n",
      "  * location                (location) object 104B 'ATL' 'BOS' ... 'PDX' 'SAC'\n",
      "    lat                     (location) float64 104B 33.63 42.36 ... 45.6 38.51\n",
      "    lon                     (location) float64 104B -84.44 -71.01 ... -121.5\n",
      "    forecast_period_weekly  (lead_weekly, nbnds) datetime64[ns] 80B 2015-01-0...\n",
      "  * lead_weekly             (lead_weekly) int32 20B 1 2 3 4 5\n",
      "    forecast_date_weekly    datetime64[ns] 8B 2015-01-01\n",
      "Dimensions without coordinates: nbnds\n",
      "Data variables:\n",
      "    vals_weekly             (lead_weekly, location, quantiles) float64 12kB 4...\n",
      "Attributes:\n",
      "    clim_period:  ['1990-01-01', '2019-12-31']\n",
      "    region:       north-america\n",
      "    short_name:   temp\n",
      "    timescale:    sub-seasonal\n"
     ]
    }
   ],
   "source": [
    "# Check to see if there are any missing forecasts:\n",
    "fcst_na = fcst[fcst[\"file_name\"].isna()]\n",
    "if not fcst_na.empty:\n",
    "    print(\"Missing forecast dates:\")\n",
    "    print(fcst_na)\n",
    "\n",
    "# Example forecast file is for a single model and a single forecast_date\n",
    "print(xr.load_dataset(fcst[\"file_name\"].values[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Skill Metrics\n",
    "\n",
    "Compare the forecast and ERA5 datasets to see how well they match. Here we will calculate the same \"Continuous Ranked Probability Score\" that resulted from the call to `hindcast_summary` earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_model = sk.skill.crps(\n",
    "    observations=hist,\n",
    "    forecasts=fcst[fcst[\"model\"] == model],\n",
    ")\n",
    "print(skill_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `crps_weekly` value shows the average of all error values over the full date range. `crps_weekly_all` preserves the crps for each `forecast_date`, so we can also show that CRPS varies over time and has a seasonal component:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_model[f\"crps_{frequency}_all\"].mean(dim=f\"lead_{frequency}\").rolling(\n",
    "    **{\n",
    "        f\"forecast_date_{frequency}\": max(\n",
    "            1, int(len(skill_model[f\"forecast_date_{frequency}\"]) * 0.1)\n",
    "        )\n",
    "    },\n",
    "    center=True,\n",
    ").mean().plot(x=f\"forecast_date_{frequency}\", hue=\"location\", figsize=(12, 6))\n",
    "\n",
    "units = xr.load_dataset(hist).attrs[\"units\"]\n",
    "plt.title(\"Error varies over time\")\n",
    "plt.ylabel(f\"{model} crps {var} [{units}]\")\n",
    "plt.xlabel(\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also calculate the skill of the \"reference\" model for purposes of comparison:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_ref = sk.skill.crps(\n",
    "    observations=hist,\n",
    "    forecasts=fcst[fcst[\"model\"] == ref_model],\n",
    ")\n",
    "print(skill_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills = xr.concat(\n",
    "    [\n",
    "        skill_ref.assign_coords(source=ref_model),\n",
    "        skill_model.assign_coords(source=model),\n",
    "    ],\n",
    "    dim=\"source\",\n",
    ")\n",
    "print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_skill(skills: xr.Dataset):\n",
    "    \"\"\"Show CRPS by lead, for 2 or more models.\"\"\"\n",
    "    skills[f\"crps_{frequency}\"].plot.line(\n",
    "        x=f\"lead_{frequency}\", hue=\"source\", col=\"location\", col_wrap=3, figsize=(10, 10)\n",
    "    )\n",
    "    plt.suptitle(f\"{var} CRPS\", fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "compare_model_skill(skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Relative Skill\n",
    "\n",
    "CRPS shows skill without context. A \"skill score\" will compare two different skills to generate relative value. In the example below, we will compare the Salient blend with climatology (historical averages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_score = sk.skill.crpss(forecast=skill_model, reference=skill_ref)\n",
    "\n",
    "# Represent the skill scores as a human-readable table of the same format as we generated\n",
    "# for the hindcast_summary results.\n",
    "skill_table = (\n",
    "    xr.merge(\n",
    "        [\n",
    "            (skill_ref[f\"crps_{frequency}\"].rename(\"Reference CRPS\")).round(2),\n",
    "            skill_model[f\"crps_{frequency}\"].rename(\"Salient CRPS\").round(2),\n",
    "            (skill_score[f\"crpss_{frequency}\"] * 100)\n",
    "            .rename(\"Salient CRPS Skill Score (%)\")\n",
    "            .round(1),\n",
    "        ]\n",
    "    )\n",
    "    .to_dataframe()\n",
    "    .reset_index()\n",
    "    .dropna(how=\"any\")\n",
    "    .drop(columns=[\"lat\", \"lon\"])\n",
    "    .rename(columns={\"location\": \"Location\", f\"lead_{frequency}\": \"Lead\"})\n",
    ")\n",
    "skill_table[\"Lead\"] = \"Week \" + skill_table[\"Lead\"].astype(str)\n",
    "skill_table.set_index([\"Location\", \"Lead\"], inplace=True)\n",
    "\n",
    "print(skill_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skill(skill_summ, title=f\"manually-calculated crps {model} vs {ref_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare manually-calculated to pre-computed skill\n",
    "\n",
    "Now that we have a CRPS calculated manually as well as downloaded from `hindcast_summary` we can evaluate how close the two values are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_merge = pd.merge(\n",
    "    skill_summ.add_prefix(\"Summary \"),\n",
    "    skill_table.add_prefix(\"Manual \"),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "print(skill_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the manually-calculated skill score with the precomputed skill scores published by `hindcast_summary`.\n",
    "\n",
    "Note that when using `split_set = sample` the values won't match exactly. In this case we are plotting skill scores calculated from a single year of forecasts against the skill scores from the `test` set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cols(col_name: str) -> None:\n",
    "    \"\"\"Plot manual and precalculated skill columns.\"\"\"\n",
    "    summary_col = f\"Summary {col_name}\"\n",
    "    manual_col = f\"Manual {col_name}\"\n",
    "\n",
    "    df = skill_merge.reset_index()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for location in df[\"Location\"].unique():\n",
    "        subset = df[df[\"Location\"] == location]\n",
    "        plt.scatter(subset[summary_col], subset[manual_col], label=location, s=100)\n",
    "\n",
    "    # Same limits for both axes\n",
    "    min_limit = min(df[summary_col].min(), df[manual_col].min())\n",
    "    max_limit = max(df[summary_col].max(), df[manual_col].max())\n",
    "    plt.xlim(min_limit, max_limit)\n",
    "    plt.ylim(min_limit, max_limit)\n",
    "    plt.plot(\n",
    "        [min_limit, max_limit], [min_limit, max_limit], color=\"gray\", linestyle=\"--\", linewidth=1\n",
    "    )\n",
    "    plt.gca().set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "    plt.title(f\"Summary vs Manual {col_name}\")\n",
    "    plt.xlabel(summary_col)\n",
    "    plt.ylabel(manual_col)\n",
    "    plt.legend(title=\"Location\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    if split_set == \"sample\":\n",
    "        plt.text(\n",
    "            min_limit,\n",
    "            max_limit,\n",
    "            \"Results not expected to match for split_set = 'summary'.\\nUse split_set = 'test' for a full comparison.\",\n",
    "            fontsize=10,\n",
    "            verticalalignment=\"top\",\n",
    "            horizontalalignment=\"left\",\n",
    "            color=\"red\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "compare_cols(\"Salient CRPS Skill Score (%)\")\n",
    "# compare_cols(\"Salient CRPS\")\n",
    "# compare_cols(\"Reference CRPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating vs Observation Station Data\n",
    "\n",
    "Get observed historical data from meteorological stations. In this case, we'll write a `get_ghcnd` function that downloads observed station data and returns a list of pandas `DataFrame`s. If you have observed data from a proprietary source, you can substitute a function here that returns a vector of `DataFrame`s. Make sure that the `DataFrame`s have a column that matches the `variable` of interest (such as `temp`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example function: ghcnd\n",
    "\n",
    "When validating against observations you can use any observation source that you have access to. For this example, we'll pull observation station data from NCEI. The main requirement is that the data be tabular, daily in frequency, and contain a column with the same name as `var`.\n",
    "\n",
    "Salient's `debias` forecasts are debiased vs GHCN and other public data sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_ghcnd(\n",
    "    ghcnd_id: str | Iterable[str],\n",
    "    start_date: str = \"2000-01-01\",\n",
    "    xdd: float = (65 - 32) * 5 / 9,\n",
    "    destination: str = \"-default\",\n",
    "    force: bool = False,\n",
    ") -> pd.DataFrame | list[pd.DataFrame]:\n",
    "    \"\"\"Get GHCNd observed data timeseries for a single station.\n",
    "\n",
    "    Global Historical Climatology Network - Daily.\n",
    "\n",
    "    Args:\n",
    "        ghcnd_id (str | list[str]): GHCND station ID or list of IDs\n",
    "        start_date (str): omit data before this date\n",
    "        xdd (float): base temperature for heating/cooling degree days, in degC.\n",
    "        destination (str): The directory to download the data to\n",
    "        force (bool): if True (default False), force update of observed data\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame | list[pd.DataFrame]: observed data timeseries with\n",
    "        columns `time`, `precip`, `wspd`, `tmin`, `tmax`, `tavg`, `hdd` and `cdd`\n",
    "        in units `degC`.  Also meta-data columns for `ghcn_id`, `lat`, `lon`, `elev`, and `name`.\n",
    "        Will return a list of DataFrames if wban_id is iterable.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "        >>> obs_single = get_ghcnd(\"USW00013874\")\n",
    "        >>> obs_vector = get_ghcnd([\"USW00013874\", \"USW00014739\"])\n",
    "    \"\"\"\n",
    "    if isinstance(ghcnd_id, Iterable) and not isinstance(ghcnd_id, str):\n",
    "        return [get_ghcnd(single_id, start_date, xdd, force) for single_id in ghcnd_id]\n",
    "\n",
    "    file_name = os.path.join(sk.get_file_destination(), f\"{ghcnd_id}.csv\")\n",
    "    if force or not os.path.exists(file_name):\n",
    "        GHCND_ROOT = \"https://www.ncei.noaa.gov\"\n",
    "        GHCND_DIR = \"data/global-historical-climatology-network-daily/access\"\n",
    "        ghcnd_url = f\"{GHCND_ROOT}/{GHCND_DIR}/{ghcnd_id}.csv\"\n",
    "        r = requests.get(ghcnd_url)\n",
    "        r.raise_for_status()\n",
    "        with open(file_name, \"w\") as f:\n",
    "            f.write(r.text)\n",
    "\n",
    "    # Gusts: WSF1, WSF2, WSF5 are fastest 1-min, 2-min, 5-sec wind speed\n",
    "    # Humidity: RHAV, RHMN, RHMX\n",
    "    keep_cols = {\n",
    "        \"STATION\": \"ghcnd_id\",\n",
    "        \"DATE\": \"time\",\n",
    "        \"LATITUDE\": \"lat\",\n",
    "        \"LONGITUDE\": \"lon\",\n",
    "        \"ELEVATION\": \"elev\",  # meters\n",
    "        \"NAME\": \"name\",\n",
    "        \"TMAX\": \"tmax\",\n",
    "        \"TMIN\": \"tmin\",\n",
    "        \"PRCP\": \"precip\",\n",
    "        \"TAVG\": \"temp\",\n",
    "        \"AWND\": \"wspd\",\n",
    "    }\n",
    "\n",
    "    obs = pd.read_csv(file_name, usecols=keep_cols.keys())\n",
    "    obs.rename(columns=keep_cols, inplace=True)\n",
    "\n",
    "    # ncei uses 9999 for missing data\n",
    "    INVALID_NUMBER = 9999\n",
    "    obs.replace(INVALID_NUMBER, pd.NA, inplace=True)\n",
    "\n",
    "    # ncei uses 10ths of values\n",
    "    columns_to_decimalize = [\"precip\", \"tmax\", \"tmin\", \"wspd\", \"temp\"]\n",
    "    for col in columns_to_decimalize:\n",
    "        if col in obs.columns:\n",
    "            obs[col] = obs[col] / 10.0\n",
    "\n",
    "    obs = obs[obs[\"time\"] >= start_date]\n",
    "    obs[\"time\"] = pd.to_datetime(obs[\"time\"])\n",
    "\n",
    "    # Only calculate degree days if both tmin and tmax are available\n",
    "    if \"tmin\" in obs.columns and \"tmax\" in obs.columns:\n",
    "        # Note that these long_names are identical to what data_timeseries returns:\n",
    "        obs[\"tmax\"].attrs[\"units\"] = \"degC\"\n",
    "        obs[\"tmax\"].attrs[\"long_name\"] = \"2 metre daily temperature\"\n",
    "\n",
    "        obs[\"tmin\"].attrs[\"units\"] = \"degC\"\n",
    "        obs[\"tmin\"].attrs[\"long_name\"] = \"2 metre daily temperature\"\n",
    "\n",
    "        # HDD & CDD settle off the mean of tmin/tmax, not the daily mean\n",
    "        temp = pd.concat([obs[\"tmin\"], obs[\"tmax\"]], axis=1).mean(axis=1)\n",
    "        obs[\"cdd\"] = (temp - xdd).clip(lower=0).round(1)\n",
    "        obs[\"hdd\"] = (xdd - temp).clip(lower=0).round(1)\n",
    "\n",
    "        # Some stations such as USW00023152 don't record temp but do have tmin/tmax.\n",
    "        # Replace missing temp with mean(tmin,tmax) as an approximation.\n",
    "        temp_na_mask = obs[\"temp\"].isna()\n",
    "        obs.loc[temp_na_mask, \"temp\"] = temp[temp_na_mask]\n",
    "\n",
    "        obs[\"hdd\"].attrs[\"units\"] = \"HDD day-1 (degC)\"\n",
    "        obs[\"hdd\"].attrs[\"long_name\"] = \"Heating Degree Days\"\n",
    "\n",
    "        obs[\"cdd\"].attrs[\"units\"] = \"CDD day-1 (degC)\"\n",
    "        obs[\"cdd\"].attrs[\"long_name\"] = \"Cooling Degree Days\"\n",
    "\n",
    "    obs[\"temp\"].attrs[\"units\"] = \"degC\"\n",
    "    obs[\"temp\"].attrs[\"long_name\"] = \"2 metre temperature\"\n",
    "\n",
    "    obs[\"precip\"].attrs[\"units\"] = \"mm day-1\"\n",
    "    obs[\"precip\"].attrs[\"long_name\"] = \"Total precipitation\"\n",
    "\n",
    "    obs[\"wspd\"].attrs[\"units\"] = \"m s**-1\"\n",
    "    obs[\"wspd\"].attrs[\"long_name\"] = \"Wind Speed\"\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    obs_df = get_ghcnd(stations.ghcnd, start_date=start_date, force=force)\n",
    "    # Output is a vector of DataFrames, one per station.  Let's inspect the first:\n",
    "    print(obs_df[0])\n",
    "else:\n",
    "    print(\"skipped: not comparing to observed data\")\n",
    "    obs_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format tabular observation data to xarray\n",
    "\n",
    "Use the `make_observed_ds` function to reformat the `DataFrame`s of observed data into an `xarray.Dataset` with the same format as the historical timeseries returned by `data_timeseries`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    obs = sk.observed.make_observed_ds(\n",
    "        obs_df=obs_df,  # a DataFrame or vector of DataFrames\n",
    "        name=stations.name,  # this will populate the location coordinate\n",
    "        variable=var,  # make sure that the DataFrame(s) in obs_df contain this column name\n",
    "        time_col=\"time\",  # the name of the column in obs_df containing the date\n",
    "    )\n",
    "    print(obs)\n",
    "else:\n",
    "    print(\"skipped: not comparing to observed data\")\n",
    "    obs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare observed and ERA5 datasets\n",
    "\n",
    "Via `make_observed_ds`, the observed station data (`obs`) is formatted the same as the ERA5 historical data (`hist`). This means we can easily compare one to the other and see the degree of bias that exists between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    # Pull observed and ERA5 into a single dataset for easy comparison:\n",
    "    merged = xr.merge(\n",
    "        [\n",
    "            obs.rename({\"vals\": \"obs\"}),\n",
    "            xr.load_dataset(hist).rename({\"vals\": \"hist\"}),\n",
    "        ],\n",
    "        join=\"inner\",\n",
    "    )\n",
    "    merged[\"delta_raw\"] = merged[\"obs\"] - merged[\"hist\"]\n",
    "    # Daily bias is noisy.  Smooth it out to make trends clearer.\n",
    "    # This will induce nans at the beginning and end of the timeseries.\n",
    "    merged[\"delta\"] = (\n",
    "        merged[\"delta_raw\"]\n",
    "        .rolling(time=max(1, int(len(merged[\"time\"]) * 0.1)), center=True)\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    print(merged)\n",
    "else:\n",
    "    print(\"skipped: not comparing to observed data\")\n",
    "    merged = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    # Visualize obs-era5 bias over time at each station\n",
    "    merged[\"delta\"].plot.line(x=\"time\", hue=\"location\")\n",
    "    plt.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.title(\"Delta of Observed to Historical Values\")\n",
    "    plt.ylabel(f'{merged.attrs[\"long_name\"]} obs - hist [{merged.attrs[\"units\"]}]')\n",
    "    plt.xlabel(\"\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate the need for station debiasing\n",
    "\n",
    "Salient's forecasts are trained to predict the ERA5 reanalysis. The bias between station observations and ERA5 means that native forecast skill will be lower than desired when compared to station observations. Let's calculate that undebiased skill so we can quantify the magnitude of the effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    # Explicitly flag the native forecasts as undebiased:\n",
    "    fcst[\"debias\"] = False\n",
    "    skill_undebiased_obs = sk.skill.crps(\n",
    "        observations=obs,  # note that we're using observation stations as \"truth\"\n",
    "        forecasts=fcst[(fcst[\"model\"] == model) & (fcst[\"debias\"] == False)],\n",
    "    ).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases where there is not much bias between ERA5 and the observation station (Philadelphia, Cincinnati) we see that there is also not much difference in skill depending on the source of truth. In other locations like Boston or New York (LGA) we see that the difference between the observation station and ERA5 induces nontrivial skill degradation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    skills_obs = xr.concat(\n",
    "        [\n",
    "            skill_undebiased_obs.assign_coords(source=\"undebiased vs obs\"),\n",
    "            skill_model.assign_coords(source=\"undebiased vs ERA5\"),\n",
    "        ],\n",
    "        dim=\"source\",\n",
    "        coords=\"minimal\",\n",
    "    )\n",
    "\n",
    "    compare_model_skill(skills_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get debiased forecasts\n",
    "\n",
    "Salient's models natively predict ERA5 values, not observation station values. The `forecast_timeseries`, `data_timeseries`, and `downscale` functions have a `debias` option that adjusts ERA5-based data to more closely match obserations. Let's get a set of forecasts with debiasing on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get debiased hindcasts, if we are validating vs observations\n",
    "if validate_obs:\n",
    "    fcst_debias = sk.forecast_timeseries(\n",
    "        loc=loc,\n",
    "        variable=var,\n",
    "        field=fld,\n",
    "        date=date_range,\n",
    "        timescale=timescale,\n",
    "        model=model,  # debias only avilable for model blend\n",
    "        reference_clim=\"30_yr\",\n",
    "        debias=True,  # Note debias\n",
    "        verbose=False,\n",
    "        force=force,\n",
    "        strict=False,\n",
    "    )\n",
    "    # Add synthetic columns so that the two forecast tables can concatenate:\n",
    "    fcst_debias[\"model\"] = model\n",
    "    fcst_debias[\"debias\"] = True\n",
    "\n",
    "    fcst = pd.concat([fcst, fcst_debias], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare undebiased forecast skill to debiased\n",
    "\n",
    "Now quantify the effect of debiasing by calculating the skill of the debiased forecast against observation station data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    skill_debiased_obs = sk.skill.crps(\n",
    "        observations=obs,\n",
    "        forecasts=fcst[(fcst[\"model\"] == model) & (fcst[\"debias\"] == True)],\n",
    "    ).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of debiasing is to bring the debiased-obs forecast skill (green) as close as possible to the undebiased-era5 skill (orange).\n",
    "\n",
    "- Atlanta, Portland, Minneapolis, Philadelphia, and Sacramento have consistent offsets that result in near-perfect debiasing.\n",
    "- Boston, Burbank, and New York (LGA) see a significant reduction in error as a result of debiasing, but there is still some residual skill loss.\n",
    "- Chicago (ORD)'s observation-ERA5 offset is inconsistent, so the debiaser recovers limited skill.\n",
    "- Cincinnati (CVG) had little bias to begin with, so the debiaser doesn't change the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_obs:\n",
    "    skills_obs = xr.concat(\n",
    "        [\n",
    "            skill_undebiased_obs.assign_coords(source=\"undebiased vs obs\"),\n",
    "            skill_model.assign_coords(source=\"undebiased vs ERA5\"),\n",
    "            skill_debiased_obs.assign_coords(source=\"debiased vs obs\"),\n",
    "        ],\n",
    "        dim=\"source\",\n",
    "        coords=\"minimal\",\n",
    "    )\n",
    "\n",
    "    compare_model_skill(skills_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can attribute the incremental error of debiasing against observations. The relatively small size of the orange bars indicates that debiased (vs obs) forecasts are only slightly less skillful than the native (vs era5) forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_skills = xr.concat(\n",
    "    [\n",
    "        skill_model.assign_coords(source=\"era5 forecast\"),\n",
    "        (skill_debiased_obs - skill_model).assign_coords(source=\"obs-era5\"),\n",
    "    ],\n",
    "    dim=\"source\",\n",
    "    coords=\"minimal\",\n",
    ").mean(dim=f\"lead_{frequency}\")[f\"crps_{frequency}\"]\n",
    "\n",
    "location_order = (\n",
    "    skill_debiased_obs.mean(dim=f\"lead_{frequency}\")\n",
    "    .sortby(f\"crps_{frequency}\", ascending=False)\n",
    "    .location\n",
    ")\n",
    "\n",
    "combined_skills.reindex(location=location_order).T.to_pandas().plot(\n",
    "    kind=\"bar\", stacked=True, figsize=(12, 6)\n",
    ")\n",
    "\n",
    "plt.ylabel(f\"CRPS {var} (all leads)\")\n",
    "plt.title(\"Error Contribution by Source\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salient",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
