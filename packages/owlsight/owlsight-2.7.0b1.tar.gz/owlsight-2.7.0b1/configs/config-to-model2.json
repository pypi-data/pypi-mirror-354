{
    "main": {
        "max_retries_on_error": 3,
        "prompt_retry_on_error": true,
        "prompt_code_execution": true,
        "track_model_usage": false,
        "extra_index_url": "",
        "python_compile_mode": "single",
        "dynamic_system_prompt": false,
        "sequence_on_loading": [
        ]
    },
    "model": {
        "model_id": "lmstudio-community/Llama-3.2-3B-Instruct-GGUF",
        "apply_chat_history": true,
        "system_prompt": "",
        "model_kwargs": {},
        "transformers__device": null,
        "transformers__quantization_bits": null,
        "transformers__stream": true,
        "gguf__filename": "Llama-3.2-3B-Instruct-Q8_0.gguf",
        "gguf__verbose": false,
        "gguf__n_ctx": 16384,
        "gguf__n_gpu_layers": 0,
        "gguf__n_batch": 8,
        "gguf__n_cpu_threads": 8,
        "onnx__model_dir": "",
        "onnx__verbose": false,
        "onnx__n_cpu_threads": 8
    },
    "generate": {
        "stopwords": [],
        "max_new_tokens": 512,
        "temperature": 0.0,
        "generation_kwargs": {}
    },
    "rag": {
        "active": false,
        "target_library": "",
        "top_k": 10,
        "sentence_transformer_weight": 0.5,
        "sentence_transformer_name_or_path": "",
        "search": ""
    },
    "agentic": {
        "apply_tools": true,
        "max_steps": 8,
        "enable_python_agent": false
    },
    "huggingface": {
        "search": "gguf 1b llama",
        "top_k": 49,
        "select_model": "lmstudio-community/Llama-3.2-1B-Instruct-GGUF",
        "task": null
    }
}