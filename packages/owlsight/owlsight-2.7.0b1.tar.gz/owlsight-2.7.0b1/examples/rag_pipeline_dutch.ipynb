{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfcb46c0-cc95-4bf1-8515-d85b5c2981e3",
   "metadata": {},
   "source": [
    "# RAG Pipeline\n",
    "\n",
    "Onderstaande code is een voorbeeld van een RAG-pipeline met een taalmodel dat lokaal gedraaid kan worden.\n",
    "Het model in kwestie is een quantized versie van de 4B-parameterer-variant van Qwen3.\n",
    "Dit model is kleiner dan 3G GB en daarom geschikt om ook op consumer-grade laptops te draaien.\n",
    "Daarnaast heeft dit model reasoning-capaciteiten (hij \"denkt\" eerst na voordat hij een antwoord geeft) en is redelijk capabel in Nederlands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07394094-47d7-4e2f-be8d-824483dffacb",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Importeer de nodige functionaliteit uit Owlsight voor de RAG-Pipeline.\n",
    "\n",
    "Documentsearcher: Fungeert als retriever voor het zoeken van relevante stukken tekst op basis van een gebruikersvraag\n",
    "\n",
    "SemanticTextSplitter: Split tekst op in chunks voor de retriever\n",
    "\n",
    "DocumentReader: Voor het inlezen en omzetten van tekst van documenten\n",
    "\n",
    "select_processor_type: Koppelt juiste modelfunctionaliteit vanuit owlsight aan model op basis van modelnaam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f093f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stappen runnen llama-cpp-python met owlsight:\n",
    "\n",
    "# uv pip install owlsight\n",
    "\n",
    "# voor macbook, voer deze stappen extra uit:\n",
    "# export FORCE_CMAKE=1                             \n",
    "# export CMAKE_ARGS=\"-DGGML_METAL=on\"\n",
    "\n",
    "# uv pip install -U llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06886b2d-9dda-40ff-a843-d773fb450049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlsight import DocumentSearcher, DocumentReader, SemanticTextSplitter, select_processor_type\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f94d62-60d9-48c4-9688-f1fbc741d537",
   "metadata": {},
   "source": [
    "## Lees documenten in\n",
    "\n",
    "Met de DocumentReader class, gebouwd op Apache Tika, kunnen files in allerlei verschillende extensies naar tekst worden omgezet.\n",
    "Voor het inlezen van een enkele file, kan de read_file methode worden gebruikt.\n",
    "\n",
    "Als alternatief kan ook de read_directory methode worden gebruikt, waarmee een complete directory (recursief) kan worden ingelezen.\n",
    "Dit is handig voor het verwerken van een grote set aan documenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900deb-da7e-4c3f-bd69-763fd0e76d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_path = \"/path/naar/jouw/documenten/hier\"\n",
    "\n",
    "if not os.path.exists(documents_path):\n",
    "    print(f\"path {documents_path} bestaat niet!\")\n",
    "\n",
    "reader = DocumentReader()\n",
    "if not os.path.isdir(documents_path):\n",
    "    content = reader.read_file(documents_path)\n",
    "    documents = {documents_path: content}\n",
    "else:\n",
    "    documents = dict(reader.read_directory(documents_path, recursive=True))\n",
    "    \n",
    "print(documents.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50decb7-9b8b-4869-b41e-9b34016fdae2",
   "metadata": {},
   "source": [
    "## Componenten RAG-pipeline \n",
    "\n",
    "Een typische RAG-pipeline bestaat, naast een taalmodel, uit twee hoofdcomponenten:\n",
    "\n",
    "TextSplitter:\n",
    "Deze component splitst documenten of tekstbestanden in kleinere fragmenten (chunks), \n",
    "vaak op basis van paragrafen, zinnen, of tokens.\n",
    "In Owlsight wordt exclusief gebruik gemaakt van de Sentence-Transformers library.\n",
    "\n",
    "\n",
    "Retriever (met Encoder):\n",
    "Elk fragment wordt omgezet in een vector (numerieke representatie) met behulp van een encoder, \n",
    "meestal een embedding-model. Deze vectoren worden vervolgens opgeslagen. \n",
    "Meestal gebeurt dit in een vector database (zoals FAISS of Weaviate).\n",
    "In Owlsight worden de vectoren opgeslagen met pickle.\n",
    "\n",
    "Bij een gebruikersvraag wordt dezelfde encoder gebruikt om ook de vraag naar een vector om te zetten, waarna de meest relevante fragmenten (dichtstbijzijnde vectoren) worden opgehaald."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5417fd9e-7b7d-4d52-a351-38457f189730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_model = \"NetherlandsForensicInstitute/robbert-2022-dutch-sentence-transformers\"\n",
    "text_splitter=SemanticTextSplitter(model_name=embedding_model, target_chunk_length=500)\n",
    "searcher = DocumentSearcher(documents, \n",
    "                            text_splitter=text_splitter,\n",
    "                            sentence_transformer_model = embedding_model,\n",
    "                            cache_dir=\"documents\", \n",
    "                           cache_dir_suffix=\"rag_pipeline_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb567aa6-b94f-4811-9fd9-2b6040bd3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laat dataframe zien met relevante chunks en scores\n",
    "# aggregated_score is een combinatie van tfidf en het embedding model\n",
    "# het voordeel van het gebruiken van tfidf is dat dit een snelle keysearch is die ook woorden meeneemt waar een embedding model niet per se\n",
    "# op is getraind. Als in een query een \"moderner\" woord als \"fatbike\" staat, wordt deze alsnog meegenomen.\n",
    "retrieved_chunks = searcher.search(\"Stel je vraag over je document hier\", top_k=20, as_context=False)\n",
    "retrieved_chunks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ff099-92e8-4ee8-91c7-bf70d922c1de",
   "metadata": {},
   "source": [
    "# Taalmodel\n",
    "\n",
    "Nu laden we het taalmodel in. Met de select_processor_type functie, kunnen we gelijk het juiste model inladen.\n",
    "In Owlsight gebeurt dit door een abstractielaag, genaamd TextGenerationProcessor.\n",
    "Model-specifieke parameters zijn uitgedrukt met dubbele underscore \"__\", bijvoorbeeld gguf__filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740cc1c1-7bcb-44ef-a11a-801500a67397",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Qwen3-4B-GGUF\"\n",
    "\n",
    "# koppel juiste processor aan model op basis van model_id\n",
    "model_processor_cls = select_processor_type(model_id)\n",
    "\n",
    "model_processor = model_processor_cls(\n",
    "    model_id=model_id,\n",
    "    apply_chat_history = False,\n",
    "    system_prompt=\"Je bent een behulpzame assistent. Als een vraag niet beantwoord kan worden op basis van de context, zeg dan: 'Ik kan de vraag op basis van de context niet beantwoorden.'\",\n",
    "    gguf__verbose=False,\n",
    "    gguf__n_gpu_layers=32,\n",
    "    gguf__n_ctx=4000,\n",
    "    gguf__filename=\"Qwen3-4B-UD-Q5_K_XL.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa28ae-d0f3-4986-83cb-49667b7d23bb",
   "metadata": {},
   "source": [
    "# Chatbot met RAG\n",
    "\n",
    "Nu hebben we alle elementen voor een chatbot met RAG-pipeline.\n",
    "In ons voorbeeld is dit een \"instruct\"-bot, waarbij de chathistorie bij elke nieuwe vraag of query wordt ververst.\n",
    "Dit zodat we niet opnieuw de nieuwe chunks uit een vervolgvraag meegeven.\n",
    "Met behulp van de searcher (retriever), geven we elke keer de 5 meest relevante documenten mee op basis van de userquery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3f0c5-1bdc-4225-84a2-265d57211d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "model_processor.clear_history()\n",
    "while True:\n",
    "    query = input(\"(Typ 'stop' voor stoppen, 'clear' voor nieuwe chat)\\nJij: \")\n",
    "    if query.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
    "        print(\"Chatbot: Tot ziens!\")\n",
    "        break\n",
    "    # elif query.lower() == \"clear\":\n",
    "    #     model_processor.clear_history()\n",
    "    #     clear_output(wait=True)  # wist de output van notebookcel\n",
    "    #     print(\"Chatgeschiedenis is gewist en uitvoer is ververst.\")\n",
    "    #     continue\n",
    "\n",
    "    # Zoek relevante context\n",
    "    try:\n",
    "        context=searcher.search(query, top_k=top_k, as_context=True)\n",
    "        seperator = \"-\" * 50\n",
    "        print(f\"Gebruikte context:\\n{seperator}\\n{context}\\n{seperator}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fout bij zoeken: {e}\")\n",
    "        context = \"\"\n",
    "\n",
    "    # voeg context toe aan de query\n",
    "    full_query = query + \"\\n\\nGebruik onderstaande tekst om je antwoord te geven:\\n\" + context\n",
    "\n",
    "    # Genereer antwoord\n",
    "    try:\n",
    "        print(\"Chatbot:\")\n",
    "        response = model_processor.generate(full_query, max_new_tokens=800)\n",
    "        model_processor.clear_history()\n",
    "    except Exception as e:\n",
    "        response = f\"Fout bij genereren: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d513466-c7c3-40c8-b54e-6461c70f190b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
