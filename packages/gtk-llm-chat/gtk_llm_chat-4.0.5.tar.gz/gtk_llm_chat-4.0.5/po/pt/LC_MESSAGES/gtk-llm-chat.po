# Portuguese translations for gtk-llm-chat package.
# Copyright (C) 2025 THE PACKAGE'S COPYRIGHT HOLDER
# This file is distributed under the same license as the gtk-llm-chat package.
# Sebastian Silva <sebastian@fuentelibre.org>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: gtk-llm-chat 0.1\n"
"Report-Msgid-Bugs-To: your@email.com\n"
"POT-Creation-Date: 2025-06-04 18:35-0500\n"
"PO-Revision-Date: 2025-06-04 18:50-0500\n"
"Last-Translator: Sebastian Silva <sebastian@fuentelibre.org>\n"
"Language-Team: Portuguese <pt@tp.org.pt>\n"
"Language: pt\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"

#: gtk_llm_chat/chat_window.py:135
msgid "Model Settings"
msgstr "Configurações do modelo"

#: gtk_llm_chat/chat_window.py:141
msgid "Rename"
msgstr "Renomear"

#: gtk_llm_chat/chat_window.py:211
msgid "Send"
msgstr "Enviar"

#: gtk_llm_chat/chat_sidebar.py:65 gtk_llm_chat/chat_sidebar.py:278
#: gtk_llm_chat/wide_model_selector.py:454
msgid "Model"
msgstr "Modelo"

#: gtk_llm_chat/chat_sidebar.py:69
msgid "Change Model"
msgstr "Alterar modelo"

#: gtk_llm_chat/chat_sidebar.py:70 gtk_llm_chat/chat_sidebar.py:312
#: gtk_llm_chat/chat_sidebar.py:432 gtk_llm_chat/wide_model_selector.py:516
msgid "Provider"
msgstr "Provedor"

#: gtk_llm_chat/chat_sidebar.py:79
msgid "Set as Default Model"
msgstr "Definir como modelo padrão"

#: gtk_llm_chat/chat_sidebar.py:88 gtk_llm_chat/chat_sidebar.py:138
msgid "Model Parameters"
msgstr "Parâmetros do modelo"

#: gtk_llm_chat/chat_sidebar.py:97
#, fuzzy
msgid "Conversation"
msgstr "Conversa"

#: gtk_llm_chat/chat_sidebar.py:100 gtk_llm_chat/chat_application.py:417
msgid "Delete Conversation"
msgstr "Apagar conversa"

#: gtk_llm_chat/chat_sidebar.py:110
msgid "Information"
msgstr "Informação"

#: gtk_llm_chat/chat_sidebar.py:112
msgid "About"
msgstr "Sobre"

#: gtk_llm_chat/chat_sidebar.py:121
msgid "Actions"
msgstr "Ações"

#: gtk_llm_chat/chat_sidebar.py:145
msgid "Temperature"
msgstr "Temperatura"

#: gtk_llm_chat/chat_sidebar.py:158
msgid "System Prompt"
msgstr "Prompt do sistema"

#: gtk_llm_chat/chat_sidebar.py:165
msgid "Parameters"
msgstr "Parâmetros"

#: gtk_llm_chat/chat_sidebar.py:241 gtk_llm_chat/chat_sidebar.py:308
#: gtk_llm_chat/chat_sidebar.py:310 gtk_llm_chat/chat_sidebar.py:432
#: gtk_llm_chat/model_selection.py:129
msgid "Unknown Provider"
msgstr "Provedor desconhecido"

#: gtk_llm_chat/chat_sidebar.py:247
msgid "Set Default Model"
msgstr "Definir modelo padrão"

#: gtk_llm_chat/chat_sidebar.py:248
msgid "Do you want to set"
msgstr "Deseja definir"

#: gtk_llm_chat/chat_sidebar.py:248
msgid "from"
msgstr "de"

#: gtk_llm_chat/chat_sidebar.py:248
msgid "as the default model for new conversations?"
msgstr "como modelo padrão para novas conversas?"

#: gtk_llm_chat/chat_sidebar.py:251 gtk_llm_chat/chat_sidebar.py:330
#: gtk_llm_chat/model_selector.py:291 gtk_llm_chat/chat_application.py:420
#: gtk_llm_chat/wide_model_selector.py:310
msgid "Cancel"
msgstr "Cancelar"

#: gtk_llm_chat/chat_sidebar.py:252
msgid "Set as Default"
msgstr "Definir como padrão"

#: gtk_llm_chat/chat_sidebar.py:278
msgid "set as default"
msgstr "definido como padrão"

#: gtk_llm_chat/chat_sidebar.py:327
msgid "Set System Prompt"
msgstr "Definir prompt do sistema"

#: gtk_llm_chat/chat_sidebar.py:328
msgid "Enter the system prompt for the AI model:"
msgstr "Digite o prompt do sistema para o modelo de IA:"

#: gtk_llm_chat/chat_sidebar.py:331
msgid "Set"
msgstr "Definir"

#: gtk_llm_chat/chat_sidebar.py:381
msgid "Current"
msgstr "Atual"

#: gtk_llm_chat/chat_sidebar.py:383
msgid "Not set"
msgstr "Não definido"

#: gtk_llm_chat/chat_sidebar.py:413
msgid "This is the current default model"
msgstr "Este é o modelo padrão atual"

#: gtk_llm_chat/chat_sidebar.py:417
msgid "Set as default model"
msgstr "Definir como modelo padrão"

#: gtk_llm_chat/model_selector.py:46
msgid "Providers"
msgstr "Provedores"

#: gtk_llm_chat/model_selector.py:50
msgid "Models"
msgstr "Modelos"

#: gtk_llm_chat/model_selector.py:68
msgid "Select Provider"
msgstr "Selecionar provedor"

#: gtk_llm_chat/model_selector.py:95
msgid "Select Model"
msgstr "Selecionar modelo"

#: gtk_llm_chat/model_selector.py:124
msgid "No models found"
msgstr "Nenhum modelo encontrado"

#: gtk_llm_chat/model_selector.py:145
msgid "models"
msgstr "Modelos"

#: gtk_llm_chat/model_selector.py:150
msgid "API key required"
msgstr "Chave API obrigatória"

#: gtk_llm_chat/model_selector.py:152
msgid "No models"
msgstr "Nenhum modelo encontrado"

#: gtk_llm_chat/model_selector.py:210 gtk_llm_chat/model_selector.py:335
msgid "API Key is configured"
msgstr "Chave API configurada"

#: gtk_llm_chat/model_selector.py:211 gtk_llm_chat/model_selector.py:336
msgid "Change Key"
msgstr "Alterar chave"

#: gtk_llm_chat/model_selector.py:216 gtk_llm_chat/model_selector.py:340
msgid "API Key Required"
msgstr "Chave API obrigatória"

#: gtk_llm_chat/model_selector.py:217 gtk_llm_chat/model_selector.py:292
#: gtk_llm_chat/model_selector.py:341 gtk_llm_chat/wide_model_selector.py:311
msgid "Set Key"
msgstr "Definir chave"

#: gtk_llm_chat/model_selector.py:235
msgid "No models available"
msgstr "Nenhum modelo disponível"

#: gtk_llm_chat/model_selector.py:236
msgid "Configure an API key to access models from this provider"
msgstr "Configure uma chave API para aceder a modelos deste provedor"

#: gtk_llm_chat/model_selector.py:241 gtk_llm_chat/wide_model_selector.py:251
msgid "No models found for this provider"
msgstr "Nenhum modelo encontrado para este provedor"

#: gtk_llm_chat/model_selector.py:288 gtk_llm_chat/wide_model_selector.py:307
msgid "Enter API Key"
msgstr "Digite a chave API"

#: gtk_llm_chat/model_selector.py:289 gtk_llm_chat/wide_model_selector.py:308
msgid "Enter the API key for"
msgstr "Digite a chave API para"

#: gtk_llm_chat/model_selector.py:298 gtk_llm_chat/wide_model_selector.py:317
msgid "Paste your API key here"
msgstr "Cole sua chave API aqui"

#: gtk_llm_chat/model_selector.py:356
msgid "Model Selector Test"
msgstr "Teste do Seletor de Modelo"

#: gtk_llm_chat/tray_applet.py:103 gtk_llm_chat/tray_applet.py:239
#: gtk_llm_chat/welcome.py:59 gtk_llm_chat/llm_client.py:22
msgid "New Conversation"
msgstr "Nova Conversa"

#: gtk_llm_chat/tray_applet.py:107 gtk_llm_chat/tray_applet.py:241
msgid "Quit"
msgstr "Sair"

#: gtk_llm_chat/tray_applet.py:210 gtk_llm_chat/chat_application.py:68
msgid ""
"\n"
"Closing application..."
msgstr ""
"\n"
"A fechar a aplicação..."

#: gtk_llm_chat/tray_applet.py:225
msgid "LLM Conversations"
msgstr "Conversas LLM"

#: gtk_llm_chat/welcome.py:20
msgid "Tray applet"
msgstr "Applet da bandeja"

#: gtk_llm_chat/welcome.py:20
msgid "Default Model"
msgstr "Modelo Padrão"

#: gtk_llm_chat/welcome.py:53
msgid "Next"
msgstr "Próximo"

#: gtk_llm_chat/welcome.py:117
msgid "Own the conversation."
msgstr "Controle a conversa."

#: gtk_llm_chat/welcome.py:118
msgid "Use any model you want. Your conversations are stored locally."
msgstr "Use qualquer modelo que desejar. Suas conversas são armazenadas localmente."

#: gtk_llm_chat/welcome.py:119
msgid "This wizard will guide you through the initial setup"
msgstr "Este assistente irá guiá-lo através da configuração inicial"

#: gtk_llm_chat/welcome.py:125
msgid "Start"
msgstr "Iniciar"

#: gtk_llm_chat/welcome.py:157
msgid "Access conversations from the convenience of your system tray"
msgstr "Aceda a conversas a partir da conveniência da sua bandeja do sistema"

#: gtk_llm_chat/welcome.py:163
msgid "Would you like to start the applet with your session?"
msgstr "Gostaria de iniciar o applet com a sua sessão?"

#: gtk_llm_chat/welcome.py:173
msgid "Yes, with my session"
msgstr "Sim, com a minha sessão"

#: gtk_llm_chat/welcome.py:174
msgid "No, only when I start the app"
msgstr "Não, apenas quando eu iniciar a aplicação"

#: gtk_llm_chat/welcome.py:209
msgid "Loading model selection..."
msgstr "A carregar seleção de modelo..."

#: gtk_llm_chat/welcome.py:219
msgid "Ready to start!"
msgstr "Pronto para começar!"

#: gtk_llm_chat/welcome.py:429
#, fuzzy
msgid "Set API Key"
msgstr "Definir chave"

#: gtk_llm_chat/welcome.py:429
#, fuzzy
msgid "Change API Key"
msgstr "Alterar chave"

#: gtk_llm_chat/chat_application.py:193
msgid "Error: _version.py not found"
msgstr "Erro: _version.py não encontrado"

#: gtk_llm_chat/chat_application.py:418
msgid "Are you sure you want to delete the conversation?"
msgstr "Tem a certeza que quer apagar a conversa?"

#: gtk_llm_chat/chat_application.py:421
msgid "Delete"
msgstr "Apagar"

#: gtk_llm_chat/chat_application.py:443
msgid "Gtk LLM Chat"
msgstr "Gtk LLM Chat"

#: gtk_llm_chat/chat_application.py:446
msgid "A frontend for LLM"
msgstr "Um frontend para LLM"

#: gtk_llm_chat/wide_model_selector.py:92
msgid ""
"Please select a provider from the list on the left.\n"
"Then, choose a model from the list that appears here."
msgstr "Por favor, selecione um provedor da lista à esquerda.\nDepois, escolha um modelo da lista que aparece aqui."

#: gtk_llm_chat/wide_model_selector.py:127
msgid "Most AI models require an API key"
msgstr "A maioria dos modelos de IA requer uma chave API"

#: gtk_llm_chat/wide_model_selector.py:134
msgid ""
"You'll need to register with each provider to obtain these authentication "
"tokens."
msgstr "Precisará de se registar em cada provedor para obter estes tokens de autenticação."

#: gtk_llm_chat/wide_model_selector.py:147
msgid "No Selection"
msgstr "Nenhuma seleção"

#: gtk_llm_chat/wide_model_selector.py:162
msgid "No models or providers found."
msgstr "Nenhum modelo encontrado"

#: gtk_llm_chat/wide_model_selector.py:163
msgid "Error"
msgstr "Erro"

#: gtk_llm_chat/wide_model_selector.py:442
msgid "Model information not available"
msgstr "Informação do modelo não disponível"

#: gtk_llm_chat/wide_model_selector.py:443
msgid "Unable to retrieve model details"
msgstr "Não foi possível obter os detalhes do modelo"

#: gtk_llm_chat/wide_model_selector.py:467
msgid "Aliases"
msgstr "Aliases"

#: gtk_llm_chat/wide_model_selector.py:478
msgid "API Key"
msgstr "Chave API"

#: gtk_llm_chat/wide_model_selector.py:487
msgid "Required • Set"
msgstr "Obrigatório • Definido"

#: gtk_llm_chat/wide_model_selector.py:493
msgid "Required • Not set"
msgstr "Obrigatório • Não definido"

#: gtk_llm_chat/wide_model_selector.py:499
msgid "Not required"
msgstr "Não obrigatório"

#: gtk_llm_chat/wide_model_selector.py:510
#: gtk_llm_chat/wide_model_selector.py:511
#, fuzzy
msgid "Unknown"
msgstr "Provedor desconhecido"

#: gtk_llm_chat/wide_model_selector.py:512
msgid "Plugin"
msgstr "Plugin"

#: gtk_llm_chat/single_instance.py:26 gtk_llm_chat/single_instance.py:32
msgid "Another instance is already running."
msgstr "Outra instância já está em execução."

#: gtk_llm_chat/llm_client.py:234
msgid "LLMClient: Ignoring invalid temperature:"
msgstr "LLMClient: A ignorar temperatura inválida:"

#: gtk_llm_chat/llm_client.py:269
msgid "LLMClient: Starting stream processing..."
msgstr "LLMClient: A iniciar o processamento de stream..."

#: gtk_llm_chat/llm_client.py:272
msgid "LLMClient: Stream processing cancelled externally."
msgstr "LLMClient: Processamento de stream cancelado externamente."

#: gtk_llm_chat/llm_client.py:278
msgid "LLMClient: Stream finished normally."
msgstr "LLMClient: Stream terminado normalmente."

#: gtk_llm_chat/model_selection.py:119 gtk_llm_chat/model_selection.py:125
msgid "Local/Other"
msgstr "Local/Outro"

#~ msgid "Settings"
#~ msgstr "Configurações"

#~ msgid "Error reading keys file"
#~ msgstr "Erro ao ler o arquivo de chaves"

#~ msgid "Check File"
#~ msgstr "Verificar arquivo"

#~ msgid "Error accessing keys file"
#~ msgstr "Erro ao acessar o arquivo de chaves"

#~ msgid "Check Permissions"
#~ msgstr "Verificar permissões"

#~ msgid "LLMClient: Cancel request received."
#~ msgstr "LLMClient: Pedido de cancelamento recebido."

#~ msgid "LLMClient: Terminating active stream thread."
#~ msgstr "LLMClient: A terminar a thread de stream ativa."

#~ msgid "LLMClient: No active stream thread to cancel."
#~ msgstr "LLMClient: Nenhuma thread de stream ativa para cancelar."

#~ msgid "LLMClient: Error - Conversación no disponible para cargar historial."
#~ msgstr "LLMClient: Erro - Conversa não disponível para carregar histórico."

#, fuzzy
#~ msgid "LLMClient: Historial cargado. Total de respuestas en conversación: "
#~ msgstr "LLMClient: Histórico carregado. Total de respostas na conversa: "

#~ msgid "Exiting..."
#~ msgstr "Saindo..."

#~ msgid "LLM Chat"
#~ msgstr "LLM Chat"

#~ msgid "Error: conversation_id is required to add to history."
#~ msgstr "Erro: o conversation_id é obrigatório para adicionar ao histórico."

#~ msgid "Error: conversation_id is required to create the conversation."
#~ msgstr "Erro: o conversation_id é obrigatório para criar a conversa."

#~ msgid ""
#~ "LLMClient: Error - Attempting to load history with model initialization "
#~ "error."
#~ msgstr ""
#~ "LLMClient: Erro - A tentar carregar o histórico com erro de inicialização "
#~ "do modelo."

#~ msgid ""
#~ "LLMClient: Error - Attempting to load history without initialized "
#~ "conversation."
#~ msgstr ""
#~ "LLMClient: Erro - A tentar carregar o histórico sem conversa inicializada."

#~ msgid ""
#~ "LLMClient: Warning - Assistant response without previous user prompt in "
#~ "history."
#~ msgstr ""
#~ "LLMClient: Aviso - Resposta do assistente sem pedido anterior do "
#~ "utilizador no histórico."
