"""
AsyncFrame è‡ªåŠ¨è¿ç§»ç³»ç»Ÿ
åœ¨åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æŸ¥æ¨¡å‹å˜åŒ–å¹¶åŒæ­¥åˆ°æ•°æ®åº“
æ”¯æŒå¢é‡æ›´æ–°ï¼Œé¿å…æ•°æ®ä¸¢å¤±
"""

import hashlib
import json
import logging
import inspect
import importlib
import importlib.util
import pkgutil
from pathlib import Path
from typing import Dict, List, Set, Any, Optional, Type
from datetime import datetime

from .models import Model
from .database import db_manager
from .migrations import migration_manager


logger = logging.getLogger(__name__)


class TableSchema:
    """æ•°æ®åº“è¡¨ç»“æ„ä¿¡æ¯"""
    
    def __init__(self, table_name: str):
        self.table_name = table_name
        self.columns = {}  # {column_name: column_info}
        self.indexes = {}
        self.constraints = {}


class FieldComparator:
    """å­—æ®µå¯¹æ¯”å™¨"""
    
    @staticmethod
    def get_column_definition(field, db_type: str) -> str:
        """æ ¹æ®å­—æ®µç±»å‹ç”Ÿæˆæ•°æ®åº“åˆ—å®šä¹‰"""
        field_type = field.__class__.__name__
        
        # ç±»å‹æ˜ å°„
        if db_type == 'mysql':
            type_mapping = {
                'CharField': f"VARCHAR({getattr(field, 'max_length', 255)})",
                'TextField': "TEXT",
                'IntegerField': "INT",
                'BigIntegerField': "BIGINT", 
                'FloatField': "FLOAT",
                'BooleanField': "BOOLEAN",
                'DateTimeField': "DATETIME",
                'DateField': "DATE",
                'JSONField': "JSON"
            }
        elif db_type == 'postgresql':
            type_mapping = {
                'CharField': f"VARCHAR({getattr(field, 'max_length', 255)})",
                'TextField': "TEXT",
                'IntegerField': "INTEGER",
                'BigIntegerField': "BIGINT",
                'FloatField': "REAL", 
                'BooleanField': "BOOLEAN",
                'DateTimeField': "TIMESTAMP",
                'DateField': "DATE",
                'JSONField': "JSONB"
            }
        elif db_type == 'sqlite':
            type_mapping = {
                'CharField': "TEXT",
                'TextField': "TEXT", 
                'IntegerField': "INTEGER",
                'BigIntegerField': "INTEGER",
                'FloatField': "REAL",
                'BooleanField': "INTEGER",
                'DateTimeField': "TEXT",
                'DateField': "TEXT",
                'JSONField': "TEXT"
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
        
        column_type = type_mapping.get(field_type, "TEXT")
        
        # æ„å»ºå®Œæ•´åˆ—å®šä¹‰
        definition = column_type
        
        # NULL/NOT NULL
        if not getattr(field, 'null', True):
            definition += " NOT NULL"
        
        # PRIMARY KEY
        if getattr(field, 'primary_key', False):
            if db_type == 'mysql':
                definition += " AUTO_INCREMENT PRIMARY KEY"
            elif db_type == 'postgresql':
                definition = "SERIAL PRIMARY KEY"
            elif db_type == 'sqlite':
                definition += " PRIMARY KEY AUTOINCREMENT"
        
        # UNIQUE
        elif getattr(field, 'unique', False):
            definition += " UNIQUE"
        
        # DEFAULT
        default = getattr(field, 'default', None)
        if default is not None and not callable(default):
            if isinstance(default, str):
                definition += f" DEFAULT '{default}'"
            elif isinstance(default, bool):
                definition += f" DEFAULT {1 if default else 0}"
            else:
                definition += f" DEFAULT {default}"
        
        return definition
    
    @staticmethod
    def compare_columns(field_name: str, model_field, db_column_info: Dict, db_type: str) -> List[str]:
        """å¯¹æ¯”æ¨¡å‹å­—æ®µå’Œæ•°æ®åº“åˆ—ï¼Œè¿”å›éœ€è¦æ‰§è¡Œçš„SQLè¯­å¥"""
        changes = []
        
        # è·å–æœŸæœ›çš„åˆ—å®šä¹‰
        expected_def = FieldComparator.get_column_definition(model_field, db_type)
        
        # æ¯”è¾ƒç±»å‹
        expected_type = expected_def.split()[0]
        actual_type = db_column_info.get('Type', '').upper()
        
        # ç®€åŒ–ç±»å‹å¯¹æ¯”ï¼ˆè¿™é‡Œå¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼‰
        if not FieldComparator._types_compatible(expected_type, actual_type, db_type):
            # ç±»å‹ä¸å…¼å®¹ï¼Œéœ€è¦ä¿®æ”¹
            changes.append(f"MODIFY COLUMN {field_name} {expected_def}")
        
        # æ¯”è¾ƒNULLçº¦æŸ
        expected_null = getattr(model_field, 'null', True)
        actual_null = db_column_info.get('Null', 'YES') == 'YES'
        
        if expected_null != actual_null:
            null_constraint = "NULL" if expected_null else "NOT NULL"
            changes.append(f"MODIFY COLUMN {field_name} {expected_def}")
        
        return changes
    
    @staticmethod
    def _types_compatible(expected: str, actual: str, db_type: str) -> bool:
        """æ£€æŸ¥ç±»å‹æ˜¯å¦å…¼å®¹"""
        # ç®€åŒ–çš„ç±»å‹å…¼å®¹æ€§æ£€æŸ¥
        if db_type == 'mysql':
            type_groups = {
                'VARCHAR': ['VARCHAR', 'CHAR'],
                'TEXT': ['TEXT', 'LONGTEXT', 'MEDIUMTEXT'],
                'INT': ['INT', 'INTEGER'],
                'BIGINT': ['BIGINT'],
                'FLOAT': ['FLOAT', 'DOUBLE'],
                'BOOLEAN': ['TINYINT', 'BOOLEAN'],
                'DATETIME': ['DATETIME', 'TIMESTAMP'],
                'DATE': ['DATE'],
                'JSON': ['JSON']
            }
            
            for group_type, variants in type_groups.items():
                if expected.startswith(group_type) and any(actual.startswith(v) for v in variants):
                    return True
        
        return expected.upper() == actual.upper()


class ModelScanner:
    """æ¨¡å‹æ‰«æå™¨ - è‡ªåŠ¨å‘ç°é¡¹ç›®ä¸­çš„æ¨¡å‹ç±»"""
    
    def __init__(self, scan_paths: List[str] = None):
        self.scan_paths = scan_paths or ['models', 'app.models', 'apps']
        self._discovered_models = {}
        self._model_registry = {}
    
    def discover_models(self) -> Dict[str, Type[Model]]:
        """è‡ªåŠ¨å‘ç°æ‰€æœ‰æ¨¡å‹ç±»"""
        models = {}
        
        # æ‰«ææŒ‡å®šè·¯å¾„
        for path in self.scan_paths:
            try:
                models.update(self._scan_module(path))
            except ImportError:
                # å¦‚æœæ¨¡å—ä¸å­˜åœ¨å°±è·³è¿‡
                continue
        
        # æ‰«æå½“å‰å·¥ä½œç›®å½•ä¸­çš„Pythonæ–‡ä»¶
        models.update(self._scan_directory('.'))
        
        # æ›´æ–°ç¼“å­˜
        self._discovered_models = models
        logger.info(f"ğŸ” å‘ç° {len(models)} ä¸ªæ¨¡å‹ç±»: {list(models.keys())}")
        
        return models
    
    def _scan_module(self, module_path: str) -> Dict[str, Type[Model]]:
        """æ‰«ææŒ‡å®šæ¨¡å—"""
        models = {}
        
        try:
            module = importlib.import_module(module_path)
            models.update(self._extract_models_from_module(module))
        except ImportError as e:
            logger.debug(f"æ— æ³•å¯¼å…¥æ¨¡å— {module_path}: {e}")
        
        return models
    
    def _scan_directory(self, directory: str) -> Dict[str, Type[Model]]:
        """æ‰«æç›®å½•ä¸­çš„Pythonæ–‡ä»¶"""
        models = {}
        
        try:
            for file_path in Path(directory).glob("**/*.py"):
                if file_path.name.startswith('.') or file_path.name.startswith('__'):
                    continue
                
                # è½¬æ¢æ–‡ä»¶è·¯å¾„ä¸ºæ¨¡å—è·¯å¾„
                module_path = str(file_path.with_suffix('')).replace('/', '.')
                
                try:
                    spec = importlib.util.spec_from_file_location(module_path, file_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        models.update(self._extract_models_from_module(module))
                except Exception as e:
                    logger.debug(f"æ‰«ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                    continue
        
        except Exception as e:
            logger.debug(f"æ‰«æç›®å½• {directory} æ—¶å‡ºé”™: {e}")
        
        return models
    
    def _extract_models_from_module(self, module) -> Dict[str, Type[Model]]:
        """ä»æ¨¡å—ä¸­æå–æ¨¡å‹ç±»"""
        models = {}
        
        for name, obj in inspect.getmembers(module):
            if (inspect.isclass(obj) and 
                issubclass(obj, Model) and 
                obj is not Model and
                hasattr(obj, '_meta')):
                
                models[f"{obj.__module__}.{obj.__name__}"] = obj
        
        return models
    
    def get_models(self) -> Dict[str, Type[Model]]:
        """è·å–å·²å‘ç°çš„æ¨¡å‹"""
        if not self._discovered_models:
            self.discover_models()
        return self._discovered_models


class ModelFingerprinter:
    """æ¨¡å‹æŒ‡çº¹ç”Ÿæˆå™¨ - ä¸ºæ¨¡å‹ç»“æ„ç”Ÿæˆå”¯ä¸€æ ‡è¯†"""
    
    @staticmethod
    def generate_model_fingerprint(model_class: Type[Model]) -> str:
        """ä¸ºæ¨¡å‹ç”ŸæˆæŒ‡çº¹"""
        model_info = {
            'name': model_class.__name__,
            'table_name': model_class._meta.table_name,
            'fields': {}
        }
        
        # æ”¶é›†å­—æ®µä¿¡æ¯
        for field_name, field in model_class._meta.fields.items():
            field_info = {
                'type': field.__class__.__name__,
                'null': getattr(field, 'null', True),
                'unique': getattr(field, 'unique', False),
                'primary_key': getattr(field, 'primary_key', False),
                'max_length': getattr(field, 'max_length', None),
                'default': str(getattr(field, 'default', None)),
                'auto_now': getattr(field, 'auto_now', False),
                'auto_now_add': getattr(field, 'auto_now_add', False)
            }
            model_info['fields'][field_name] = field_info
        
        # ç”ŸæˆJSONå­—ç¬¦ä¸²å¹¶è®¡ç®—MD5
        json_str = json.dumps(model_info, sort_keys=True)
        return hashlib.md5(json_str.encode()).hexdigest()
    
    @staticmethod
    def generate_models_fingerprint(models: Dict[str, Type[Model]]) -> str:
        """ä¸ºæ‰€æœ‰æ¨¡å‹ç”Ÿæˆæ€»ä½“æŒ‡çº¹"""
        all_fingerprints = {}
        
        for model_name, model_class in models.items():
            all_fingerprints[model_name] = ModelFingerprinter.generate_model_fingerprint(model_class)
        
        # ç”Ÿæˆæ€»ä½“æŒ‡çº¹
        json_str = json.dumps(all_fingerprints, sort_keys=True)
        return hashlib.md5(json_str.encode()).hexdigest()


class AutoMigrationManager:
    """è‡ªåŠ¨è¿ç§»ç®¡ç†å™¨ - æ”¯æŒå¢é‡æ›´æ–°"""
    
    def __init__(self, enable_column_deletion: bool = False, enable_table_deletion: bool = False):
        self.scanner = ModelScanner()
        self.fingerprinter = ModelFingerprinter()
        self.field_comparator = FieldComparator()
        self.schema_table = "asyncframe_schema_versions"
        self._current_models = {}
        # æ–°å¢é…ç½®é€‰é¡¹
        self.enable_column_deletion = enable_column_deletion
        self.enable_table_deletion = enable_table_deletion
    
    async def initialize(self):
        """åˆå§‹åŒ–è‡ªåŠ¨è¿ç§»ç³»ç»Ÿ"""
        await self._ensure_schema_table()
        logger.info("âœ… è‡ªåŠ¨è¿ç§»ç³»ç»Ÿå·²åˆå§‹åŒ–")
    
    async def _ensure_schema_table(self):
        """ç¡®ä¿æ¨¡å¼ç‰ˆæœ¬è¡¨å­˜åœ¨"""
        pool = db_manager.get_database()
        db_type = pool.config.db_type
        
        if db_type == 'mysql':
            create_sql = f"""
            CREATE TABLE IF NOT EXISTS {self.schema_table} (
                id INT AUTO_INCREMENT PRIMARY KEY,
                fingerprint VARCHAR(32) NOT NULL UNIQUE,
                model_count INT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
            )
            """
        elif db_type == 'postgresql':
            create_sql = f"""
            CREATE TABLE IF NOT EXISTS {self.schema_table} (
                id SERIAL PRIMARY KEY,
                fingerprint VARCHAR(32) NOT NULL UNIQUE,
                model_count INT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
        elif db_type == 'sqlite':
            create_sql = f"""
            CREATE TABLE IF NOT EXISTS {self.schema_table} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                fingerprint VARCHAR(32) NOT NULL UNIQUE,
                model_count INTEGER NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
        
        await pool.execute(create_sql)
    
    async def check_and_sync(self) -> bool:
        """æ£€æŸ¥æ¨¡å‹å˜åŒ–å¹¶è‡ªåŠ¨åŒæ­¥"""
        logger.info("ğŸ” æ£€æŸ¥æ¨¡å‹å˜åŒ–...")
        
        # å‘ç°å½“å‰æ¨¡å‹
        current_models = self.scanner.discover_models()
        self._current_models = current_models
        
        if not current_models:
            logger.info("âš ï¸  æœªå‘ç°ä»»ä½•æ¨¡å‹ï¼Œè·³è¿‡åŒæ­¥")
            return False
        
        # ç”Ÿæˆå½“å‰æ¨¡å‹æŒ‡çº¹
        current_fingerprint = self.fingerprinter.generate_models_fingerprint(current_models)
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æœ€æ–°æŒ‡çº¹
        last_fingerprint = await self._get_last_fingerprint()
        
        if last_fingerprint == current_fingerprint:
            logger.info("âœ… æ¨¡å‹æ— å˜åŒ–ï¼Œè·³è¿‡åŒæ­¥")
            return False
        
        # æ¨¡å‹æœ‰å˜åŒ–ï¼Œéœ€è¦åŒæ­¥
        if last_fingerprint is None:
            logger.info("ğŸ†• é¦–æ¬¡è¿è¡Œï¼Œåˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨...")
            await self._create_all_tables(current_models)
        else:
            logger.info("ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹å˜åŒ–ï¼Œå¢é‡åŒæ­¥æ•°æ®åº“ç»“æ„...")
            await self._incremental_sync(current_models)
        
        # è®°å½•æ–°çš„æŒ‡çº¹
        await self._record_fingerprint(current_fingerprint, len(current_models))
        
        logger.info("âœ… æ•°æ®åº“åŒæ­¥å®Œæˆ")
        return True
    
    async def _get_last_fingerprint(self) -> Optional[str]:
        """è·å–æœ€åä¸€æ¬¡è®°å½•çš„æŒ‡çº¹"""
        try:
            pool = db_manager.get_database()
            query = f"SELECT fingerprint FROM {self.schema_table} ORDER BY applied_at DESC LIMIT 1"
            result = await pool.fetch_one(query)
            return result['fingerprint'] if result else None
        except Exception:
            # å¦‚æœè¡¨ä¸å­˜åœ¨æˆ–æŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›None
            return None
    
    async def _record_fingerprint(self, fingerprint: str, model_count: int):
        """è®°å½•æ–°çš„æŒ‡çº¹ï¼ˆå¦‚æœå·²å­˜åœ¨åˆ™æ›´æ–°ï¼‰"""
        pool = db_manager.get_database()
        db_type = pool.config.db_type
        
        try:
            if db_type == 'mysql':
                # MySQLä½¿ç”¨INSERT ... ON DUPLICATE KEY UPDATE
                query = f"""
                INSERT INTO {self.schema_table} (fingerprint, model_count, created_at, applied_at) 
                VALUES (:fingerprint, :model_count, NOW(), NOW())
                ON DUPLICATE KEY UPDATE 
                    model_count = VALUES(model_count),
                    applied_at = NOW()
                """
            elif db_type == 'postgresql':
                # PostgreSQLä½¿ç”¨INSERT ... ON CONFLICT
                query = f"""
                INSERT INTO {self.schema_table} (fingerprint, model_count, created_at, applied_at) 
                VALUES (:fingerprint, :model_count, NOW(), NOW())
                ON CONFLICT (fingerprint) DO UPDATE SET
                    model_count = EXCLUDED.model_count,
                    applied_at = NOW()
                """
            elif db_type == 'sqlite':
                # SQLiteä½¿ç”¨INSERT OR REPLACE
                query = f"""
                INSERT OR REPLACE INTO {self.schema_table} (fingerprint, model_count, created_at, applied_at) 
                VALUES (:fingerprint, :model_count, datetime('now'), datetime('now'))
                """
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {db_type}")
            
            await pool.execute(query, {
                'fingerprint': fingerprint,
                'model_count': model_count
            })
            logger.debug(f"âœ… è®°å½•æŒ‡çº¹: {fingerprint}")
            
        except Exception as e:
            logger.error(f"âŒ è®°å½•æŒ‡çº¹å¤±è´¥: {e}")
            # å¦‚æœUPSERTå¤±è´¥ï¼Œå°è¯•ç®€å•çš„UPDATE
            try:
                update_query = f"""
                UPDATE {self.schema_table} 
                SET model_count = :model_count, applied_at = CURRENT_TIMESTAMP 
                WHERE fingerprint = :fingerprint
                """
                result = await pool.execute(update_query, {
                    'fingerprint': fingerprint,
                    'model_count': model_count
                })
                logger.debug(f"âœ… æ›´æ–°æŒ‡çº¹è®°å½•: {fingerprint}")
            except Exception as update_error:
                logger.error(f"âŒ æ›´æ–°æŒ‡çº¹è®°å½•ä¹Ÿå¤±è´¥: {update_error}")
                raise
    
    async def _create_all_tables(self, models: Dict[str, Type[Model]]):
        """åˆ›å»ºæ‰€æœ‰æ•°æ®åº“è¡¨"""
        from .models import create_tables
        
        model_classes = list(models.values())
        await create_tables(*model_classes)
        
        logger.info(f"ğŸ‰ æˆåŠŸåˆ›å»º {len(model_classes)} ä¸ªæ•°æ®åº“è¡¨")
    
    async def _incremental_sync(self, models: Dict[str, Type[Model]]):
        """å¢é‡åŒæ­¥æ•°æ®åº“ç»“æ„"""
        pool = db_manager.get_database()
        db_type = pool.config.db_type
        
        # è·å–ç°æœ‰è¡¨åˆ—è¡¨
        existing_tables = await self._get_existing_tables()
        
        # å¤„ç†æ¨¡å‹å¯¹åº”çš„è¡¨
        for model_name, model_class in models.items():
            table_name = model_class._meta.table_name
            
            if table_name not in existing_tables:
                # æ–°è¡¨ï¼Œç›´æ¥åˆ›å»º
                logger.info(f"ğŸ“ åˆ›å»ºæ–°è¡¨: {table_name}")
                await self._create_single_table(model_class)
            else:
                # ç°æœ‰è¡¨ï¼Œæ£€æŸ¥å­—æ®µå˜åŒ–
                logger.info(f"ğŸ”§ æ£€æŸ¥è¡¨å­—æ®µå˜åŒ–: {table_name}")
                await self._sync_table_columns(model_class)
        
        # æ–°å¢ï¼šæ£€æŸ¥éœ€è¦åˆ é™¤çš„è¡¨ï¼ˆå¦‚æœå¯ç”¨äº†è¡¨åˆ é™¤åŠŸèƒ½ï¼‰
        if self.enable_table_deletion:
            await self._remove_obsolete_tables(models, existing_tables)
    
    async def _get_existing_tables(self) -> Set[str]:
        """è·å–æ•°æ®åº“ä¸­ç°æœ‰çš„è¡¨"""
        pool = db_manager.get_database()
        db_type = pool.config.db_type
        
        if db_type == 'mysql':
            query = "SHOW TABLES"
        elif db_type == 'postgresql':
            query = "SELECT tablename FROM pg_tables WHERE schemaname='public'"
        elif db_type == 'sqlite':
            query = "SELECT name FROM sqlite_master WHERE type='table'"
        
        tables = await pool.fetch_all(query)
        return {list(table.values())[0] for table in tables}
    
    async def _create_single_table(self, model_class: Type[Model]):
        """åˆ›å»ºå•ä¸ªè¡¨"""
        from .models import create_tables
        await create_tables(model_class)
    
    async def _sync_table_columns(self, model_class: Type[Model]):
        """åŒæ­¥è¡¨çš„åˆ—ç»“æ„"""
        pool = db_manager.get_database()
        db_type = pool.config.db_type
        table_name = model_class._meta.table_name
        
        logger.info(f"ğŸ”§ å¼€å§‹åŒæ­¥è¡¨ {table_name} çš„åˆ—ç»“æ„")
        
        # è·å–ç°æœ‰åˆ—ä¿¡æ¯
        existing_columns = await self._get_table_columns(table_name)
        logger.debug(f"ğŸ“‹ {table_name} ç°æœ‰åˆ—: {list(existing_columns.keys())}")
        
        # æ£€æŸ¥éœ€è¦æ·»åŠ çš„æ–°åˆ—
        model_fields = model_class._meta.fields
        logger.debug(f"ğŸ“‹ {table_name} æ¨¡å‹å­—æ®µ: {list(model_fields.keys())}")
        
        added_count = 0
        modified_count = 0
        
        for field_name, field in model_fields.items():
            if field_name not in existing_columns:
                # æ·»åŠ æ–°åˆ—
                logger.info(f"â• å‘ç°æ–°å­—æ®µéœ€è¦æ·»åŠ : {table_name}.{field_name}")
                await self._add_column(table_name, field_name, field, db_type)
                added_count += 1
            else:
                # æ£€æŸ¥åˆ—æ˜¯å¦éœ€è¦ä¿®æ”¹
                logger.debug(f"ğŸ”§ æ£€æŸ¥å­—æ®µæ˜¯å¦éœ€è¦ä¿®æ”¹: {table_name}.{field_name}")
                await self._modify_column_if_needed(table_name, field_name, field, existing_columns[field_name], db_type)
        
        # æ–°å¢ï¼šæ£€æŸ¥éœ€è¦åˆ é™¤çš„åˆ—ï¼ˆå¦‚æœå¯ç”¨äº†åˆ—åˆ é™¤åŠŸèƒ½ï¼‰
        if self.enable_column_deletion:
            await self._remove_obsolete_columns(table_name, model_fields, existing_columns, db_type)
        
        logger.info(f"âœ… {table_name} åˆ—ç»“æ„åŒæ­¥å®Œæˆ (æ·»åŠ : {added_count}, ä¿®æ”¹: {modified_count})")
    
    async def _get_table_columns(self, table_name: str) -> Dict[str, Dict]:
        """è·å–è¡¨çš„åˆ—ä¿¡æ¯"""
        pool = db_manager.get_database()
        db_type = pool.config.db_type
        
        logger.debug(f"ğŸ” è·å–è¡¨ {table_name} çš„åˆ—ä¿¡æ¯")
        
        if db_type == 'mysql':
            query = f"DESCRIBE {table_name}"
        elif db_type == 'postgresql':
            query = f"""
            SELECT column_name, data_type, is_nullable, column_default
            FROM information_schema.columns 
            WHERE table_name = '{table_name}'
            """
        elif db_type == 'sqlite':
            query = f"PRAGMA table_info({table_name})"
        
        columns = await pool.fetch_all(query)
        logger.debug(f"ğŸ“Š æŸ¥è¯¢åˆ° {len(columns)} åˆ—ä¿¡æ¯")
        
        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        column_info = {}
        for col in columns:
            if db_type == 'mysql':
                column_info[col['Field']] = {
                    'Type': col['Type'],
                    'Null': col['Null'],
                    'Key': col['Key'],
                    'Default': col['Default']
                }
            elif db_type == 'postgresql':
                column_info[col['column_name']] = {
                    'Type': col['data_type'],
                    'Null': 'YES' if col['is_nullable'] == 'YES' else 'NO',
                    'Default': col['column_default']
                }
            elif db_type == 'sqlite':
                column_info[col['name']] = {
                    'Type': col['type'],
                    'Null': 'YES' if col['notnull'] == 0 else 'NO',
                    'Default': col['dflt_value']
                }
        
        logger.debug(f"ğŸ“‹ è§£æåçš„åˆ—ä¿¡æ¯: {list(column_info.keys())}")
        return column_info
    
    async def _add_column(self, table_name: str, field_name: str, field, db_type: str):
        """æ·»åŠ æ–°åˆ—"""
        pool = db_manager.get_database()
        
        logger.info(f"â• å¼€å§‹æ·»åŠ åˆ—: {table_name}.{field_name}")
        logger.debug(f"   å­—æ®µç±»å‹: {field.__class__.__name__}")
        logger.debug(f"   å­—æ®µå±æ€§: null={getattr(field, 'null', True)}, max_length={getattr(field, 'max_length', None)}")
        
        try:
            column_def = self.field_comparator.get_column_definition(field, db_type)
            logger.debug(f"   ç”Ÿæˆçš„åˆ—å®šä¹‰: {column_def}")
            
            sql = f"ALTER TABLE {table_name} ADD COLUMN {field_name} {column_def}"
            logger.debug(f"   æ‰§è¡ŒSQL: {sql}")
            
            await pool.execute(sql)
            logger.info(f"âœ… æˆåŠŸæ·»åŠ åˆ—: {table_name}.{field_name}")
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ åˆ—å¤±è´¥: {table_name}.{field_name}, é”™è¯¯: {e}")
            logger.error(f"   SQL: ALTER TABLE {table_name} ADD COLUMN {field_name} {column_def if 'column_def' in locals() else 'N/A'}")
            raise
    
    async def _modify_column_if_needed(self, table_name: str, field_name: str, field, existing_column: Dict, db_type: str):
        """å¦‚æœéœ€è¦åˆ™ä¿®æ”¹åˆ—"""
        changes = self.field_comparator.compare_columns(field_name, field, existing_column, db_type)
        
        if changes:
            pool = db_manager.get_database()
            
            for change in changes:
                if db_type == 'mysql':
                    sql = f"ALTER TABLE {table_name} {change}"
                elif db_type == 'postgresql':
                    # PostgreSQLéœ€è¦ç‰¹æ®Šå¤„ç†
                    sql = self._generate_postgresql_alter(table_name, field_name, field)
                elif db_type == 'sqlite':
                    # SQLiteä¸æ”¯æŒä¿®æ”¹åˆ—ï¼Œéœ€è¦é‡å»ºè¡¨
                    logger.warning(f"âš ï¸  SQLiteä¸æ”¯æŒä¿®æ”¹åˆ—: {table_name}.{field_name}")
                    continue
                
                try:
                    await pool.execute(sql)
                    logger.info(f"âœ… ä¿®æ”¹åˆ—: {table_name}.{field_name}")
                except Exception as e:
                    logger.error(f"âŒ ä¿®æ”¹åˆ—å¤±è´¥: {table_name}.{field_name}, é”™è¯¯: {e}")
    
    def _generate_postgresql_alter(self, table_name: str, field_name: str, field) -> str:
        """ç”ŸæˆPostgreSQLçš„ALTERè¯­å¥"""
        # PostgreSQLçš„ALTERè¯­æ³•æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        column_def = self.field_comparator.get_column_definition(field, 'postgresql')
        type_part = column_def.split()[0]
        return f"ALTER TABLE {table_name} ALTER COLUMN {field_name} TYPE {type_part}"
    
    async def force_sync(self):
        """å¼ºåˆ¶åŒæ­¥æ‰€æœ‰æ¨¡å‹åˆ°æ•°æ®åº“"""
        logger.info("ğŸ”§ å¼ºåˆ¶åŒæ­¥æ‰€æœ‰æ¨¡å‹...")
        
        current_models = self.scanner.discover_models()
        if not current_models:
            logger.warning("âš ï¸  æœªå‘ç°ä»»ä½•æ¨¡å‹")
            return
        
        await self._create_all_tables(current_models)
        
        # æ›´æ–°æŒ‡çº¹
        current_fingerprint = self.fingerprinter.generate_models_fingerprint(current_models)
        await self._record_fingerprint(current_fingerprint, len(current_models))
        
        logger.info("âœ… å¼ºåˆ¶åŒæ­¥å®Œæˆ")
    
    async def _remove_obsolete_columns(self, table_name: str, model_fields: Dict, existing_columns: Dict, db_type: str):
        """åˆ é™¤æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„åˆ—"""
        pool = db_manager.get_database()
        
        # æ‰¾å‡ºæ•°æ®åº“ä¸­å­˜åœ¨ä½†æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„åˆ—
        obsolete_columns = []
        for column_name in existing_columns.keys():
            if column_name not in model_fields and column_name != 'id':  # ä¿ç•™ä¸»é”®åˆ—
                obsolete_columns.append(column_name)
        
        for column_name in obsolete_columns:
            if db_type == 'mysql':
                sql = f"ALTER TABLE {table_name} DROP COLUMN {column_name}"
            elif db_type == 'postgresql':
                sql = f"ALTER TABLE {table_name} DROP COLUMN {column_name}"
            elif db_type == 'sqlite':
                # SQLiteä¸æ”¯æŒç›´æ¥åˆ é™¤åˆ—ï¼Œéœ€è¦é‡å»ºè¡¨
                logger.warning(f"âš ï¸  SQLiteä¸æ”¯æŒåˆ é™¤åˆ—: {table_name}.{column_name}")
                logger.warning("ğŸ’¡ å»ºè®®æ‰‹åŠ¨å¤„ç†SQLiteåˆ—åˆ é™¤")
                continue
            
            try:
                logger.info(f"ğŸ—‘ï¸  åˆ é™¤åˆ—: {table_name}.{column_name}")
                await pool.execute(sql)
                logger.info(f"âœ… æˆåŠŸåˆ é™¤åˆ—: {table_name}.{column_name}")
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤åˆ—å¤±è´¥: {table_name}.{column_name}, é”™è¯¯: {e}")

    async def _remove_obsolete_tables(self, models: Dict[str, Type[Model]], existing_tables: Set[str]):
        """åˆ é™¤æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„è¡¨"""
        pool = db_manager.get_database()
        
        # è·å–æ‰€æœ‰æ¨¡å‹å¯¹åº”çš„è¡¨å
        model_tables = {model_class._meta.table_name for model_class in models.values()}
        
        # ç³»ç»Ÿè¡¨ï¼Œä¸åº”åˆ é™¤
        system_tables = {self.schema_table, 'information_schema', 'performance_schema', 'mysql', 'sys'}
        
        # æ‰¾å‡ºæ•°æ®åº“ä¸­å­˜åœ¨ä½†æ¨¡å‹ä¸­ä¸å­˜åœ¨çš„è¡¨
        obsolete_tables = []
        for table_name in existing_tables:
            if (table_name not in model_tables and 
                table_name not in system_tables and 
                not table_name.startswith('__')):
                obsolete_tables.append(table_name)
        
        for table_name in obsolete_tables:
            try:
                logger.info(f"ğŸ—‘ï¸  åˆ é™¤è¡¨: {table_name}")
                await pool.execute(f"DROP TABLE {table_name}")
                logger.info(f"âœ… æˆåŠŸåˆ é™¤è¡¨: {table_name}")
            except Exception as e:
                logger.error(f"âŒ åˆ é™¤è¡¨å¤±è´¥: {table_name}, é”™è¯¯: {e}")

    async def get_status(self) -> Dict[str, Any]:
        """è·å–è‡ªåŠ¨è¿ç§»çŠ¶æ€"""
        current_models = self.scanner.discover_models()
        current_fingerprint = self.fingerprinter.generate_models_fingerprint(current_models)
        last_fingerprint = await self._get_last_fingerprint()
        
        # è·å–ç°æœ‰è¡¨ä¿¡æ¯
        existing_tables = await self._get_existing_tables()
        model_tables = {model_class._meta.table_name for model_class in current_models.values()}
        
        return {
            'current_models_count': len(current_models),
            'current_fingerprint': current_fingerprint,
            'last_fingerprint': last_fingerprint,
            'needs_sync': last_fingerprint != current_fingerprint,
            'is_first_run': last_fingerprint is None,
            'discovered_models': list(current_models.keys()),
            'existing_tables': list(existing_tables),
            'model_tables': list(model_tables),
            'enable_column_deletion': self.enable_column_deletion,
            'enable_table_deletion': self.enable_table_deletion
        }


# å…¨å±€è‡ªåŠ¨è¿ç§»ç®¡ç†å™¨
auto_migration_manager = AutoMigrationManager()


async def auto_migrate_on_startup():
    """åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨æ£€æŸ¥å’Œè¿ç§»"""
    try:
        await auto_migration_manager.initialize()
        await auto_migration_manager.check_and_sync()
    except Exception as e:
        logger.error(f"âŒ è‡ªåŠ¨è¿ç§»å¤±è´¥: {e}")
        raise


def enable_auto_migration(app, enable_column_deletion: bool = False, enable_table_deletion: bool = False):
    """ä¸ºåº”ç”¨å¯ç”¨è‡ªåŠ¨è¿ç§»
    
    Args:
        app: AsyncFrameåº”ç”¨å®ä¾‹
        enable_column_deletion: æ˜¯å¦å¯ç”¨åˆ—åˆ é™¤åŠŸèƒ½ï¼ˆé»˜è®¤Falseï¼Œé¿å…æ•°æ®ä¸¢å¤±ï¼‰
        enable_table_deletion: æ˜¯å¦å¯ç”¨è¡¨åˆ é™¤åŠŸèƒ½ï¼ˆé»˜è®¤Falseï¼Œé¿å…æ•°æ®ä¸¢å¤±ï¼‰
    """
    global auto_migration_manager
    
    # é‡æ–°åˆ›å»ºç®¡ç†å™¨å®ä¾‹ä»¥åº”ç”¨æ–°é…ç½®
    auto_migration_manager = AutoMigrationManager(
        enable_column_deletion=enable_column_deletion,
        enable_table_deletion=enable_table_deletion
    )
    
    # åˆ›å»ºå¸¦é…ç½®çš„å¯åŠ¨å‡½æ•°
    async def startup_auto_migrate_with_config():
        """å¸¦é…ç½®çš„è‡ªåŠ¨è¿ç§»å¯åŠ¨å‡½æ•°"""
        try:
            # ä½¿ç”¨å½“å‰é…ç½®çš„ç®¡ç†å™¨å®ä¾‹
            logger.info(f"ğŸ”§ å¼€å§‹è‡ªåŠ¨è¿ç§» (åˆ—åˆ é™¤: {enable_column_deletion}, è¡¨åˆ é™¤: {enable_table_deletion})")
            await auto_migration_manager.initialize()
            await auto_migration_manager.check_and_sync()
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨è¿ç§»å¤±è´¥: {e}")
            raise
    
    # æ³¨å†Œå¯åŠ¨å›è°ƒ
    app.on_startup(startup_auto_migrate_with_config)
    
    deletion_status = []
    if enable_column_deletion:
        deletion_status.append("åˆ—åˆ é™¤")
    if enable_table_deletion:
        deletion_status.append("è¡¨åˆ é™¤")
    
    if deletion_status:
        logger.info(f"ğŸ”§ å·²å¯ç”¨è‡ªåŠ¨è¿ç§»åŠŸèƒ½ (åŒ…å«: {', '.join(deletion_status)})")
        logger.warning("âš ï¸  å¯ç”¨åˆ é™¤åŠŸèƒ½å¯èƒ½å¯¼è‡´æ•°æ®ä¸¢å¤±ï¼Œè¯·è°¨æ…ä½¿ç”¨")
    else:
        logger.info("ğŸ”§ å·²å¯ç”¨è‡ªåŠ¨è¿ç§»åŠŸèƒ½ (å®‰å…¨æ¨¡å¼: ä¸åˆ é™¤è¡¨/åˆ—)") 