"""Functionality for concordance analysis."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/72_concordance.ipynb.

# %% ../nbs/72_concordance.ipynb 3
from __future__ import annotations
import time
import numpy as np
import polars as pl
import math
from fastcore.basics import patch
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import ipywidgets as widgets

# %% auto 0
__all__ = ['Concordance']

# %% ../nbs/72_concordance.ipynb 4
from .corpus import Corpus
from .result import Result
from .core import logger, PAGE_SIZE, EOF_TOKEN_STR, ERR_TOKEN_STR

# %% ../nbs/72_concordance.ipynb 10
class Concordance:
	""" Class for concordancing. """
	def __init__(self,
			  corpus:Corpus # Corpus instance
			  ): 
		self.corpus = corpus


# %% ../nbs/72_concordance.ipynb 11
@patch
def _get_concordance_sort(self:Concordance, 
						 token_positions: list[np.ndarray], # token index to get sort columns for
						 sort_columns: list # sort columns to use
						 ) -> tuple[np.ndarray, np.ndarray]: # token ids for first sort column and corresponding sort order
	""" Get the first sort column for a concordance. """

	start_time = time.time()
	index = 'orth_index'
	seq = np.array(token_positions[0]+sort_columns[0])
	sort_column_ids = self.corpus.get_tokens_by_index('orth_index')[seq]
	sort_column_order = self.corpus.token_ids_to_sort_order(sort_column_ids)
	logger.info(f'Concordance sort column ({sort_column_ids.shape[0]}) retrieval time: {(time.time() - start_time):.5f} seconds')
	return sort_column_ids, sort_column_order


# %% ../nbs/72_concordance.ipynb 16
@patch
def concordance(self: Concordance, 
				token_str: str, # token string to get concordance for 
				context_length:int = 5, # number of words to show on left and right of token string
				order:str='1R2R3R', # order of sort columns - one of 1L2L3L, 3L2L1L, 2L1L1R, 1L1R2R, 1R2R3R, LEFT, RIGHT
				page_size:int=PAGE_SIZE, # number of results to display per results page
				page_current:int=1, # current page of results
				show_all_columns:bool = False, # df with all columns or just essentials
				use_cache:bool = True # retrieve the results from cache if available
				) -> Result: # concordance report results
	""" Report concordance for a token string. """

	# DONE - reducing data retrieved to just the sort columns and then doing the concordance display separately here
	# DONE - speed up the sort so that does a partial sort (e.g. just one or two columns) to get position of the slice - then handle ordering with smaller slice of data
	# IDEA: potentially get sort columns until small enough result
	
	if order not in ['1L2L3L', '3L2L1L', '2L1L1R', '1L1R2R', '1R2R3R', 'LEFT', 'RIGHT']:
		raise ValueError(f'Invalid order: order must be one of: 1L2L3L, 3L2L1L, 2L1L1R, 1L1R2R, 1R2R3R, LEFT, RIGHT')
	
	if order == 'LEFT':
		order = '1L2L3L'
	elif order == 'RIGHT':
		order = '1R2R3R'

	token_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)

	start_time = time.time()
	sequence_len = len(token_sequence[0])
	concordance_range = range(-1 * context_length, context_length + sequence_len)
	positional_columns = [str(x) for x in concordance_range]

	index = 'orth_index'

	cache_id = tuple(['concordance'] + list(token_sequence) + [order])

	if use_cache == True and cache_id in self.corpus.results_cache:
		logger.info('Using cached concordance results')
		positional_columns = self.corpus.results_cache[cache_id][0]
		concordance_df = self.corpus.results_cache[cache_id][1]
		total_count = self.corpus.results_cache[cache_id][2]
		total_docs = self.corpus.results_cache[cache_id][3]
		sort_columns = self.corpus.results_cache[cache_id][4]
	else:
		logger.info('Processing concordance results')
		token_positions = self.corpus.get_token_positions(token_sequence, index_id)

		if len(token_positions[0]) == 0:
			logger.info('No tokens found')
			return Result(type = 'concordance', df=pl.DataFrame(), title=f'Concordance for "{token_str}"', description=f'No matches', summary_data={}, formatted_data=[])

		if order == '1L2L3L':
			sort_columns = [-1,-2,-3]
		elif order == '3L2L1L':
			sort_columns = [-3,-2,-1]
		elif order == '2L1L1R':
			sort_columns = [-2,-1,sequence_len + 1 - 1]
		elif order == '1L1R2R':
			sort_columns = [-1,sequence_len + 1 - 1,sequence_len + 2 - 1]
		else:
			# i.e. 1R2R3R
			sort_columns = [sequence_len + 1 - 1,sequence_len + 2 - 1,sequence_len + 3 - 1]

		# getting first sort column here
		sort_column_ids, sort_column_order = self._get_concordance_sort(token_positions, sort_columns)
		
		concordance_df = pl.DataFrame([pl.Series(name='index', values=token_positions[0]), pl.Series(name='sort0', values=sort_column_order), pl.Series(name=str(sort_columns[0]), values=sort_column_ids)])
		concordance_df = concordance_df.sort('sort0')
		concordance_df = concordance_df.with_row_index('row')

		total_count = len(concordance_df)
		total_docs = len(np.unique(self.corpus.get_tokens_by_index('token2doc_index')[np.array(token_positions[0])])) # REFACTORED - was using old self.corpus.token2doc_index

		self.corpus.results_cache[cache_id] = [positional_columns, concordance_df, total_count, total_docs, sort_columns]

	# working out relevant slice to populate 
	resultset_start = page_size*(page_current-1)
	resultset_len = page_size
	resultset_end = min(resultset_start + resultset_len, len(concordance_df) - 1)
	
	start_order = concordance_df['sort0'][resultset_start]
	end_order = concordance_df['sort0'][resultset_end]
	start_order_pos = concordance_df.filter(pl.col("sort0") == start_order).head(1)['row'].item()
	end_order_pos = concordance_df.filter(pl.col("sort0") == end_order).tail(1)['row'].item()
	
	# populating a smaller chunk of the concordance report - as only need to retrieve/sort a subset
	concordance_result_df = concordance_df.slice(start_order_pos, end_order_pos - start_order_pos + 1)

	results_start_time = time.time()
	concordance_columns = []
	seq = concordance_result_df['index'].to_numpy()
	for pos in concordance_range:
		tokens = self.corpus.get_tokens_by_index(index)[np.array(seq+pos)] # REFACTORED - was using getattr call to get orth_index here
		concordance_columns.append(pl.Series(name=str(pos), values=tokens))
		if pos in sort_columns:
			column_name = 'sort'+str(sort_columns.index(pos))
			if column_name != 'sort0':
				concordance_columns.append(pl.Series(name=column_name, values=self.corpus.token_ids_to_sort_order(tokens)))
	logger.info(f'Concordance results ({len(concordance_columns[0])}) retrieval time: {(time.time() - results_start_time):.5f} seconds')

	concordance_result_df = concordance_result_df.with_columns(concordance_columns)
	#offsets_arr = np.array(self.corpus.offsets,dtype=np.uint64) # FIX
	#document_ids = np.searchsorted(offsets_arr, concordance_result_df['index'], side = 'right') - 1 
	document_ids = self.corpus.get_tokens_by_index('token2doc_index')[np.array(concordance_result_df['index'])] # REFACTORED to remove offsets functionality
	concordance_result_df = concordance_result_df.with_columns(pl.Series(name="document_id", values=document_ids))
	concordance_result_df = concordance_result_df.sort(['sort0','sort1','sort2'])
		
	# slicing this further to get only the required page of results and then populating with left, keyword, right strings
	concordance_view_df = concordance_result_df.slice(start_order_pos - resultset_start, page_size)

	concordance_left = []
	concordance_right = []
	concordance_keyword = []

	for pos in positional_columns:
		if int(pos) < 0:
			concordance_left.append(self.corpus.token_ids_to_tokens(concordance_view_df[str(pos)].to_numpy()))
		elif int(pos) == 0 or int(pos) < sequence_len:
			concordance_keyword.append(self.corpus.token_ids_to_tokens(concordance_view_df[str(pos)].to_numpy()))
		else:
			concordance_right.append(self.corpus.token_ids_to_tokens(concordance_view_df[str(pos)].to_numpy()))

	concordance_left = [(' '.join(column)).split(EOF_TOKEN_STR)[-1] for column in np.array(concordance_left).T]
	concordance_keyword = [' '.join(column) for column in np.array(concordance_keyword).T]
	concordance_right = [(' '.join(column)).split(EOF_TOKEN_STR)[0] for column in np.array(concordance_right).T]

	concordance_view_df = concordance_view_df.with_columns(pl.Series(name='left', values=concordance_left), pl.Series(name='node', values=concordance_keyword), pl.Series(name='right', values=concordance_right))

	total_pages = math.ceil(total_count/page_size)
	summary_data = {'total_count': total_count, 'total_docs': total_docs, 'page': page_current, 'total_pages': total_pages}
	formatted_data = [f'Total Concordance Lines: {total_count}', f'Total Documents: {total_docs}', f'Showing {min(page_size, total_count)} lines', f'Page {page_current} of {total_pages}']

	if show_all_columns == False:
		concordance_view_df = concordance_view_df[['document_id', 'left', 'node', 'right']]
	
	logger.info(f'Concordance report time: {(time.time() - start_time):.5f} seconds')

	return Result(type = 'concordance', df=concordance_view_df, title=f'Concordance for "{token_str}"', description=f'{self.corpus.name}, Context tokens: {context_length}, Order: {order}', summary_data=summary_data, formatted_data=formatted_data)


# %% ../nbs/72_concordance.ipynb 25
@patch
def concordance_plot(self: Concordance,
					 token_str: str,
					 page_size: int = 10):
	"""Display concordance plot."""

	import numpy as np
	import plotly.graph_objects as go
	from plotly.subplots import make_subplots
	import ipywidgets as widgets

	# Tokenize and get positions
	token_sequence, index_id = self.corpus.tokenize(token_str, simple_indexing=True)
	token_positions = self.corpus.get_token_positions(token_sequence, index_id)
	sequence_len = len(token_sequence[0])

	if len(token_positions[0]) == 0:
		print("No matches found.")
		return

	document_ids = self.corpus.get_tokens_by_index('token2doc_index')[token_positions[0]]
	unique_document_ids = np.unique(document_ids)
	num_docs = len(unique_document_ids)

	font_family = "'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;"
	plots_per_page = page_size
	num_pages = int(np.ceil(num_docs / plots_per_page))
	per_subplot_height = 50

	# Create FigureWidget with empty subplots
	fig = make_subplots(
		rows=plots_per_page,
		cols=1,
		vertical_spacing=0.05,
		subplot_titles=[None] * plots_per_page,
	)
	fig_widget = go.FigureWidget(fig)

	# Add background shapes and configure axes for each subplot
	for idx in range(plots_per_page):
		row = idx + 1
		fig_widget.add_shape(
			type="rect",
			x0=0, y0=0, x1=100, y1=1,
			line=dict(color="black", width=1),
			fillcolor="gray",
			opacity=0.08,
			layer="below",
			row=row, col=1
		)
		fig_widget.update_xaxes(range=[0, 100], visible=False, fixedrange=True, row=row, col=1)
		fig_widget.update_yaxes(range=[0, 1], visible=False, fixedrange=True, row=row, col=1)

	fig_widget.update_layout(
		height=per_subplot_height * plots_per_page,
		width=600,
		showlegend=False,
		title_text=f'Concordance Plot for "{token_str}"',
		title_x=0.5,
		margin=dict(t=40, b=20, l=80, r=20),
		font=dict(family=font_family, size=12),
	)

	# Slider for paging
	page_slider = widgets.IntSlider(value=1, min=1, max=num_pages, step=1, description='Page', layout=widgets.Layout(width='600px', margin='10px 0 10px 0'))

	footer = widgets.HTML(
		value=f"<div style='text-align: left; font-size: 12px; color: black; margin-left: 80px;margin-bottom:10px;line-height:1.7;'>{self.corpus.name}<br>Total Documents: {num_docs}<br>Total Concordance Lines: {len(token_positions[0])}</div>"
	)

	def update(change):
		page = change['new']
		fig_widget.data = ()  # Remove all previous traces
		fig_widget.layout.annotations = ()

		start = (page - 1) * plots_per_page
		end = min(start + plots_per_page, num_docs)
		page_document_ids = unique_document_ids[start:end]

		# Get min/max positions for normalization
		doc_range = self.corpus.tokens.with_row_index('position').filter(
			pl.col('token2doc_index').is_in(page_document_ids)
		).group_by('token2doc_index').agg([
			pl.col('position').min().alias('min'),
			pl.col('position').max().alias('max')
		]).collect()

		# Prepare normalized positions for each doc on this page
		documents = [[] for _ in range(len(page_document_ids))]
		normalized_documents = [[] for _ in range(len(page_document_ids))]
		examples = [[] for _ in range(len(page_document_ids))]
		for i, doc_id in enumerate(document_ids):
			if doc_id not in page_document_ids:
				continue
			doc_index = np.where(page_document_ids == doc_id)[0][0]
			documents[doc_index].append(token_positions[0][i])
			position_min, position_max = doc_range.filter(
				pl.col('token2doc_index') == doc_id
			).select(['min', 'max']).to_numpy()[0]
			norm_pos = (token_positions[0][i] - position_min) / (position_max - position_min) * 100 if position_max > position_min else 0
			normalized_documents[doc_index].append(norm_pos)
			tokens_for_example = self.corpus.get_tokens_by_index('orth_index')[token_positions[0][i]-5:token_positions[0][i]+6]
			
			if self.corpus.EOF_TOKEN in tokens_for_example:
				positions = np.where(tokens_for_example == self.corpus.EOF_TOKEN)[0]
				if len(positions) > 0 and positions[0] < 5:
					tokens_for_example = tokens_for_example[positions[0] + 1:]
				else:
					positions = np.where(tokens_for_example == self.corpus.EOF_TOKEN)[0]
					if len(positions) > 0 and positions[-1] > 5:
						tokens_for_example = tokens_for_example[:positions[-1]]
			tokens_for_example = self.corpus.token_ids_to_tokens(tokens_for_example)
			examples[doc_index].append(' '.join(tokens_for_example))

			# yref = "y" if i == 0 else f"y{i + 1} domain"

		n_plots_this_page = end - start
		if n_plots_this_page < plots_per_page or ('old' in change and change['old'] == num_pages):
			fig_widget.layout.shapes = ()
			for idx in range(n_plots_this_page):
				row = idx + 1
				fig_widget.add_shape(
					type="rect",
					x0=0, y0=0, x1=100, y1=1,
					line=dict(color="black", width=1),
					fillcolor="gray",
					opacity=0.08,
					layer="below",
					row=row, col=1
				)

		# Add traces for each subplot
		for idx, positions in enumerate(normalized_documents):
			row = idx + 1
			for pos, x0 in enumerate(positions):
				fig_widget.add_trace(
					go.Scatter(
						x=[x0, x0],
						y=[0, 1],
						mode='lines',
						line=dict(color='black', width=2),
						showlegend=False,
						hoverinfo='text',
						hovertext=f"{examples[idx][pos]}" if examples[idx] else ""
					),
					row=row, col=1
				)

			yref = "y" if row == 1 else f"y{row} domain"

			# doc id from document_ids
			doc_id = page_document_ids[idx]
			fig_widget.add_annotation(
				text=f"Doc {doc_id}",
				xref="paper", yref=yref,
				x=-0.03, y=0.65,
				showarrow=False,
				xanchor="right",
				yanchor="middle",
				font=dict(size=12, family = font_family)
			)

			lines_count = len(normalized_documents[idx])
			if lines_count == 0:
				lines_string = "No lines"
			elif lines_count == 1:
				lines_string = "1 line"
			else:
				# pluralize 'line' based on count
				# e.g. "5 lines"
				lines_string = f"{lines_count} lines"

			fig_widget.add_annotation(
				text=f"{lines_string}",
				xref="paper", yref=yref,
				x=-0.03, y=0.22, 
				showarrow=False,
				xanchor="right",
				yanchor="middle",
				font=dict(size=11, color="gray", family = font_family)
			)

	# Initial plot
	update({'new': 1})
	page_slider.observe(update, names='value')

	display(page_slider)
	display(fig_widget)
	display(footer)





