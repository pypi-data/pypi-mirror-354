{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `next_tool` and `manual_call`\n",
    "\n",
    "## Overview \n",
    "\n",
    "If you have worked with function calls before, you know that often the model does not call the function you want to call. This poses a knonw problem of AI Agents, because often want a particular flow to be respected. \n",
    "\n",
    "In this notebook, we will see how to use the `next_tool` and `manual_call` to control the flow of the AI Agent, make it more reliable and predictable.\n",
    "\n",
    "#### next_tool\n",
    "\n",
    "The `next_tool` is a parameter that can be used with the `function_tool` or  `agent_tool` to specify the next tool to be called. Behind the scenes, the `next_tool` is just setting the completion parameter `tool_choice` to the name of the next tool to be called. \n",
    "\n",
    "To use it, you just need to pass the name of of the method you want to call next as the value of the `next_tool` parameter. Note that the method must be decored as a tool and must be in the same class as the current tool.\n",
    "\n",
    "\n",
    "#### manual_call\n",
    "\n",
    "The `manual_call` is a parameter that can be used together with the `next_tool` to specify that the next tool call will be inserted \"manually\" in the message history. By manually, we mean that the next tool call will not be done by a new model call, but inserted directly in the message history. This is usefull in the situations where the output of the current tool can be directly used as the input of the next tool, without the need of a new model call.\n",
    "\n",
    "To use it, you need to pass a callable as the value of the `manual_call` parameter, that will receive the output of the current tool and return a dictionay that should match the input of the next tool. In this way you can add some processing to the output of the current tool before passing it to the next tool.\n",
    "\n",
    "---\n",
    "\n",
    "## Let's see how to use these parameters in practice.\n",
    "\n",
    "\n",
    "In the example bellow we have a simple agent that has two tools, one to generate a random topic and another that genrate a joken, given a topic.\n",
    "\n",
    "To show that the `next_tool` is working, we will force the agent to call the `generate_joke` tool after a random topic is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agente.core.base import BaseAgent,BaseTaskAgent\n",
    "from agente.core.decorators import function_tool,agent_tool\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class JokeTeller(BaseTaskAgent):\n",
    "    agent_name: str = \"JokeTeller\"\n",
    "    system_prompt:str = \"Your task is to write a funny joke.\"\n",
    "    completion_kwargs: dict = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    @function_tool\n",
    "    def complete_task(self,joke:str):\n",
    "        \"\"\"To be used as a tool to complete the task.\n",
    "\n",
    "        Args:\n",
    "            joke: The joke to return.\n",
    "        \"\"\"\n",
    "        return joke\n",
    "\n",
    "\n",
    "\n",
    "class MainAgent(BaseAgent):\n",
    "    agent_name: str = \"main_agent\"\n",
    "    \n",
    "    @function_tool(next_tool = \"get_joke\") # To make sure the agent calls the get_joke tool we add the next_tool argument to force it.\n",
    "    def random_topic(self):\n",
    "        \"\"\"Tool to get a random topic.\n",
    "        \"\"\"\n",
    "        topics = [\"programming\",\"science\",\"animals\",\"food\",\"sports\"]\n",
    "        topic = random.choice(topics)\n",
    "\n",
    "        return topic\n",
    "\n",
    "\n",
    "    @agent_tool()\n",
    "    def get_joke(self,joke_topic:str):\n",
    "        \"\"\"Tool to get a joke.\n",
    "\n",
    "        Args:\n",
    "            joke_topic: The topic of the joke.\n",
    "        \"\"\"\n",
    "\n",
    "        joke_agent = JokeTeller()\n",
    "        joke_agent.add_message(role = \"user\", content = \"Tell me a joke about \" + joke_topic)\n",
    "        return joke_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing agent: main_agent\n",
      "Executing tool: random_topic from agent main_agent\n",
      "Executing agent: main_agent\n",
      "Executing agent: JokeTeller\n",
      "Executing tool: complete_task from agent JokeTeller\n",
      "Executing agent: main_agent\n"
     ]
    }
   ],
   "source": [
    "example_agent = MainAgent()\n",
    "example_agent.add_message(role = \"user\", content = \"Generate a random topic.\")\n",
    "responses = [r async for r in example_agent.run()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random topic is \"science.\" Here's a science joke for you:\n",
      "\n",
      "Why can't you trust an atom? Because they make up everything!\n"
     ]
    }
   ],
   "source": [
    "print(example_agent.conv_history.messages[-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='user', agent_name='main_agent', content='Generate a random topic.', tool_calls=None, tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None),\n",
       " Message(role='assistant', agent_name='main_agent', content=None, tool_calls=[{'index': 0, 'function': {'arguments': '{}', 'name': 'random_topic'}, 'id': 'call_8rpvzweXykUzvjX1Z4G7wOj0', 'type': 'function'}], tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None),\n",
       " Message(role='tool', agent_name='main_agent', content='\"science\"', tool_calls=None, tool_call_id='call_8rpvzweXykUzvjX1Z4G7wOj0', tool_name='random_topic', hidden=False, id=None, usage=None),\n",
       " Message(role='assistant', agent_name='main_agent', content=None, tool_calls=[{'index': 0, 'function': {'arguments': '{\"joke_topic\":\"science\"}', 'name': 'get_joke'}, 'id': 'call_3iH7Zle7DY0XgV107U81vsc4', 'type': 'function'}], tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None),\n",
       " Message(role='tool', agent_name='main_agent', content='\"Why can\\'t you trust an atom? Because they make up everything!\"', tool_calls=None, tool_call_id='call_3iH7Zle7DY0XgV107U81vsc4', tool_name='get_joke', hidden=False, id=None, usage=None),\n",
       " Message(role='assistant', agent_name='main_agent', content='The random topic is \"science.\" Here\\'s a science joke for you:\\n\\nWhy can\\'t you trust an atom? Because they make up everything!', tool_calls=None, tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_agent.conv_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inspection to the full message history will show that the `get_joke` tool was called after the `random_topic` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 0, 'function': {'arguments': '{}', 'name': 'random_topic'}, 'id': 'call_8rpvzweXykUzvjX1Z4G7wOj0', 'type': 'function'}]\n",
      "[{'index': 0, 'function': {'arguments': '{\"joke_topic\":\"science\"}', 'name': 'get_joke'}, 'id': 'call_3iH7Zle7DY0XgV107U81vsc4', 'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "for m in example_agent.conv_history.messages:\n",
    "    if m.tool_calls:\n",
    "        print(m.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in total we add 3 model calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_agent.log_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how to use the `manual_call` parameter. \n",
    "\n",
    "We just need to add a function that will pass the generated topic to the `get_joke` tool. We will use a simple lambda function to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainAgent(BaseAgent):\n",
    "    agent_name: str = \"main_agent\"\n",
    "    \n",
    "    @function_tool(next_tool = \"get_joke\", manual_call= lambda x: {'joke_topic':x})\n",
    "    def random_topic(self):\n",
    "        \"\"\"Tool to get a random topic.\n",
    "        \"\"\"\n",
    "        topics = [\"programming\",\"science\",\"animals\",\"food\",\"sports\"]\n",
    "        topic = random.choice(topics)\n",
    "\n",
    "        return topic\n",
    "\n",
    "\n",
    "    @agent_tool()\n",
    "    def get_joke(self,joke_topic:str):\n",
    "        \"\"\"Tool to get a joke.\n",
    "\n",
    "        Args:\n",
    "            joke_topic: The topic of the joke.\n",
    "        \"\"\"\n",
    "\n",
    "        joke_agent = JokeTeller()\n",
    "        joke_agent.add_message(role = \"user\", content = \"Tell me a joke about \" + joke_topic)\n",
    "        return joke_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing agent: main_agent\n",
      "Executing tool: random_topic from agent main_agent\n",
      "Executing agent: JokeTeller\n",
      "Executing tool: complete_task from agent JokeTeller\n",
      "Executing agent: main_agent\n"
     ]
    }
   ],
   "source": [
    "example_agent = MainAgent()\n",
    "example_agent.add_message(role = \"user\", content = \"Generate a random topic.\")\n",
    "responses = [r async for r in example_agent.run()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='user', agent_name='main_agent', content='Generate a random topic.', tool_calls=None, tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None),\n",
       " Message(role='assistant', agent_name='main_agent', content=None, tool_calls=[{'index': 0, 'function': {'arguments': '{}', 'name': 'random_topic'}, 'id': 'call_c7EyI8RGJjNzSHUCSSs5EyrW', 'type': 'function'}], tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None),\n",
       " Message(role='tool', agent_name='main_agent', content='\"animals\"', tool_calls=None, tool_call_id='call_c7EyI8RGJjNzSHUCSSs5EyrW', tool_name='random_topic', hidden=False, id=None, usage=None),\n",
       " Message(role='assistant', agent_name='main_agent', content=None, tool_calls=[{'id': 'man_EeLk12aazyCEPmK3VogtUxQX', 'function': {'name': 'get_joke', 'arguments': '{\"joke_topic\": \"animals\"}'}, 'type': 'function'}], tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None),\n",
       " Message(role='tool', agent_name='main_agent', content='\"Why don\\'t scientists trust atoms? Because they make up everything... just like cats make up their own rules!\"', tool_calls=None, tool_call_id='man_EeLk12aazyCEPmK3VogtUxQX', tool_name='get_joke', hidden=False, id=None, usage=None),\n",
       " Message(role='assistant', agent_name='main_agent', content='Here\\'s a joke on the random topic of \"animals\":\\n\\n\"Why don\\'t scientists trust atoms? Because they make up everything... just like cats make up their own rules!\"', tool_calls=None, tool_call_id=None, tool_name=None, hidden=False, id=None, usage=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_agent.conv_history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'index': 0, 'function': {'arguments': '{}', 'name': 'random_topic'}, 'id': 'call_c7EyI8RGJjNzSHUCSSs5EyrW', 'type': 'function'}]\n",
      "[{'id': 'man_EeLk12aazyCEPmK3VogtUxQX', 'function': {'name': 'get_joke', 'arguments': '{\"joke_topic\": \"animals\"}'}, 'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "for m in example_agent.conv_history.messages:\n",
    "    if m.tool_calls:\n",
    "        print(m.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual call has the prefix \"man_\" to indicate that the tool call was inserted manually in the message history and not by a new model call.\n",
    "\n",
    "We can see in the call log that we now have only 2 model calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_agent.log_calls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
