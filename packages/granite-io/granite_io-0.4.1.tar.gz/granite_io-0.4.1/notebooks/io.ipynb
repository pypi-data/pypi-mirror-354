{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of operations on the classes in the \"io\" package.\n",
    "\n",
    "This notebook should eventually turn into documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import aconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_JSON_STRS = [\n",
    "    \"\"\"\n",
    "{\n",
    "    \"messages\":\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "{\n",
    "    \"messages\":\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"How much wood could a wood chuck chuck?\"}\n",
    "    ],\n",
    "    \"thinking\": true\n",
    "}\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "input_json_str = INPUT_JSON_STRS[1]\n",
    "print(input_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_json = json.loads(input_json_str)\n",
    "input_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.base import ChatCompletionInputs\n",
    "\n",
    "input_obj = ChatCompletionInputs.model_validate(input_json)\n",
    "input_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.granite_3_2.input_processors.granite_3_2_input_processor import (\n",
    "    Granite3Point2Inputs,\n",
    ")  # noqa: E501\n",
    "\n",
    "granite_input_obj = Granite3Point2Inputs.model_validate(input_obj.model_dump())\n",
    "granite_input_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstituted_json = input_obj.model_dump_json(indent=4)\n",
    "print(reconstituted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "GRANITE_3_2_MODEL_STR = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(GRANITE_3_2_MODEL_STR)\n",
    "\n",
    "input_kwargs = input_json.copy()\n",
    "del input_kwargs[\"messages\"]\n",
    "transformers_str = tokenizer.apply_chat_template(\n",
    "    input_json[\"messages\"], **input_kwargs, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(transformers_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from granite_io.io.granite_3_2.granite_3_2 import Granite3Point2InputOutputProcessor\n",
    "\n",
    "inputs = ChatCompletionInputs.model_validate_json(input_json_str)\n",
    "io_proc_str = Granite3Point2InputOutputProcessor().inputs_to_string(inputs)\n",
    "print(io_proc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model onto the GPU and wrap it in an I/O processor.\n",
    "\n",
    "import torch\n",
    "\n",
    "from granite_io.io.granite_3_2.granite_3_2 import Granite3Point2InputOutputProcessor\n",
    "from granite_io.backend.transformers import TransformersBackend\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "# Re-enable once we have a smaller model that can run in a laptop GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_name = \"mps\"\n",
    "else:\n",
    "    device_name = \"cpu\"\n",
    "    # CPU mode; prevent thrashing\n",
    "    torch.set_num_threads(4)\n",
    "\n",
    "backend = TransformersBackend(\n",
    "    aconfig.Config(\n",
    "        {\"model_name\": GRANITE_3_2_MODEL_STR, \"device\": device_name},\n",
    "        override_env_vars=False,\n",
    "    ),\n",
    ")\n",
    "io_processor = Granite3Point2InputOutputProcessor(backend=backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = io_processor.create_chat_completion(inputs)\n",
    "print(result.results[0].next_message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
