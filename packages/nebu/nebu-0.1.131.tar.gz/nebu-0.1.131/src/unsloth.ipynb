{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from nebu.processors.decorate import processor\n",
    "from nebu import Message\n",
    "from nebu.processors.models import (\n",
    "    V1Scale,\n",
    "    V1ScaleDown,\n",
    "    V1ScaleUp,\n",
    "    V1ScaleZero,\n",
    ")\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Decorator Init] @processor decorating function 'train_unsloth_sft'\n",
      "[DEBUG Decorator] Determining execution environment...\n",
      "[DEBUG Helper] Checking if running in Jupyter...\n",
      "[DEBUG Helper] is_jupyter_notebook: IPython class name: <class 'ipykernel.zmqshell.ZMQInteractiveShell'>\n",
      "[DEBUG Helper] is_jupyter_notebook: Jupyter detected (ZMQInteractiveShell).\n",
      "[DEBUG Decorator] Jupyter environment detected.\n",
      "[DEBUG Helper] Attempting to get notebook execution history...\n",
      "[DEBUG Helper] get_notebook_executed_code: Retrieved 2 history entries.\n",
      "[DEBUG Helper] get_notebook_executed_code: Total history source length: 8306\n",
      "[DEBUG Decorator] Retrieved notebook history (length: 8306).\n",
      "[DEBUG Decorator] No manually included objects specified.\n",
      "[DEBUG Decorator] Validating signature and type hints for train_unsloth_sft...\n",
      "[DEBUG Decorator] Raw type hints: {'message': <class 'nebu.processors.models.Message[TrainingRequest]'>, 'return': <class '__main__.TrainingResponse'>}\n",
      "[DEBUG Decorator] Parameter 'message' type hint: <class 'nebu.processors.models.Message[TrainingRequest]'>\n",
      "[DEBUG Decorator] Return type hint: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] Determining input type structure for param type hint: <class 'nebu.processors.models.Message[TrainingRequest]'>\n",
      "[DEBUG Decorator] get_origin result: None, get_args result: ()\n",
      "[DEBUG Decorator] get_origin failed. Attempting regex fallback on type string: '<class 'nebu.processors.models.Message[TrainingRequest]'>'\n",
      "[DEBUG Decorator] Regex matched generic Message pattern!\n",
      "[DEBUG Decorator] Captured content type name via regex: 'TrainingRequest'\n",
      "[DEBUG Decorator] Successfully resolved content type name 'TrainingRequest' to type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Final Input Type Determination: is_stream_message=True, content_type=<class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Validating parameter and return types are BaseModel subclasses...\n",
      "[DEBUG Decorator] check_basemodel: Checking Parameter 'message' - Type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] check_basemodel: Actual type for Parameter 'message': <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] check_basemodel: OK - Parameter 'message' effective type (TrainingRequest) is a BaseModel subclass.\n",
      "[DEBUG Decorator] check_basemodel: Checking Return value - Type: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] check_basemodel: Actual type for Return value: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] check_basemodel: OK - Return value effective type (TrainingResponse) is a BaseModel subclass.\n",
      "[DEBUG Decorator] Type validation complete.\n",
      "[DEBUG Decorator] Getting source code for function 'train_unsloth_sft'...\n",
      "[DEBUG Decorator] Attempting notebook history extraction for function 'train_unsloth_sft'...\n",
      "[DEBUG Helper] Extracting 'train_unsloth_sft' (FunctionDef) from history string (len: 8306)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'train_unsloth_sft' in cell #1.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'train_unsloth_sft'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'train_unsloth_sft' from cell #1.\n",
      "[DEBUG Decorator] Found function 'train_unsloth_sft' source in notebook history.\n",
      "[DEBUG Decorator] Final function source obtained for 'train_unsloth_sft' (len: 6715). Source starts:\n",
      "-------def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\n",
      "    import time\n",
      "    from unsloth import FastVisionModel, is_bf16_supported\n",
      "    from unsloth.trainer import UnslothVisionDataCollator\n",
      "    from trl import SFTTrainer, SFTConf...\n",
      "-------\n",
      "[DEBUG Decorator] No init_func provided.\n",
      "[DEBUG Decorator] Getting model sources...\n",
      "[DEBUG Decorator] Getting base Message source...\n",
      "[DEBUG get_type_source] Getting source for type: <class 'nebu.processors.models.Message'>\n",
      "[DEBUG get_model_source] Getting source for: Message\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: Message\n",
      "[DEBUG Helper] Extracting 'Message' (ClassDef) from history string (len: 8306)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Definition 'Message' of type ClassDef not found in history search.\n",
      "[DEBUG get_model_source] Notebook history extraction failed for: Message. Proceeding to dill.\n",
      "[DEBUG get_model_source] Attempting dill fallback for: Message\n",
      "[DEBUG get_model_source] Using dill source for: Message\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class 'nebu.processors.models.Message'>\n",
      "[DEBUG Decorator] Input is StreamMessage. Content type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Getting source for content_type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG get_type_source] Getting source for type: <class '__main__.TrainingRequest'>\n",
      "[DEBUG get_model_source] Getting source for: TrainingRequest\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: TrainingRequest\n",
      "[DEBUG Helper] Extracting 'TrainingRequest' (ClassDef) from history string (len: 8306)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'TrainingRequest' in cell #1.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'TrainingRequest'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'TrainingRequest' from cell #1.\n",
      "[DEBUG get_model_source] Using notebook history source for: TrainingRequest\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class '__main__.TrainingRequest'>\n",
      "[DEBUG Decorator] Getting source for return_type: <class '__main__.TrainingResponse'>\n",
      "[DEBUG get_type_source] Getting source for type: <class '__main__.TrainingResponse'>\n",
      "[DEBUG get_model_source] Getting source for: TrainingResponse\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: TrainingResponse\n",
      "[DEBUG Helper] Extracting 'TrainingResponse' (ClassDef) from history string (len: 8306)...\n",
      "[DEBUG Helper] extract: Split history into 3 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'TrainingResponse' in cell #1.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'TrainingResponse'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'TrainingResponse' from cell #1.\n",
      "[DEBUG get_model_source] Using notebook history source for: TrainingResponse\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator] Source Result - Content Type: Found\n",
      "[DEBUG Decorator] Source Result - Input Model (non-stream): Not Found or N/A\n",
      "[DEBUG Decorator] Source Result - Output Model: Found\n",
      "[DEBUG Decorator] Source Result - Base StreamMessage: Found\n",
      "[DEBUG Decorator] Populating environment variables...\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'INPUT_MODEL'\n",
      "[DEBUG Decorator] add_source_to_env: No source for 'INPUT_MODEL', skipping.\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'OUTPUT_MODEL'\n",
      "[DEBUG Decorator] Added env var OUTPUT_MODEL_SOURCE (string)\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'CONTENT_TYPE'\n",
      "[DEBUG Decorator] Added env var CONTENT_TYPE_SOURCE (string)\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'STREAM_MESSAGE'\n",
      "[DEBUG Decorator] Added env var STREAM_MESSAGE_SOURCE (string)\n",
      "[DEBUG Decorator] Adding type info env vars...\n",
      "[DEBUG Decorator] Finished populating environment variables.\n",
      "[DEBUG Decorator] Preparing final Processor object...\n",
      "[DEBUG Decorator] Adding setup script to command.\n",
      "[DEBUG Decorator] Final container command:\n",
      "-------\n",
      "python -m pip install dill pydantic redis nebu\n",
      "\n",
      "\n",
      "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
      "pip uninstall -y xformers\n",
      "pip install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
      "pip install unsloth trl\n",
      "\n",
      "\n",
      "python -u -m nebu.processors.consumer\n",
      "-------\n",
      "[DEBUG Decorator] Final Container Request Env Vars (Summary):\n",
      "[DEBUG Decorator]  FUNCTION_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  FUNCTION_NAME: train_unsloth_sft\n",
      "[DEBUG Decorator]  OUTPUT_MODEL_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  CONTENT_TYPE_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  STREAM_MESSAGE_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  PARAM_TYPE_STR: <class 'nebu.processors.models.Message[TrainingRequest]'>\n",
      "[DEBUG Decorator]  RETURN_TYPE_STR: <class '__main__.TrainingResponse'>\n",
      "[DEBUG Decorator]  IS_STREAM_MESSAGE: True\n",
      "[DEBUG Decorator]  CONTENT_TYPE_NAME: TrainingRequest\n",
      "[DEBUG Decorator]  MODULE_NAME: __main__\n",
      "Using namespace: pbarker\n",
      "Existing processors: processors=[]\n",
      "Processor: None\n",
      "Creating processor\n",
      "Request:\n",
      "{'kind': 'Processor', 'metadata': {'name': 'train_unsloth_sft', 'namespace': 'pbarker'}, 'container': {'kind': 'Container', 'platform': 'runpod', 'metadata': {'name': 'train_unsloth_sft'}, 'image': 'pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel', 'env': [{'key': 'FUNCTION_SOURCE', 'value': 'def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\\n    import time\\n    from unsloth import FastVisionModel, is_bf16_supported\\n    from unsloth.trainer import UnslothVisionDataCollator\\n    from trl import SFTTrainer, SFTConfig\\n    from nebu import (\\n        Bucket,\\n        ContainerConfig,\\n        Cache,\\n        Adapter,\\n        find_latest_checkpoint,\\n        is_allowed,\\n        oai_to_unsloth,\\n    )\\n    import requests\\n    import json\\n\\n    print(\"message\", message)\\n    training_request: TrainingRequest = message.content\\n    if not training_request:\\n        raise ValueError(\"No training request provided\")\\n\\n    container_config = ContainerConfig.from_env()\\n    print(\"container_config\", container_config)\\n\\n    cache = Cache()\\n    bucket = Bucket()\\n\\n    print(\"loading model...\")\\n    adapter_uri = f\"{container_config.namespace_volume_uri}/adapters/{training_request.adapter_name}\"\\n    time_start_load = time.time()\\n    model = None\\n\\n    cache_key = f\"adapters:{training_request.adapter_name}\"\\n    print(\"checking cache for adapter\", cache_key)\\n    val_raw = cache.get(cache_key)\\n\\n    is_continue = False\\n    epochs_trained = 0\\n    if val_raw:\\n        adapter = Adapter.model_validate_json(val_raw)\\n        print(\"Found adapter: \", adapter)\\n\\n        epochs_trained = adapter.epochs_trained\\n\\n        if not is_allowed(adapter.owner, message.user_id, message.orgs):\\n            raise ValueError(\"You are not allowed to train this existing adapter\")\\n\\n        time_start = time.time()\\n        bucket.sync(adapter.uri, \"/latest\")\\n        print(f\"Synced in {time.time() - time_start} seconds\")\\n\\n        model, tokenizer = FastVisionModel.from_pretrained(\\n            \"/latest\",\\n            load_in_4bit=False,\\n            use_gradient_checkpointing=\"unsloth\",\\n        )\\n        is_continue = True\\n    if not model:\\n        print(\"Loading model from scratch\")\\n        model, tokenizer = FastVisionModel.from_pretrained(\\n            training_request.model,\\n            load_in_4bit=False,  # Use 4bit to reduce memory use. False for 16bit LoRA.\\n            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\\n        )\\n\\n        print(\"getting peft model...\")\\n        model = FastVisionModel.get_peft_model(\\n            model,\\n            finetune_vision_layers=True,  # False if not finetuning vision layers\\n            finetune_language_layers=True,  # False if not finetuning language layers\\n            finetune_attention_modules=True,  # False if not finetuning attention layers\\n            finetune_mlp_modules=True,  # False if not finetuning MLP layers\\n            r=training_request.lora_rank,  # The larger, the higher the accuracy, but might overfit\\n            lora_alpha=training_request.lora_alpha,  # Recommended alpha == r at least\\n            lora_dropout=training_request.lora_dropout,\\n            bias=\"none\",\\n            random_state=3407,\\n            use_rslora=False,  # We support rank stabilized LoRA\\n            loftq_config=None,  # And LoftQ\\n            use_fast=True,\\n            # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\\n        )\\n    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\\n\\n    print(\"Downloading dataset\")\\n    time_start_download = time.time()\\n    response = requests.get(training_request.dataset)\\n    response.raise_for_status()  # optional: raises if request failed\\n    print(f\"Downloaded dataset in {time.time() - time_start_download} seconds\")\\n\\n    # Decode and split into lines\\n    lines = response.content.decode(\"utf-8\").splitlines()\\n\\n    # Parse and convert each JSON line\\n    time_start_convert = time.time()\\n    converted_dataset = [\\n        oai_to_unsloth(json.loads(line)) for line in lines if line.strip()\\n    ]\\n    print(f\"Converted dataset in {time.time() - time_start_convert} seconds\")\\n\\n    print(converted_dataset)\\n\\n    FastVisionModel.for_training(model)  # Enable for training!\\n\\n    train_epochs = epochs_trained + training_request.epochs\\n\\n    trainer = SFTTrainer(\\n        model=model,\\n        tokenizer=tokenizer,\\n        data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\\n        train_dataset=converted_dataset,\\n        args=SFTConfig(\\n            per_device_train_batch_size=training_request.batch_size,\\n            gradient_accumulation_steps=training_request.gradient_accumulation_steps,\\n            warmup_steps=training_request.warmup_steps,\\n            # max_steps=training_request.max_steps,\\n            num_train_epochs=train_epochs,\\n            learning_rate=training_request.learning_rate,\\n            fp16=not is_bf16_supported(),\\n            bf16=is_bf16_supported(),\\n            logging_steps=training_request.logging_steps,\\n            optim=training_request.optimizer,\\n            weight_decay=training_request.weight_decay,\\n            lr_scheduler_type=\"linear\",\\n            seed=3407,\\n            output_dir=\"outputs\",\\n            report_to=\"none\",  # For Weights and Biases\\n            # You MUST put the below items for vision finetuning:\\n            remove_unused_columns=False,\\n            dataset_text_field=\"\",\\n            dataset_kwargs={\"skip_prepare_dataset\": True},\\n            dataset_num_proc=4,\\n            max_seq_length=training_request.max_length,\\n        ),\\n    )\\n\\n    time_start_train = time.time()\\n    trainer_stats = trainer.train(resume_from_checkpoint=is_continue)\\n    print(trainer_stats)\\n    print(f\"Trained in {time.time() - time_start_train} seconds\")\\n\\n    latest_checkpoint = find_latest_checkpoint(\"outputs\")\\n    print(\"latest checkpoint\")\\n    if latest_checkpoint:\\n        print(\"Copying latest checkpoint to bucket\")\\n        bucket.copy(\\n            latest_checkpoint,\\n            adapter_uri,\\n        )\\n\\n    # TODO: store this in the bucket so we don\\'t need to copy every time\\n    adapter = Adapter(\\n        name=training_request.adapter_name,\\n        uri=adapter_uri,\\n        owner=message.content.owner if message.content.owner else message.user_id,  # type: ignore\\n        base_model=training_request.model,\\n        epochs_trained=train_epochs,\\n        last_trained=int(time.time()),\\n        lora_rank=training_request.lora_rank,\\n        lora_alpha=training_request.lora_alpha,\\n        lora_dropout=training_request.lora_dropout,\\n    )\\n    cache.set(cache_key, adapter.model_dump_json())\\n\\n    return TrainingResponse(\\n        loss=trainer_stats.training_loss,\\n        train_steps_per_second=trainer_stats.metrics[\"train_steps_per_second\"],\\n        train_samples_per_second=trainer_stats.metrics[\"train_samples_per_second\"],\\n        train_runtime=trainer_stats.metrics[\"train_runtime\"],\\n        adapter_name=training_request.adapter_name,\\n        adapter_uri=adapter_uri,\\n    )'}, {'key': 'FUNCTION_NAME', 'value': 'train_unsloth_sft'}, {'key': 'OUTPUT_MODEL_SOURCE', 'value': 'class TrainingResponse(BaseModel):\\n    loss: float\\n    train_steps_per_second: float\\n    train_samples_per_second: float\\n    train_runtime: float\\n    adapter_name: str\\n    adapter_uri: str'}, {'key': 'CONTENT_TYPE_SOURCE', 'value': 'class TrainingRequest(BaseModel):\\n    adapter_name: str\\n    dataset: str\\n    model: str = \"unsloth/Qwen2.5-VL-7B-Instruct\"\\n    max_length: int = 65536\\n    epochs: int = 5\\n    batch_size: int = 2\\n    gradient_accumulation_steps: int = 4\\n    learning_rate: float = 2e-4\\n    weight_decay: float = 0.01\\n    warmup_steps: int = 5\\n    logging_steps: int = 1\\n    save_steps: int = 5\\n    lora_alpha: int = 128\\n    lora_rank: int = 64\\n    lora_dropout: float = 0\\n    optimizer: str = \"adamw_8bit\"\\n    owner: Optional[str] = None'}, {'key': 'STREAM_MESSAGE_SOURCE', 'value': 'class Message(BaseModel, Generic[T]):\\n    kind: str = \"Message\"\\n    id: str\\n    content: Optional[T] = None\\n    created_at: int\\n    return_stream: Optional[str] = None\\n    user_id: Optional[str] = None\\n    orgs: Optional[Any] = None\\n    handle: Optional[str] = None\\n    adapter: Optional[str] = None\\n'}, {'key': 'PARAM_TYPE_STR', 'value': \"<class 'nebu.processors.models.Message[TrainingRequest]'>\"}, {'key': 'RETURN_TYPE_STR', 'value': \"<class '__main__.TrainingResponse'>\"}, {'key': 'IS_STREAM_MESSAGE', 'value': 'True'}, {'key': 'CONTENT_TYPE_NAME', 'value': 'TrainingRequest'}, {'key': 'MODULE_NAME', 'value': '__main__'}], 'command': 'python -m pip install dill pydantic redis nebu\\n\\n\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\\npip uninstall -y xformers\\npip install -U xformers --index-url https://download.pytorch.org/whl/cu126\\npip install unsloth trl\\n\\n\\npython -u -m nebu.processors.consumer', 'accelerators': ['1:A100_SXM'], 'restart': 'Always'}, 'min_replicas': 1, 'max_replicas': 10, 'scale': {'up': {'above_pressure': 10, 'duration': '5m'}, 'down': {'below_pressure': 2, 'duration': '10m'}, 'zero': {'duration': '10m'}}}\n",
      "Created Processor train_unsloth_sft\n",
      "[DEBUG Decorator] Processor instance 'train_unsloth_sft' created successfully.\n"
     ]
    }
   ],
   "source": [
    "class TrainingRequest(BaseModel):\n",
    "    adapter_name: str\n",
    "    dataset: str\n",
    "    model: str = \"unsloth/Qwen2.5-VL-7B-Instruct\"\n",
    "    max_length: int = 65536\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 5\n",
    "    logging_steps: int = 1\n",
    "    save_steps: int = 5\n",
    "    lora_alpha: int = 128\n",
    "    lora_rank: int = 64\n",
    "    lora_dropout: float = 0\n",
    "    optimizer: str = \"adamw_8bit\"\n",
    "    owner: Optional[str] = None\n",
    "\n",
    "\n",
    "class TrainingResponse(BaseModel):\n",
    "    loss: float\n",
    "    train_steps_per_second: float\n",
    "    train_samples_per_second: float\n",
    "    train_runtime: float\n",
    "    adapter_name: str\n",
    "    adapter_uri: str\n",
    "\n",
    "\n",
    "# TODO: add default scale\n",
    "scale = V1Scale(\n",
    "    up=V1ScaleUp(above_pressure=10, duration=\"5m\"),\n",
    "    down=V1ScaleDown(below_pressure=2, duration=\"10m\"),\n",
    "    zero=V1ScaleZero(duration=\"10m\"),\n",
    ")\n",
    "\n",
    "setup_script = \"\"\"\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\n",
    "pip uninstall -y xformers\n",
    "pip install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install unsloth trl\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@processor(\n",
    "    image=\"pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel\",\n",
    "    setup_script=setup_script,\n",
    "    scale=scale,\n",
    "    accelerators=[\"1:A100_SXM\"],\n",
    "    platform=\"runpod\",\n",
    ")\n",
    "def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\n",
    "    import time\n",
    "    from unsloth import FastVisionModel, is_bf16_supported\n",
    "    from unsloth.trainer import UnslothVisionDataCollator\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "    from nebu import (\n",
    "        Bucket,\n",
    "        ContainerConfig,\n",
    "        Cache,\n",
    "        Adapter,\n",
    "        find_latest_checkpoint,\n",
    "        is_allowed,\n",
    "        oai_to_unsloth,\n",
    "    )\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    print(\"message\", message)\n",
    "    training_request: TrainingRequest = message.content\n",
    "    if not training_request:\n",
    "        raise ValueError(\"No training request provided\")\n",
    "\n",
    "    container_config = ContainerConfig.from_env()\n",
    "    print(\"container_config\", container_config)\n",
    "\n",
    "    cache = Cache()\n",
    "    bucket = Bucket()\n",
    "\n",
    "    print(\"loading model...\")\n",
    "    adapter_uri = f\"{container_config.namespace_volume_uri}/adapters/{training_request.adapter_name}\"\n",
    "    time_start_load = time.time()\n",
    "    model = None\n",
    "\n",
    "    cache_key = f\"adapters:{training_request.adapter_name}\"\n",
    "    print(\"checking cache for adapter\", cache_key)\n",
    "    val_raw = cache.get(cache_key)\n",
    "\n",
    "    is_continue = False\n",
    "    epochs_trained = 0\n",
    "    if val_raw:\n",
    "        adapter = Adapter.model_validate_json(val_raw)\n",
    "        print(\"Found adapter: \", adapter)\n",
    "\n",
    "        epochs_trained = adapter.epochs_trained\n",
    "\n",
    "        if not is_allowed(adapter.owner, message.user_id, message.orgs):\n",
    "            raise ValueError(\"You are not allowed to train this existing adapter\")\n",
    "\n",
    "        time_start = time.time()\n",
    "        bucket.sync(adapter.uri, \"/latest\")\n",
    "        print(f\"Synced in {time.time() - time_start} seconds\")\n",
    "\n",
    "        model, tokenizer = FastVisionModel.from_pretrained(\n",
    "            \"/latest\",\n",
    "            load_in_4bit=False,\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "        )\n",
    "        is_continue = True\n",
    "    if not model:\n",
    "        print(\"Loading model from scratch\")\n",
    "        model, tokenizer = FastVisionModel.from_pretrained(\n",
    "            training_request.model,\n",
    "            load_in_4bit=False,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
    "        )\n",
    "\n",
    "        print(\"getting peft model...\")\n",
    "        model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers=True,  # False if not finetuning vision layers\n",
    "            finetune_language_layers=True,  # False if not finetuning language layers\n",
    "            finetune_attention_modules=True,  # False if not finetuning attention layers\n",
    "            finetune_mlp_modules=True,  # False if not finetuning MLP layers\n",
    "            r=training_request.lora_rank,  # The larger, the higher the accuracy, but might overfit\n",
    "            lora_alpha=training_request.lora_alpha,  # Recommended alpha == r at least\n",
    "            lora_dropout=training_request.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,  # We support rank stabilized LoRA\n",
    "            loftq_config=None,  # And LoftQ\n",
    "            use_fast=True,\n",
    "            # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    "        )\n",
    "    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\n",
    "\n",
    "    print(\"Downloading dataset\")\n",
    "    time_start_download = time.time()\n",
    "    response = requests.get(training_request.dataset)\n",
    "    response.raise_for_status()  # optional: raises if request failed\n",
    "    print(f\"Downloaded dataset in {time.time() - time_start_download} seconds\")\n",
    "\n",
    "    # Decode and split into lines\n",
    "    lines = response.content.decode(\"utf-8\").splitlines()\n",
    "\n",
    "    # Parse and convert each JSON line\n",
    "    time_start_convert = time.time()\n",
    "    converted_dataset = [\n",
    "        oai_to_unsloth(json.loads(line)) for line in lines if line.strip()\n",
    "    ]\n",
    "    print(f\"Converted dataset in {time.time() - time_start_convert} seconds\")\n",
    "\n",
    "    print(converted_dataset)\n",
    "\n",
    "    FastVisionModel.for_training(model)  # Enable for training!\n",
    "\n",
    "    train_epochs = epochs_trained + training_request.epochs\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
    "        train_dataset=converted_dataset,\n",
    "        args=SFTConfig(\n",
    "            per_device_train_batch_size=training_request.batch_size,\n",
    "            gradient_accumulation_steps=training_request.gradient_accumulation_steps,\n",
    "            warmup_steps=training_request.warmup_steps,\n",
    "            # max_steps=training_request.max_steps,\n",
    "            num_train_epochs=train_epochs,\n",
    "            learning_rate=training_request.learning_rate,\n",
    "            fp16=not is_bf16_supported(),\n",
    "            bf16=is_bf16_supported(),\n",
    "            logging_steps=training_request.logging_steps,\n",
    "            optim=training_request.optimizer,\n",
    "            weight_decay=training_request.weight_decay,\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            seed=3407,\n",
    "            output_dir=\"outputs\",\n",
    "            report_to=\"none\",  # For Weights and Biases\n",
    "            # You MUST put the below items for vision finetuning:\n",
    "            remove_unused_columns=False,\n",
    "            dataset_text_field=\"\",\n",
    "            dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "            dataset_num_proc=4,\n",
    "            max_seq_length=training_request.max_length,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    time_start_train = time.time()\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=is_continue)\n",
    "    print(trainer_stats)\n",
    "    print(f\"Trained in {time.time() - time_start_train} seconds\")\n",
    "\n",
    "    latest_checkpoint = find_latest_checkpoint(\"outputs\")\n",
    "    print(\"latest checkpoint\")\n",
    "    if latest_checkpoint:\n",
    "        print(\"Copying latest checkpoint to bucket\")\n",
    "        bucket.copy(\n",
    "            latest_checkpoint,\n",
    "            adapter_uri,\n",
    "        )\n",
    "\n",
    "    # TODO: store this in the bucket so we don't need to copy every time\n",
    "    adapter = Adapter(\n",
    "        name=training_request.adapter_name,\n",
    "        uri=adapter_uri,\n",
    "        owner=message.content.owner if message.content.owner else message.user_id,  # type: ignore\n",
    "        base_model=training_request.model,\n",
    "        epochs_trained=train_epochs,\n",
    "        last_trained=int(time.time()),\n",
    "        lora_rank=training_request.lora_rank,\n",
    "        lora_alpha=training_request.lora_alpha,\n",
    "        lora_dropout=training_request.lora_dropout,\n",
    "    )\n",
    "    cache.set(cache_key, adapter.model_dump_json())\n",
    "\n",
    "    return TrainingResponse(\n",
    "        loss=trainer_stats.training_loss,\n",
    "        train_steps_per_second=trainer_stats.metrics[\"train_steps_per_second\"],\n",
    "        train_samples_per_second=trainer_stats.metrics[\"train_samples_per_second\"],\n",
    "        train_runtime=trainer_stats.metrics[\"train_runtime\"],\n",
    "        adapter_name=training_request.adapter_name,\n",
    "        adapter_uri=adapter_uri,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_req = TrainingRequest(\n",
    "    adapter_name=\"clinton1\",\n",
    "    dataset=\"https://storage.googleapis.com/orign/testdata/nebu/clinton30.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'stream_id': '1744502568448-0',\n",
       " 'message_id': 'ujypv1weB7hs7Xv8F2tCHf'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_unsloth_sft.send(training_req.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unsloth_sft.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nebu.processors.decorate import processor\n",
    "from nebu import Message\n",
    "from nebu.chatx.openai import (\n",
    "    ChatCompletionRequest,\n",
    "    ChatCompletionResponse,\n",
    "    ChatCompletionChoice,\n",
    "    ChatCompletionResponseMessage,\n",
    "    Logprobs,\n",
    ")\n",
    "from pydantic import BaseModel\n",
    "from dataclasses import dataclass\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG Decorator Init] @processor decorating function 'infer_qwen_vl'\n",
      "[DEBUG Decorator] Determining execution environment...\n",
      "[DEBUG Helper] Checking if running in Jupyter...\n",
      "[DEBUG Helper] is_jupyter_notebook: IPython class name: <class 'ipykernel.zmqshell.ZMQInteractiveShell'>\n",
      "[DEBUG Helper] is_jupyter_notebook: Jupyter detected (ZMQInteractiveShell).\n",
      "[DEBUG Decorator] Jupyter environment detected.\n",
      "[DEBUG Helper] Attempting to get notebook execution history...\n",
      "[DEBUG Helper] get_notebook_executed_code: Retrieved 6 history entries.\n",
      "[DEBUG Helper] get_notebook_executed_code: Total history source length: 18445\n",
      "[DEBUG Decorator] Retrieved notebook history (length: 18445).\n",
      "[DEBUG Decorator] No manually included objects specified.\n",
      "[DEBUG Decorator] Validating signature and type hints for infer_qwen_vl...\n",
      "[DEBUG Decorator] Raw type hints: {'message': <class 'nebu.processors.models.Message[ChatCompletionRequest]'>, 'return': <class 'nebu.chatx.openai.ChatCompletionResponse'>}\n",
      "[DEBUG Decorator] Parameter 'message' type hint: <class 'nebu.processors.models.Message[ChatCompletionRequest]'>\n",
      "[DEBUG Decorator] Return type hint: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG Decorator] Determining input type structure for param type hint: <class 'nebu.processors.models.Message[ChatCompletionRequest]'>\n",
      "[DEBUG Decorator] get_origin result: None, get_args result: ()\n",
      "[DEBUG Decorator] get_origin failed. Attempting regex fallback on type string: '<class 'nebu.processors.models.Message[ChatCompletionRequest]'>'\n",
      "[DEBUG Decorator] Regex matched generic Message pattern!\n",
      "[DEBUG Decorator] Captured content type name via regex: 'ChatCompletionRequest'\n",
      "[DEBUG Decorator] Successfully resolved content type name 'ChatCompletionRequest' to type: <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG Decorator] Final Input Type Determination: is_stream_message=True, content_type=<class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG Decorator] Validating parameter and return types are BaseModel subclasses...\n",
      "[DEBUG Decorator] check_basemodel: Checking Parameter 'message' - Type: <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG Decorator] check_basemodel: Actual type for Parameter 'message': <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG Decorator] check_basemodel: OK - Parameter 'message' effective type (ChatCompletionRequest) is a BaseModel subclass.\n",
      "[DEBUG Decorator] check_basemodel: Checking Return value - Type: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG Decorator] check_basemodel: Actual type for Return value: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG Decorator] check_basemodel: OK - Return value effective type (ChatCompletionResponse) is a BaseModel subclass.\n",
      "[DEBUG Decorator] Type validation complete.\n",
      "[DEBUG Decorator] Getting source code for function 'infer_qwen_vl'...\n",
      "[DEBUG Decorator] Attempting notebook history extraction for function 'infer_qwen_vl'...\n",
      "[DEBUG Helper] Extracting 'infer_qwen_vl' (FunctionDef) from history string (len: 18445)...\n",
      "[DEBUG Helper] extract: Split history into 7 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'infer_qwen_vl' in cell #5.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'infer_qwen_vl'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'infer_qwen_vl' from cell #5.\n",
      "[DEBUG Decorator] Found function 'infer_qwen_vl' source in notebook history.\n",
      "[DEBUG Decorator] Final function source obtained for 'infer_qwen_vl' (len: 8128). Source starts:\n",
      "-------def infer_qwen_vl(\n",
      "    message: Message[ChatCompletionRequest],\n",
      ") -> ChatCompletionResponse:\n",
      "    import time\n",
      "\n",
      "    full_time = time.time()\n",
      "\n",
      "    import uuid\n",
      "    from unsloth import FastVisionModel\n",
      "    from qwen_vl_utils import process_vision_info\n",
      "    f...\n",
      "-------\n",
      "[DEBUG Decorator] Processing init_func: init\n",
      "[DEBUG Decorator] Attempting notebook history extraction for init_func 'init'...\n",
      "[DEBUG Helper] Extracting 'init' (FunctionDef) from history string (len: 18445)...\n",
      "[DEBUG Helper] extract: Split history into 7 potential cells.\n",
      "[DEBUG Helper] extract: Found node for 'init' in cell #5.\n",
      "[DEBUG Helper] extract: Successfully extracted source using get_source_segment for 'init'.\n",
      "[DEBUG Helper] extract: Found and returning source for 'init' from cell #5.\n",
      "[DEBUG Decorator] Found init_func 'init' source in notebook history.\n",
      "[DEBUG Decorator] Final init_func source obtained for 'init'.\n",
      "[DEBUG Decorator] Getting model sources...\n",
      "[DEBUG Decorator] Getting base Message source...\n",
      "[DEBUG get_type_source] Getting source for type: <class 'nebu.processors.models.Message'>\n",
      "[DEBUG get_model_source] Getting source for: Message\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: Message\n",
      "[DEBUG Helper] Extracting 'Message' (ClassDef) from history string (len: 18445)...\n",
      "[DEBUG Helper] extract: Split history into 7 potential cells.\n",
      "[DEBUG Helper] extract: Definition 'Message' of type ClassDef not found in history search.\n",
      "[DEBUG get_model_source] Notebook history extraction failed for: Message. Proceeding to dill.\n",
      "[DEBUG get_model_source] Attempting dill fallback for: Message\n",
      "[DEBUG get_model_source] Using dill source for: Message\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class 'nebu.processors.models.Message'>\n",
      "[DEBUG Decorator] Input is StreamMessage. Content type: <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG Decorator] Getting source for content_type: <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG get_type_source] Getting source for type: <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG get_model_source] Getting source for: ChatCompletionRequest\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: ChatCompletionRequest\n",
      "[DEBUG Helper] Extracting 'ChatCompletionRequest' (ClassDef) from history string (len: 18445)...\n",
      "[DEBUG Helper] extract: Split history into 7 potential cells.\n",
      "[DEBUG Helper] extract: Definition 'ChatCompletionRequest' of type ClassDef not found in history search.\n",
      "[DEBUG get_model_source] Notebook history extraction failed for: ChatCompletionRequest. Proceeding to dill.\n",
      "[DEBUG get_model_source] Attempting dill fallback for: ChatCompletionRequest\n",
      "[DEBUG get_model_source] Using dill source for: ChatCompletionRequest\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class 'nebu.chatx.openai.ChatCompletionRequest'>\n",
      "[DEBUG Decorator] Getting source for return_type: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG get_type_source] Getting source for type: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG get_model_source] Getting source for: ChatCompletionResponse\n",
      "[DEBUG get_model_source] Attempting notebook history extraction for: ChatCompletionResponse\n",
      "[DEBUG Helper] Extracting 'ChatCompletionResponse' (ClassDef) from history string (len: 18445)...\n",
      "[DEBUG Helper] extract: Split history into 7 potential cells.\n",
      "[DEBUG Helper] extract: Definition 'ChatCompletionResponse' of type ClassDef not found in history search.\n",
      "[DEBUG get_model_source] Notebook history extraction failed for: ChatCompletionResponse. Proceeding to dill.\n",
      "[DEBUG get_model_source] Attempting dill fallback for: ChatCompletionResponse\n",
      "[DEBUG get_model_source] Using dill source for: ChatCompletionResponse\n",
      "[DEBUG get_type_source] Using fallback get_model_source for: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG Decorator] Source Result - Content Type: Found\n",
      "[DEBUG Decorator] Source Result - Input Model (non-stream): Not Found or N/A\n",
      "[DEBUG Decorator] Source Result - Output Model: Found\n",
      "[DEBUG Decorator] Source Result - Base StreamMessage: Found\n",
      "[DEBUG Decorator] Populating environment variables...\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'INPUT_MODEL'\n",
      "[DEBUG Decorator] add_source_to_env: No source for 'INPUT_MODEL', skipping.\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'OUTPUT_MODEL'\n",
      "[DEBUG Decorator] Added env var OUTPUT_MODEL_SOURCE (string)\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'CONTENT_TYPE'\n",
      "[DEBUG Decorator] Added env var CONTENT_TYPE_SOURCE (string)\n",
      "[DEBUG Decorator] add_source_to_env: Processing key 'STREAM_MESSAGE'\n",
      "[DEBUG Decorator] Added env var STREAM_MESSAGE_SOURCE (string)\n",
      "[DEBUG Decorator] Adding INIT_FUNC env vars for init\n",
      "[DEBUG Decorator] Adding type info env vars...\n",
      "[DEBUG Decorator] Finished populating environment variables.\n",
      "[DEBUG Decorator] Preparing final Processor object...\n",
      "[DEBUG Decorator] Adding setup script to command.\n",
      "[DEBUG Decorator] Final container command:\n",
      "-------\n",
      "python -m pip install dill pydantic redis nebu\n",
      "\n",
      "\n",
      "pip install torch torchvision torchaudio qwen-vl-utils --index-url https://download.pytorch.org/whl/cu126\n",
      "pip uninstall -y xformers\n",
      "pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
      "pip install tiktoken unsloth qwen-vl-utils transformers\n",
      "\n",
      "\n",
      "python -u -m nebu.processors.consumer\n",
      "-------\n",
      "[DEBUG Decorator] Final Container Request Env Vars (Summary):\n",
      "[DEBUG Decorator]  FUNCTION_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  FUNCTION_NAME: infer_qwen_vl\n",
      "[DEBUG Decorator]  OUTPUT_MODEL_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  CONTENT_TYPE_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  STREAM_MESSAGE_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  INIT_FUNC_SOURCE: <source code present>\n",
      "[DEBUG Decorator]  INIT_FUNC_NAME: init\n",
      "[DEBUG Decorator]  PARAM_TYPE_STR: <class 'nebu.processors.models.Message[ChatCompletionRequest]'>\n",
      "[DEBUG Decorator]  RETURN_TYPE_STR: <class 'nebu.chatx.openai.ChatCompletionResponse'>\n",
      "[DEBUG Decorator]  IS_STREAM_MESSAGE: True\n",
      "[DEBUG Decorator]  CONTENT_TYPE_NAME: ChatCompletionRequest\n",
      "[DEBUG Decorator]  MODULE_NAME: __main__\n",
      "Using namespace: pbarker\n",
      "Existing processors: processors=[V1Processor(kind='Processor', metadata=V1ResourceMeta(name='train_unsloth_sft', namespace='pbarker', id='bpivwBM4tPdH6eWtrAMkYF', owner='patrick.barker@kentauros.ai', created_at=1744502339, updated_at=1744502341, created_by='patrick.barker@kentauros.ai', owner_ref=None, labels=None), container=V1ContainerRequest(kind='Container', platform='runpod', metadata=V1ResourceMetaRequest(name='train_unsloth_sft', namespace=None, labels=None, owner=None, owner_ref=None), image='pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel', env=[V1EnvVar(key='FUNCTION_SOURCE', value='def train_unsloth_sft(message: Message[TrainingRequest]) -> TrainingResponse:\\n    import time\\n    from unsloth import FastVisionModel, is_bf16_supported\\n    from unsloth.trainer import UnslothVisionDataCollator\\n    from trl import SFTTrainer, SFTConfig\\n    from nebu import (\\n        Bucket,\\n        ContainerConfig,\\n        Cache,\\n        Adapter,\\n        find_latest_checkpoint,\\n        is_allowed,\\n        oai_to_unsloth,\\n    )\\n    import requests\\n    import json\\n\\n    print(\"message\", message)\\n    training_request: TrainingRequest = message.content\\n    if not training_request:\\n        raise ValueError(\"No training request provided\")\\n\\n    container_config = ContainerConfig.from_env()\\n    print(\"container_config\", container_config)\\n\\n    cache = Cache()\\n    bucket = Bucket()\\n\\n    print(\"loading model...\")\\n    adapter_uri = f\"{container_config.namespace_volume_uri}/adapters/{training_request.adapter_name}\"\\n    time_start_load = time.time()\\n    model = None\\n\\n    cache_key = f\"adapters:{training_request.adapter_name}\"\\n    print(\"checking cache for adapter\", cache_key)\\n    val_raw = cache.get(cache_key)\\n\\n    is_continue = False\\n    epochs_trained = 0\\n    if val_raw:\\n        adapter = Adapter.model_validate_json(val_raw)\\n        print(\"Found adapter: \", adapter)\\n\\n        epochs_trained = adapter.epochs_trained\\n\\n        if not is_allowed(adapter.owner, message.user_id, message.orgs):\\n            raise ValueError(\"You are not allowed to train this existing adapter\")\\n\\n        time_start = time.time()\\n        bucket.sync(adapter.uri, \"/latest\")\\n        print(f\"Synced in {time.time() - time_start} seconds\")\\n\\n        model, tokenizer = FastVisionModel.from_pretrained(\\n            \"/latest\",\\n            load_in_4bit=False,\\n            use_gradient_checkpointing=\"unsloth\",\\n        )\\n        is_continue = True\\n    if not model:\\n        print(\"Loading model from scratch\")\\n        model, tokenizer = FastVisionModel.from_pretrained(\\n            training_request.model,\\n            load_in_4bit=False,  # Use 4bit to reduce memory use. False for 16bit LoRA.\\n            use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\\n        )\\n\\n        print(\"getting peft model...\")\\n        model = FastVisionModel.get_peft_model(\\n            model,\\n            finetune_vision_layers=True,  # False if not finetuning vision layers\\n            finetune_language_layers=True,  # False if not finetuning language layers\\n            finetune_attention_modules=True,  # False if not finetuning attention layers\\n            finetune_mlp_modules=True,  # False if not finetuning MLP layers\\n            r=training_request.lora_rank,  # The larger, the higher the accuracy, but might overfit\\n            lora_alpha=training_request.lora_alpha,  # Recommended alpha == r at least\\n            lora_dropout=training_request.lora_dropout,\\n            bias=\"none\",\\n            random_state=3407,\\n            use_rslora=False,  # We support rank stabilized LoRA\\n            loftq_config=None,  # And LoftQ\\n            use_fast=True,\\n            # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\\n        )\\n    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\\n\\n    print(\"Downloading dataset\")\\n    time_start_download = time.time()\\n    response = requests.get(training_request.dataset)\\n    response.raise_for_status()  # optional: raises if request failed\\n    print(f\"Downloaded dataset in {time.time() - time_start_download} seconds\")\\n\\n    # Decode and split into lines\\n    lines = response.content.decode(\"utf-8\").splitlines()\\n\\n    # Parse and convert each JSON line\\n    time_start_convert = time.time()\\n    converted_dataset = [\\n        oai_to_unsloth(json.loads(line)) for line in lines if line.strip()\\n    ]\\n    print(f\"Converted dataset in {time.time() - time_start_convert} seconds\")\\n\\n    print(converted_dataset)\\n\\n    FastVisionModel.for_training(model)  # Enable for training!\\n\\n    train_epochs = epochs_trained + training_request.epochs\\n\\n    trainer = SFTTrainer(\\n        model=model,\\n        tokenizer=tokenizer,\\n        data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\\n        train_dataset=converted_dataset,\\n        args=SFTConfig(\\n            per_device_train_batch_size=training_request.batch_size,\\n            gradient_accumulation_steps=training_request.gradient_accumulation_steps,\\n            warmup_steps=training_request.warmup_steps,\\n            # max_steps=training_request.max_steps,\\n            num_train_epochs=train_epochs,\\n            learning_rate=training_request.learning_rate,\\n            fp16=not is_bf16_supported(),\\n            bf16=is_bf16_supported(),\\n            logging_steps=training_request.logging_steps,\\n            optim=training_request.optimizer,\\n            weight_decay=training_request.weight_decay,\\n            lr_scheduler_type=\"linear\",\\n            seed=3407,\\n            output_dir=\"outputs\",\\n            report_to=\"none\",  # For Weights and Biases\\n            # You MUST put the below items for vision finetuning:\\n            remove_unused_columns=False,\\n            dataset_text_field=\"\",\\n            dataset_kwargs={\"skip_prepare_dataset\": True},\\n            dataset_num_proc=4,\\n            max_seq_length=training_request.max_length,\\n        ),\\n    )\\n\\n    time_start_train = time.time()\\n    trainer_stats = trainer.train(resume_from_checkpoint=is_continue)\\n    print(trainer_stats)\\n    print(f\"Trained in {time.time() - time_start_train} seconds\")\\n\\n    latest_checkpoint = find_latest_checkpoint(\"outputs\")\\n    print(\"latest checkpoint\")\\n    if latest_checkpoint:\\n        print(\"Copying latest checkpoint to bucket\")\\n        bucket.copy(\\n            latest_checkpoint,\\n            adapter_uri,\\n        )\\n\\n    # TODO: store this in the bucket so we don\\'t need to copy every time\\n    adapter = Adapter(\\n        name=training_request.adapter_name,\\n        uri=adapter_uri,\\n        owner=message.content.owner if message.content.owner else message.user_id,  # type: ignore\\n        base_model=training_request.model,\\n        epochs_trained=train_epochs,\\n        last_trained=int(time.time()),\\n        lora_rank=training_request.lora_rank,\\n        lora_alpha=training_request.lora_alpha,\\n        lora_dropout=training_request.lora_dropout,\\n    )\\n    cache.set(cache_key, adapter.model_dump_json())\\n\\n    return TrainingResponse(\\n        loss=trainer_stats.training_loss,\\n        train_steps_per_second=trainer_stats.metrics[\"train_steps_per_second\"],\\n        train_samples_per_second=trainer_stats.metrics[\"train_samples_per_second\"],\\n        train_runtime=trainer_stats.metrics[\"train_runtime\"],\\n        adapter_name=training_request.adapter_name,\\n        adapter_uri=adapter_uri,\\n    )', secret_name=None), V1EnvVar(key='FUNCTION_NAME', value='train_unsloth_sft', secret_name=None), V1EnvVar(key='OUTPUT_MODEL_SOURCE', value='class TrainingResponse(BaseModel):\\n    loss: float\\n    train_steps_per_second: float\\n    train_samples_per_second: float\\n    train_runtime: float\\n    adapter_name: str\\n    adapter_uri: str', secret_name=None), V1EnvVar(key='CONTENT_TYPE_SOURCE', value='class TrainingRequest(BaseModel):\\n    adapter_name: str\\n    dataset: str\\n    model: str = \"unsloth/Qwen2.5-VL-7B-Instruct\"\\n    max_length: int = 65536\\n    epochs: int = 5\\n    batch_size: int = 2\\n    gradient_accumulation_steps: int = 4\\n    learning_rate: float = 2e-4\\n    weight_decay: float = 0.01\\n    warmup_steps: int = 5\\n    logging_steps: int = 1\\n    save_steps: int = 5\\n    lora_alpha: int = 128\\n    lora_rank: int = 64\\n    lora_dropout: float = 0\\n    optimizer: str = \"adamw_8bit\"\\n    owner: Optional[str] = None', secret_name=None), V1EnvVar(key='STREAM_MESSAGE_SOURCE', value='class Message(BaseModel, Generic[T]):\\n    kind: str = \"Message\"\\n    id: str\\n    content: Optional[T] = None\\n    created_at: int\\n    return_stream: Optional[str] = None\\n    user_id: Optional[str] = None\\n    orgs: Optional[Any] = None\\n    handle: Optional[str] = None\\n    adapter: Optional[str] = None\\n', secret_name=None), V1EnvVar(key='PARAM_TYPE_STR', value=\"<class 'nebu.processors.models.Message[TrainingRequest]'>\", secret_name=None), V1EnvVar(key='RETURN_TYPE_STR', value=\"<class '__main__.TrainingResponse'>\", secret_name=None), V1EnvVar(key='IS_STREAM_MESSAGE', value='True', secret_name=None), V1EnvVar(key='CONTENT_TYPE_NAME', value='TrainingRequest', secret_name=None), V1EnvVar(key='MODULE_NAME', value='__main__', secret_name=None)], command='python -m pip install dill pydantic redis nebu\\n\\n\\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126\\npip uninstall -y xformers\\npip install -U xformers --index-url https://download.pytorch.org/whl/cu126\\npip install unsloth trl\\n\\n\\npython -u -m nebu.processors.consumer', volumes=None, accelerators=['1:A100_SXM'], resources=None, meters=None, restart='Always', queue=None, timeout=None, ssh_keys=None, ports=None, proxy_port=None, authz=None, health_check=None), stream='processor:pbarker:train_unsloth_sft', schema_=None, common_schema=None, min_replicas=1, max_replicas=10, scale=V1Scale(up=V1ScaleUp(above_pressure=10, duration='5m'), down=V1ScaleDown(below_pressure=2, duration='10m'), zero=V1ScaleZero(duration='10m')), status=V1ProcessorStatus(status='created', message=None, pressure=None))]\n",
      "Processor: None\n",
      "Creating processor\n",
      "Request:\n",
      "{'kind': 'Processor', 'metadata': {'name': 'infer_qwen_vl', 'namespace': 'pbarker'}, 'container': {'kind': 'Container', 'platform': 'runpod', 'metadata': {'name': 'infer_qwen_vl'}, 'image': 'pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel', 'env': [{'key': 'FUNCTION_SOURCE', 'value': 'def infer_qwen_vl(\\n    message: Message[ChatCompletionRequest],\\n) -> ChatCompletionResponse:\\n    import time\\n\\n    full_time = time.time()\\n\\n    import uuid\\n    from unsloth import FastVisionModel\\n    from qwen_vl_utils import process_vision_info\\n    from nebu import (\\n        Bucket,\\n        ContainerConfig,\\n        Adapter,\\n        Cache,\\n        is_allowed,\\n        oai_to_qwen,\\n    )\\n    import qwen_vl_utils\\n    import requests\\n    from PIL import Image\\n    from io import BytesIO\\n\\n    base_model_id = \"unsloth/Qwen2.5-VL-7B-Instruct\"\\n\\n    global state\\n\\n    print(\"message\", message)\\n    training_request = message.content\\n    if not training_request:\\n        raise ValueError(\"No training request provided\")\\n\\n    print(\"content\", message.content)\\n\\n    container_config = ContainerConfig.from_env()\\n    print(\"container_config\", container_config)\\n\\n    content = message.content\\n    if not content:\\n        raise ValueError(\"No content provided\")\\n\\n    adapter_hot_start = time.time()\\n    print(\"checking cache for adapter\", f\"\\'adapters:{content.model}\\'\")\\n    val_raw = state.cache.get(f\"adapters:{content.model}\")\\n    if val_raw:\\n        print(\"val_raw\", val_raw)\\n        val = Adapter.model_validate_json(val_raw)\\n        print(\"found adapter in cache\", val)\\n\\n        if not is_allowed(val.owner, message.user_id, message.orgs):\\n            raise ValueError(\"You are not allowed to use this adapter\")\\n\\n        if not val.base_model == base_model_id:\\n            raise ValueError(\\n                \"The base model of the adapter does not match the model you are trying to use\"\\n            )\\n\\n        loaded = False\\n        for adapter in state.adapters:\\n            print(\"cached adapter: \", adapter)\\n            if val.name == content.model and val.created_at == adapter.created_at:\\n                loaded = True\\n                print(\"adapter already loaded\", content.model)\\n                break\\n        print(f\"Adapter hot start: {time.time() - adapter_hot_start} seconds\")\\n\\n        if not loaded:\\n            bucket = Bucket()\\n            print(\"copying adapter\", val.uri, f\"./adapters/{content.model}\")\\n\\n            time_start = time.time()\\n            bucket.copy(val.uri, f\"./adapters/{content.model}\")\\n            print(f\"Copied in {time.time() - time_start} seconds\")\\n\\n            print(\"loading adapter\", content.model)\\n            state.base_model.load_adapter(\\n                f\"./adapters/{content.model}\", adapter_name=content.model\\n            )\\n            state.adapters.append(val)  # type: ignore\\n            print(\"loaded adapter\", content.model)\\n\\n    else:\\n        raise ValueError(f\"Adapter \\'{content.model}\\' not found\")\\n\\n    print(\"setting adapter\", content.model)\\n    state.base_model.set_adapter(content.model)\\n\\n    conent_dict = content.model_dump()\\n    messages_oai = conent_dict[\"messages\"]\\n    messages = oai_to_qwen(messages_oai)\\n\\n    # === Simplified Debugging Block Start ===\\n    # Make sure qwen_vl_utils is available after setup script runs\\n    # try:\\n    #     from qwen_vl_utils import process_vision_info\\n    # except ImportError:\\n    #     print(\"ERROR: Failed to import qwen_vl_utils. Check setup_script.\")\\n    #     raise\\n\\n    # print(\"\\\\\\\\n--- DEBUGGING IMAGE FETCH START ---\")\\n    # # Iterate through the *converted* messages to find the image URL\\n    # debug_url = None\\n    # for msg in messages:  # Use the Qwen-formatted messages\\n    #     if isinstance(msg.get(\"content\"), list):\\n    #         for item in msg[\"content\"]:\\n    #             # Check for the \\'image\\' key which should contain the URL after oai_to_qwen\\n    #             if item.get(\"type\") == \"image\" and isinstance(item.get(\"image\"), str):\\n    #                 src = item[\"image\"]\\n    #                 if src.startswith(\"http://\") or src.startswith(\"https://\"):\\n    #                     debug_url = src\\n    #                     print(f\"Found image URL for debugging: {debug_url}\")\\n    #                     break  # Found first URL, use it\\n    #         if debug_url:\\n    #             break\\n\\n    # if debug_url:\\n    #     print(f\"Attempting to fetch and check URL: {debug_url}\")\\n    #     try:\\n    #         response = requests.get(\\n    #             debug_url, timeout=30, stream=True\\n    #         )  # Use stream=True initially\\n    #         print(f\"  Status Code: {response.status_code}\")\\n    #         print(f\"  Content-Type Header: {response.headers.get(\\'Content-Type\\')}\")\\n    #         print(f\"  Content Length: {response.headers.get(\\'Content-Length\\')}\")\\n\\n    #         # Read content carefully now\\n    #         image_content = response.content  # Reads full content into memory\\n\\n    #         print(f\"  Content (first 100 bytes): {image_content[:100]}\")\\n    #         response.raise_for_status()  # Check for HTTP errors after getting content\\n\\n    #         print(\"  Attempting to open with PIL...\")\\n    #         try:\\n    #             with BytesIO(image_content) as bio:\\n    #                 img = Image.open(bio)\\n    #                 img.load()  # Attempt to load image data\\n    #                 print(\\n    #                     f\"  PIL Success: Format={img.format}, Size={img.size}, Mode={img.mode}\"\\n    #                 )\\n    #         except Exception as pil_err:\\n    #             print(f\"  PIL Error: Could not identify image file.\")\\n    #             print(f\"  Specific PIL Error: {pil_err}\")\\n    #             # traceback.print_exc() # Uncomment for full PIL traceback if needed\\n\\n    #     except requests.exceptions.RequestException as req_err:\\n    #         print(f\"  Requests Error: {req_err}\")\\n    #         # traceback.print_exc() # Uncomment for full requests traceback\\n    #     except Exception as e:\\n    #         print(f\"  Unexpected Error during request/check: {e}\")\\n    #         # traceback.print_exc() # Uncomment for full general traceback\\n    # else:\\n    #     print(\"No HTTP/HTTPS image URL found in Qwen-formatted messages for debugging.\")\\n    # print(\"--- DEBUGGING IMAGE FETCH END ---\\\\\\\\n\")\\n    # === Simplified Debugging Block End ===\\n\\n    # Preparation for inference\\n    print(\"preparing inputs using messages: \", messages)\\n    inputs_start = time.time()\\n    text = state.model_processor.apply_chat_template(\\n        messages, tokenize=False, add_generation_prompt=True\\n    )\\n    print(\"text: \", text)\\n    print(\"processing vision info: \", messages)\\n    image_inputs, video_inputs = process_vision_info(messages)\\n    inputs = state.model_processor(\\n        text=[text],\\n        images=image_inputs,\\n        videos=video_inputs,\\n        padding=True,\\n        return_tensors=\"pt\",\\n    )\\n    inputs = inputs.to(\"cuda\")\\n    print(\"inputs\", inputs)\\n    print(f\"Inputs prepared in {time.time() - inputs_start} seconds\")\\n\\n    # Inference: Generation of the output\\n    generated_ids = state.base_model.generate(\\n        **inputs, max_new_tokens=content.max_tokens\\n    )\\n    generated_ids_trimmed = [\\n        out_ids[len(in_ids) :]\\n        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\\n    ]\\n    generation_start = time.time()\\n    output_text = state.model_processor.batch_decode(\\n        generated_ids_trimmed,\\n        skip_special_tokens=True,\\n        clean_up_tokenization_spaces=False,\\n    )\\n    print(\"output_text\", output_text)\\n    print(f\"Generation took {time.time() - generation_start} seconds\")\\n\\n    # Build the Pydantic model, referencing your enumerations and classes\\n    response = ChatCompletionResponse(\\n        id=str(uuid.uuid4()),\\n        created=int(time.time()),\\n        model=content.model,\\n        object=\"chat.completion\",\\n        choices=[\\n            ChatCompletionChoice(\\n                index=0,\\n                finish_reason=\"stop\",  # or another appropriate reason\\n                message=ChatCompletionResponseMessage(\\n                    role=\"assistant\", content=output_text[0]\\n                ),\\n                # Stub logprobs; in reality, you\\'d fill these from your model if you have them\\n                logprobs=Logprobs(content=[]),\\n            )\\n        ],\\n        service_tier=None,\\n        system_fingerprint=None,\\n        usage=None,\\n    )\\n    print(f\"Total time: {time.time() - full_time} seconds\")\\n\\n    return response'}, {'key': 'FUNCTION_NAME', 'value': 'infer_qwen_vl'}, {'key': 'OUTPUT_MODEL_SOURCE', 'value': 'class ChatCompletionResponse(BaseModel):\\n    id: str = Field(..., description=\"A unique identifier for the chat completion.\")\\n    choices: List[ChatCompletionChoice] = Field(\\n        ...,\\n        description=\"A list of chat completion choices. Can be more than one if `n` is greater than 1.\",\\n    )\\n    created: int = Field(\\n        ...,\\n        description=\"The Unix timestamp (in seconds) of when the chat completion was created.\",\\n    )\\n    model: str = Field(..., description=\"The model used for the chat completion.\")\\n    service_tier: str | None = Field(\\n        None,\\n        description=\"The service tier used for processing the request. Possible values: [\\'scale\\', \\'default\\', \\'auto\\']\",\\n        examples=[\"scale\"],\\n    )\\n    system_fingerprint: str | None = Field(\\n        None,\\n        description=\"This fingerprint represents the backend configuration that the model runs with.\\\\\\\\n\\\\\\\\nCan be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.\\\\\\\\n\",\\n    )\\n    object: Literal[\"chat.completion\"] = Field(\\n        ..., description=\"The object type, which is always `chat.completion`.\"\\n    )\\n    usage: CompletionUsage | None = None\\n'}, {'key': 'CONTENT_TYPE_SOURCE', 'value': 'class ChatCompletionRequest(ModelResponseProperties):\\n    messages: List[ChatCompletionRequestMessage] = Field(\\n        ...,\\n        description=\"A list of messages comprising the conversation so far. Depending on the\\\\n[model](/docs/models) you use, different message types (modalities) are\\\\nsupported, like [text](/docs/guides/text-generation),\\\\n[images](/docs/guides/vision), and [audio](/docs/guides/audio).\\\\n\",\\n        min_length=1,\\n    )\\n    model: str = Field(\\n        ...,\\n        description=\"Model ID used to generate the response, like `gpt-4o` or `o1`. OpenAI\\\\noffers a wide range of models with different capabilities, performance\\\\ncharacteristics, and price points. Refer to the [model guide](/docs/models)\\\\nto browse and compare available models.\\\\n\",\\n    )\\n    modalities: ChatCompletionModalities | None = None\\n    reasoning_effort: str | None = (\\n        \"medium\"  # Possible values: [\\'low\\', \\'medium\\', \\'high\\']\\n    )\\n    max_completion_tokens: int | None = Field(\\n        None,\\n        description=\"An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).\\\\n\",\\n    )\\n    frequency_penalty: float | None = Field(\\n        0.0,\\n        description=\"Number between -2.0 and 2.0. Positive values penalize new tokens based on\\\\ntheir existing frequency in the text so far, decreasing the model\\'s\\\\nlikelihood to repeat the same line verbatim.\\\\n\",\\n        ge=-2.0,\\n        le=2.0,\\n    )\\n    presence_penalty: float | None = Field(\\n        0.0,\\n        description=\"Number between -2.0 and 2.0. Positive values penalize new tokens based on\\\\nwhether they appear in the text so far, increasing the model\\'s likelihood\\\\nto talk about new topics.\\\\n\",\\n        ge=-2.0,\\n        le=2.0,\\n    )\\n    web_search_options: WebSearchOptions | None = Field(\\n        None,\\n        description=\"This tool searches the web for relevant results to use in a response.\\\\nLearn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).\\\\n\",\\n        title=\"Web search\",\\n    )\\n    top_logprobs: int | None = Field(\\n        None,\\n        description=\"An integer between 0 and 20 specifying the number of most likely tokens to\\\\nreturn at each token position, each with an associated log probability.\\\\n`logprobs` must be set to `true` if this parameter is used.\\\\n\",\\n        ge=0,\\n        le=20,\\n    )\\n    response_format: Union[\\n        ResponseFormatText, ResponseFormatJsonSchema, ResponseFormatJsonObject, None\\n    ] = Field(\\n        None,\\n        description=\\'An object specifying the format that the model must output.\\\\n\\\\nSetting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables\\\\nStructured Outputs which ensures the model will match your supplied JSON\\\\nschema. Learn more in the [Structured Outputs\\\\nguide](/docs/guides/structured-outputs).\\\\n\\\\nSetting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\\\\nensures the message the model generates is valid JSON. Using `json_schema`\\\\nis preferred for models that support it.\\\\n\\',\\n    )\\n    service_tier: str | None = Field(\\n        \"auto\",\\n        description=\"Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:\\\\n  - If set to \\'auto\\', and the Project is Scale tier enabled, the system\\\\n    will utilize scale tier credits until they are exhausted.\\\\n  - If set to \\'auto\\', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\\\\n  - If set to \\'default\\', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\\\\n  - When not set, the default behavior is \\'auto\\'.\\\\n\\\\n  When this parameter is set, the response body will include the `service_tier` utilized.\\\\n\"\\n        + \" Possible values: [\\'scale\\', \\'default\\', \\'auto\\']\",\\n    )\\n    audio: AudioOutputSettings | None = Field(\\n        None,\\n        description=\\'Parameters for audio output. Required when audio output is requested with\\\\n`modalities: [\"audio\"]`. [Learn more](/docs/guides/audio).\\\\n\\',\\n    )\\n    store: bool | None = Field(\\n        False,\\n        description=\"Whether or not to store the output of this chat completion request for \\\\nuse in our [model distillation](/docs/guides/distillation) or\\\\n[evals](/docs/guides/evals) products.\\\\n\",\\n    )\\n    stream: bool | None = Field(\\n        False,\\n        description=\"If set to true, the model response data will be streamed to the client\\\\nas it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\\\\nSee the [Streaming section below](/docs/api-reference/chat/streaming)\\\\nfor more information, along with the [streaming responses](/docs/guides/streaming-responses)\\\\nguide for more information on how to handle the streaming events.\\\\n\",\\n    )\\n    stop: StopConfiguration | None = None\\n    logit_bias: Dict[str, int] | None = Field(\\n        None,\\n        description=\"Modify the likelihood of specified tokens appearing in the completion.\\\\n\\\\nAccepts a JSON object that maps tokens (specified by their token ID in the\\\\ntokenizer) to an associated bias value from -100 to 100. Mathematically,\\\\nthe bias is added to the logits generated by the model prior to sampling.\\\\nThe exact effect will vary per model, but values between -1 and 1 should\\\\ndecrease or increase likelihood of selection; values like -100 or 100\\\\nshould result in a ban or exclusive selection of the relevant token.\\\\n\",\\n    )\\n    logprobs: bool | None = Field(\\n        False,\\n        description=\"Whether to return log probabilities of the output tokens or not. If true,\\\\nreturns the log probabilities of each output token returned in the\\\\n`content` of `message`.\\\\\\\\n\",\\n    )\\n    max_tokens: int | None = Field(\\n        None,\\n        description=\"The maximum number of [tokens](/tokenizer) that can be generated in the\\\\nchat completion. This value can be used to control\\\\n[costs](https://openai.com/api/pricing/) for text generated via API.\\\\\\\\n\\\\\\\\nThis value is now deprecated in favor of `max_completion_tokens`, and is\\\\\\\\nnot compatible with [o1 series models](/docs/guides/reasoning).\\\\\\\\n\",\\n    )\\n    n: int | None = Field(\\n        1,\\n        description=\"How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.\",\\n        examples=[1],\\n        ge=1,\\n        le=128,\\n    )\\n    prediction: PredictionContent | None = Field(\\n        None,\\n        description=\"Configuration for a [Predicted Output](/docs/guides/predicted-outputs),\\\\nwhich can greatly improve response times when large parts of the model\\\\nresponse are known ahead of time. This is most common when you are\\\\\\\\nregenerating a file with only minor changes to most of the content.\\\\\\\\n\",\\n    )\\n    seed: int | None = Field(\\n        None,\\n        description=\"This feature is in Beta.\\\\\\\\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\\\\\\\\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\\\\\\\\n\",\\n    )\\n    stream_options: ChatCompletionStreamOptions | None = None\\n    tools: List[ChatCompletionTool] | None = Field(\\n        None,\\n        description=\"A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\\\\\\\\n\",\\n    )\\n    tool_choice: ChatCompletionToolChoiceOption | None = None\\n    parallel_tool_calls: ParallelToolCalls | None = Field(\\n        default_factory=lambda: ParallelToolCalls.model_validate(True)\\n    )\\n    function_call: Union[str, ChatCompletionFunctionCallOption, None] = Field(\\n        None,\\n        description=\"Deprecated in favor of `tool_choice`.\\\\\\\\n\\\\\\\\nControls which (if any) function is called by the model.\\\\\\\\n\\\\\\\\n`none` means the model will not call a function and instead generates a\\\\\\\\nmessage.\\\\\\\\n\\\\\\\\n`auto` means the model can pick between generating a message or calling a\\\\\\\\nfunction.\\\\\\\\n\\\\\\\\nSpecifying a particular function via `{\\\\\"name\\\\\": \\\\\"my_function\\\\\"}` forces the\\\\\\\\nmodel to call that function.\\\\\\\\n\\\\\\\\n`none` is the default when no functions are present. `auto` is the default\\\\\\\\nif functions are present.\\\\\\\\n\\'. Possible values for string: [\\'none\\', \\'auto\\']\",\\n    )\\n    functions: List[ChatCompletionFunctions] | None = Field(\\n        None,\\n        description=\"Deprecated in favor of `tools`.\\\\\\\\n\\\\\\\\nA list of functions the model may generate JSON inputs for.\\\\\\\\n\",\\n        max_length=128,\\n        min_length=1,\\n    )\\n'}, {'key': 'STREAM_MESSAGE_SOURCE', 'value': 'class Message(BaseModel, Generic[T]):\\n    kind: str = \"Message\"\\n    id: str\\n    content: Optional[T] = None\\n    created_at: int\\n    return_stream: Optional[str] = None\\n    user_id: Optional[str] = None\\n    orgs: Optional[Any] = None\\n    handle: Optional[str] = None\\n    adapter: Optional[str] = None\\n'}, {'key': 'INIT_FUNC_SOURCE', 'value': 'def init():\\n    from dataclasses import dataclass\\n    import time\\n    from typing import List, Any\\n    from unsloth import FastVisionModel\\n    from nebu import Adapter, Cache\\n\\n    @dataclass\\n    class InferenceState:\\n        base_model: FastVisionModel\\n        model_processor: Any\\n        base_model_id: str\\n        adapters: List[Adapter]\\n        cache: Cache\\n\\n    base_model_id = \"unsloth/Qwen2.5-VL-7B-Instruct\"\\n\\n    print(\"loading model...\")\\n    time_start_load = time.time()\\n    base_model, model_processor = FastVisionModel.from_pretrained(\\n        base_model_id,\\n        load_in_4bit=False,\\n    )\\n    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\\n    FastVisionModel.for_inference(base_model)\\n\\n    global state\\n    state = InferenceState(\\n        base_model=base_model,\\n        model_processor=model_processor,\\n        base_model_id=base_model_id,\\n        adapters=[],\\n        cache=Cache(),\\n    )'}, {'key': 'INIT_FUNC_NAME', 'value': 'init'}, {'key': 'PARAM_TYPE_STR', 'value': \"<class 'nebu.processors.models.Message[ChatCompletionRequest]'>\"}, {'key': 'RETURN_TYPE_STR', 'value': \"<class 'nebu.chatx.openai.ChatCompletionResponse'>\"}, {'key': 'IS_STREAM_MESSAGE', 'value': 'True'}, {'key': 'CONTENT_TYPE_NAME', 'value': 'ChatCompletionRequest'}, {'key': 'MODULE_NAME', 'value': '__main__'}], 'command': 'python -m pip install dill pydantic redis nebu\\n\\n\\npip install torch torchvision torchaudio qwen-vl-utils --index-url https://download.pytorch.org/whl/cu126\\npip uninstall -y xformers\\npip3 install -U xformers --index-url https://download.pytorch.org/whl/cu126\\npip install tiktoken unsloth qwen-vl-utils transformers\\n\\n\\npython -u -m nebu.processors.consumer', 'accelerators': ['1:A100_SXM'], 'restart': 'Always'}, 'min_replicas': 1, 'max_replicas': 10, 'scale': {'up': {'above_pressure': 30, 'duration': '1m'}, 'down': {'below_pressure': 2, 'duration': '1m'}, 'zero': {'duration': '5m'}}}\n",
      "Created Processor infer_qwen_vl\n",
      "[DEBUG Decorator] Processor instance 'infer_qwen_vl' created successfully.\n"
     ]
    }
   ],
   "source": [
    "setup_script = \"\"\"\n",
    "pip install torch torchvision torchaudio qwen-vl-utils --index-url https://download.pytorch.org/whl/cu126\n",
    "pip uninstall -y xformers\n",
    "pip3 install -U xformers --index-url https://download.pytorch.org/whl/cu126\n",
    "pip install tiktoken unsloth qwen-vl-utils transformers\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def init():\n",
    "    from dataclasses import dataclass\n",
    "    import time\n",
    "    from typing import List, Any\n",
    "    from unsloth import FastVisionModel\n",
    "    from nebu import Adapter, Cache\n",
    "\n",
    "    @dataclass\n",
    "    class InferenceState:\n",
    "        base_model: FastVisionModel\n",
    "        model_processor: Any\n",
    "        base_model_id: str\n",
    "        adapters: List[Adapter]\n",
    "        cache: Cache\n",
    "\n",
    "    base_model_id = \"unsloth/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "    print(\"loading model...\")\n",
    "    time_start_load = time.time()\n",
    "    base_model, model_processor = FastVisionModel.from_pretrained(\n",
    "        base_model_id,\n",
    "        load_in_4bit=False,\n",
    "    )\n",
    "    print(f\"Loaded model in {time.time() - time_start_load} seconds\")\n",
    "    FastVisionModel.for_inference(base_model)\n",
    "\n",
    "    global state\n",
    "    state = InferenceState(\n",
    "        base_model=base_model,\n",
    "        model_processor=model_processor,\n",
    "        base_model_id=base_model_id,\n",
    "        adapters=[],\n",
    "        cache=Cache(),\n",
    "    )\n",
    "\n",
    "\n",
    "@processor(\n",
    "    image=\"pytorch/pytorch:2.6.0-cuda12.6-cudnn9-devel\",\n",
    "    setup_script=setup_script,\n",
    "    accelerators=[\"1:A100_SXM\"],\n",
    "    platform=\"runpod\",\n",
    "    init_func=init,\n",
    ")\n",
    "def infer_qwen_vl(\n",
    "    message: Message[ChatCompletionRequest],\n",
    ") -> ChatCompletionResponse:\n",
    "    import time\n",
    "\n",
    "    full_time = time.time()\n",
    "\n",
    "    import uuid\n",
    "    from unsloth import FastVisionModel\n",
    "    from qwen_vl_utils import process_vision_info\n",
    "    from nebu import (\n",
    "        Bucket,\n",
    "        ContainerConfig,\n",
    "        Adapter,\n",
    "        Cache,\n",
    "        is_allowed,\n",
    "        oai_to_qwen,\n",
    "    )\n",
    "    import qwen_vl_utils\n",
    "    import requests\n",
    "    from PIL import Image\n",
    "    from io import BytesIO\n",
    "\n",
    "    base_model_id = \"unsloth/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "    global state\n",
    "\n",
    "    print(\"message\", message)\n",
    "    training_request = message.content\n",
    "    if not training_request:\n",
    "        raise ValueError(\"No training request provided\")\n",
    "\n",
    "    print(\"content\", message.content)\n",
    "\n",
    "    container_config = ContainerConfig.from_env()\n",
    "    print(\"container_config\", container_config)\n",
    "\n",
    "    content = message.content\n",
    "    if not content:\n",
    "        raise ValueError(\"No content provided\")\n",
    "\n",
    "    adapter_hot_start = time.time()\n",
    "    print(\"checking cache for adapter\", f\"'adapters:{content.model}'\")\n",
    "    val_raw = state.cache.get(f\"adapters:{content.model}\")\n",
    "    if val_raw:\n",
    "        print(\"val_raw\", val_raw)\n",
    "        val = Adapter.model_validate_json(val_raw)\n",
    "        print(\"found adapter in cache\", val)\n",
    "\n",
    "        if not is_allowed(val.owner, message.user_id, message.orgs):\n",
    "            raise ValueError(\"You are not allowed to use this adapter\")\n",
    "\n",
    "        if not val.base_model == base_model_id:\n",
    "            raise ValueError(\n",
    "                \"The base model of the adapter does not match the model you are trying to use\"\n",
    "            )\n",
    "\n",
    "        loaded = False\n",
    "        for adapter in state.adapters:\n",
    "            print(\"cached adapter: \", adapter)\n",
    "            if val.name == content.model and val.created_at == adapter.created_at:\n",
    "                loaded = True\n",
    "                print(\"adapter already loaded\", content.model)\n",
    "                break\n",
    "        print(f\"Adapter hot start: {time.time() - adapter_hot_start} seconds\")\n",
    "\n",
    "        if not loaded:\n",
    "            bucket = Bucket()\n",
    "            print(\"copying adapter\", val.uri, f\"./adapters/{content.model}\")\n",
    "\n",
    "            time_start = time.time()\n",
    "            bucket.copy(val.uri, f\"./adapters/{content.model}\")\n",
    "            print(f\"Copied in {time.time() - time_start} seconds\")\n",
    "\n",
    "            print(\"loading adapter\", content.model)\n",
    "            state.base_model.load_adapter(\n",
    "                f\"./adapters/{content.model}\", adapter_name=content.model\n",
    "            )\n",
    "            state.adapters.append(val)  # type: ignore\n",
    "            print(\"loaded adapter\", content.model)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Adapter '{content.model}' not found\")\n",
    "    print(\"adapter total start time \", time.time() - adapter_hot_start)\n",
    "\n",
    "    loaded_adapter_names = list(state.base_model.peft_config.keys())\n",
    "    print(loaded_adapter_names)\n",
    "\n",
    "    print(\"setting adapter\", content.model)\n",
    "    state.base_model.set_adapter(content.model)\n",
    "\n",
    "    conent_dict = content.model_dump()\n",
    "    messages_oai = conent_dict[\"messages\"]\n",
    "    messages = oai_to_qwen(messages_oai)\n",
    "\n",
    "    # === Simplified Debugging Block Start ===\n",
    "    # Make sure qwen_vl_utils is available after setup script runs\n",
    "    # try:\n",
    "    #     from qwen_vl_utils import process_vision_info\n",
    "    # except ImportError:\n",
    "    #     print(\"ERROR: Failed to import qwen_vl_utils. Check setup_script.\")\n",
    "    #     raise\n",
    "\n",
    "    # print(\"\\\\n--- DEBUGGING IMAGE FETCH START ---\")\n",
    "    # # Iterate through the *converted* messages to find the image URL\n",
    "    # debug_url = None\n",
    "    # for msg in messages:  # Use the Qwen-formatted messages\n",
    "    #     if isinstance(msg.get(\"content\"), list):\n",
    "    #         for item in msg[\"content\"]:\n",
    "    #             # Check for the 'image' key which should contain the URL after oai_to_qwen\n",
    "    #             if item.get(\"type\") == \"image\" and isinstance(item.get(\"image\"), str):\n",
    "    #                 src = item[\"image\"]\n",
    "    #                 if src.startswith(\"http://\") or src.startswith(\"https://\"):\n",
    "    #                     debug_url = src\n",
    "    #                     print(f\"Found image URL for debugging: {debug_url}\")\n",
    "    #                     break  # Found first URL, use it\n",
    "    #         if debug_url:\n",
    "    #             break\n",
    "\n",
    "    # if debug_url:\n",
    "    #     print(f\"Attempting to fetch and check URL: {debug_url}\")\n",
    "    #     try:\n",
    "    #         response = requests.get(\n",
    "    #             debug_url, timeout=30, stream=True\n",
    "    #         )  # Use stream=True initially\n",
    "    #         print(f\"  Status Code: {response.status_code}\")\n",
    "    #         print(f\"  Content-Type Header: {response.headers.get('Content-Type')}\")\n",
    "    #         print(f\"  Content Length: {response.headers.get('Content-Length')}\")\n",
    "\n",
    "    #         # Read content carefully now\n",
    "    #         image_content = response.content  # Reads full content into memory\n",
    "\n",
    "    #         print(f\"  Content (first 100 bytes): {image_content[:100]}\")\n",
    "    #         response.raise_for_status()  # Check for HTTP errors after getting content\n",
    "\n",
    "    #         print(\"  Attempting to open with PIL...\")\n",
    "    #         try:\n",
    "    #             with BytesIO(image_content) as bio:\n",
    "    #                 img = Image.open(bio)\n",
    "    #                 img.load()  # Attempt to load image data\n",
    "    #                 print(\n",
    "    #                     f\"  PIL Success: Format={img.format}, Size={img.size}, Mode={img.mode}\"\n",
    "    #                 )\n",
    "    #         except Exception as pil_err:\n",
    "    #             print(f\"  PIL Error: Could not identify image file.\")\n",
    "    #             print(f\"  Specific PIL Error: {pil_err}\")\n",
    "    #             # traceback.print_exc() # Uncomment for full PIL traceback if needed\n",
    "\n",
    "    #     except requests.exceptions.RequestException as req_err:\n",
    "    #         print(f\"  Requests Error: {req_err}\")\n",
    "    #         # traceback.print_exc() # Uncomment for full requests traceback\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"  Unexpected Error during request/check: {e}\")\n",
    "    #         # traceback.print_exc() # Uncomment for full general traceback\n",
    "    # else:\n",
    "    #     print(\"No HTTP/HTTPS image URL found in Qwen-formatted messages for debugging.\")\n",
    "    # print(\"--- DEBUGGING IMAGE FETCH END ---\\\\n\")\n",
    "    # === Simplified Debugging Block End ===\n",
    "\n",
    "    # Preparation for inference\n",
    "    print(\"preparing inputs using messages: \", messages)\n",
    "    inputs_start = time.time()\n",
    "    text = state.model_processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    print(\"text: \", text)\n",
    "    print(\"processing vision info: \", messages)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = state.model_processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    print(\"inputs\", inputs)\n",
    "    print(f\"Inputs prepared in {time.time() - inputs_start} seconds\")\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = state.base_model.generate(\n",
    "        **inputs, max_new_tokens=content.max_tokens\n",
    "    )\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :]\n",
    "        for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    generation_start = time.time()\n",
    "    output_text = state.model_processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "    print(\"output_text\", output_text)\n",
    "    print(f\"Generation took {time.time() - generation_start} seconds\")\n",
    "\n",
    "    # Build the Pydantic model, referencing your enumerations and classes\n",
    "    response = ChatCompletionResponse(\n",
    "        id=str(uuid.uuid4()),\n",
    "        created=int(time.time()),\n",
    "        model=content.model,\n",
    "        object=\"chat.completion\",\n",
    "        choices=[\n",
    "            ChatCompletionChoice(\n",
    "                index=0,\n",
    "                finish_reason=\"stop\",  # or another appropriate reason\n",
    "                message=ChatCompletionResponseMessage(\n",
    "                    role=\"assistant\", content=output_text[0]\n",
    "                ),\n",
    "                # Stub logprobs; in reality, you'd fill these from your model if you have them\n",
    "                logprobs=Logprobs(content=[]),\n",
    "            )\n",
    "        ],\n",
    "        service_tier=None,\n",
    "        system_fingerprint=None,\n",
    "        usage=None,\n",
    "    )\n",
    "    print(f\"Total time: {time.time() - full_time} seconds\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebu.chatx.openai import ChatCompletionRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = ChatCompletionRequest(\n",
    "    model=\"clinton1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Who is this an image of?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://storage.googleapis.com/orign/testdata/nebu/blinken.jpg\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=500,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'StreamResponseMessage',\n",
       " 'id': '1744503553924-0',\n",
       " 'content': {'id': '4b39f92e-135d-4bdf-96c4-0915e829f868',\n",
       "  'choices': [{'finish_reason': 'stop',\n",
       "    'index': 0,\n",
       "    'message': {'content': 'This is an image of Abraham Lincoln, the 16th President of the United States. He is known for his leadership during the American Civil War and his role in the abolition of slavery. The photograph appears to be from the mid-19th century, which aligns with the time period when he served as president. The formal attire and the style of the portrait are characteristic of the era.',\n",
       "     'refusal': None,\n",
       "     'tool_calls': None,\n",
       "     'annotations': None,\n",
       "     'role': 'assistant',\n",
       "     'function_call': None,\n",
       "     'audio': None},\n",
       "    'logprobs': {'content': []}}],\n",
       "  'created': 1744503702,\n",
       "  'model': 'clinton1',\n",
       "  'service_tier': None,\n",
       "  'system_fingerprint': None,\n",
       "  'object': 'chat.completion',\n",
       "  'usage': None},\n",
       " 'status': 'success',\n",
       " 'created_at': '2025-04-13T00:21:42.198141',\n",
       " 'user_id': 'patrick.barker@kentauros.ai'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer_qwen_vl.send(req.model_dump(), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_qwen_vl.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
