{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3419f305",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization\n",
    "\n",
    "In this tutorial we will use autoregressive approaches for virtual sensors to calculate the system response of an ESP System with external force from a shaker. This Tutorial introduces the concept of Hyperparameter optimization to find optimal parameters for our model. Please make sure to go through the basic tutorials `linear_models.ipynb`, '`ARNN.ipynb` and `RNN.ipynb`. We will start by loading the data in the same way as before.\n",
    "\n",
    "The optimisation of hyperparameters defines a key point in the creation of deep learning models. The exact choice of hyperparameters is usually model and problem dependent, making hyperparameter optimisation necessary for most use cases.\n",
    "\n",
    "Hyperparameters are all adjustable model and training variables that are not adjusted by a gradient-based method. In deep learning research, there is a great deal of work on optimisation. This tutorial focuses on grid search and model-based sequential optimisation.\n",
    "\n",
    "## Loading Data\n",
    "In a first step we load the already measured data and convert them into `pandas.DataFrames`. Our example Data consists of three individually Measured datasets. To load the data properly we need information about the specific sensors. In our case we define sensors: \n",
    " * 12 input sensors\n",
    " * 3 output sensors\n",
    " \n",
    "Furthermore, (if not given as `time` in the mat file) we need the corrosponsing sampling rate `fs` of the data.In our case:\n",
    " * fs=50000\n",
    "\n",
    "Fixing the sampling rate will raise an Attention Message. We save the individual Measurments in a list of `pandas.DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb5cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "data_path = os.path.join(os.path.abspath(\"\"), 'data')\n",
    "\n",
    "input_sensors = [f'input_{i}' for i in range(1, 10)]\n",
    "output_sensors = [f'output_{i}' for i in range(1, 4)]\n",
    "\n",
    "file_names = [f for f in os.listdir(data_path) if\n",
    "                    os.path.isfile(os.path.join(data_path, f))]\n",
    "df_list = [pd.read_parquet(os.path.join(data_path, n)) for n in file_names]\n",
    "fs = 1/np.mean(df_list[0].index.diff().dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194770b",
   "metadata": {},
   "source": [
    "## Plot the Data\n",
    "Before we start with the actual preprocessing we want to plot the given data to get a feeling for the properties. These are high-frequency acceleration data recorded on a shaker. For convinience we just plot the first entry of our list of DataFrames\n",
    "<img src=\"./img/Mehrachspruefstand_hitl.jpg\" alt=\"\" width=\"600\"/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0].plot(subplots=True, sharex=True, figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8511a463",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "When looking at the data, the high range in which the accelerations move is striking. For many machine learning models, preprocessing is therefore necessary. Our toolbox uses the class `Meas_handling` for the entire preprocessing. `Meas_handling` is used to transform the data into a normally distributed range. Furthermore, the entire frequency range is not relevant for our analysis.\n",
    "\n",
    "Furtehrmore we split the data in training and evaluation data. We use the first two Measurments as training and the last one as evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53591abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from softsensor.meas_handling import Meas_handling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_handle = Meas_handling(df_list[:2], file_names[:2], input_sensors, output_sensors, fs,\n",
    "                            df_list[2:], file_names[2:])\n",
    "\n",
    "data_handle.Resample(fs=10000)\n",
    "freq_lim = (90, 4000)\n",
    "data_handle.Filter(freq_lim)\n",
    "data_handle.Scale(StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191eb6a3",
   "metadata": {},
   "source": [
    "The optimisation of hyperparameters is a complex problem that requires a high computational capacity. It therefore makes sense to optimise on a GPU. Therefore, the first step is to check whether a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Training on', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1696c003",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "For an autoregressive neural network, the hyperparameters are composed of model parameters and training parameters.  We start our consideration with a grid search. This is an extensive procedure that considers each parameter combination in a grid individually. With an increasing number of parameters, the number of models to be trained increases exponentially. We therefore focus on the parameters `learning rate` and `stabilizer` as two training parameters. If possible, it makes sense to perform the optimisation either on a gpu or a remote computer.\n",
    "\n",
    "To optimise the hyperparameters, the softsensor toolbox contains the `Hyperparameter_optimisation` tools which include the grid search. For execution, all static parameters and the parameter grid must be defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10cddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.hyperparameter_optimization import grid_search\n",
    "import torch.nn as nn\n",
    "\n",
    "static_params = {'input_channels': 9,\n",
    "                'pred_size': 3,\n",
    "                'window_size': 50,\n",
    "                'rnn_window': 50,\n",
    "                'max_epochs': 2, # 30,\n",
    "                'patience': 3,\n",
    "                'hidden_size': [128, 64, 32, 16],\n",
    "                'optimizer': 'Adam',\n",
    "                'stab_method': 'const'\n",
    "                }\n",
    "\n",
    "grid = {'s1': [5e-4, 1e-3],#, 1.5e-3, 2e-3],\n",
    "        'lr': [1e-3, 5e-3, 1e-2]#, 1.5e-2, 2e-2]\n",
    "       }\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model_type = 'ARNN'\n",
    "result_df, best_model = grid_search(data_handle, criterion, model_type, static_params, grid, val_prediction=True, device=device, key='short', print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf37032",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We now visualise the loss over the individual hyperparameters. For 2 hyperparameters it makes sense to display them as even in the 3 dimensional space. For more parameters, other techniques such as scatter plots must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665db3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8,8))\n",
    "X = result_df['lr']\n",
    "stab_list = list(result_df['stabelizer'])\n",
    "Y = [dic['eta'][0] for dic in stab_list]\n",
    "Z = result_df['loss']\n",
    "surf = ax.plot_trisurf(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "ax.set_xlabel('lr')\n",
    "ax.set_ylabel('stabelizer')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fb6248a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15c76c54",
   "metadata": {},
   "source": [
    "## Sequential model-based optimization\n",
    "\n",
    "In contrast to grid search, model-based optimisation does not specify a parameter grid in advance, but only limits. The optimisation works with the so-called Tree of Parzan Estimator which samples from a space of hyperparameters. The aim is to proceed sequentially towards the optimal hyperparameters. In each case, a combination of hyperparameters is selected and then a substitute model is used to determine which combination is likely to produce the best results. Mathematically, the optimisation works with the so-called Tree of Parzan Estimator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3904d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.hyperparameter_optimization import hyperopt_search\n",
    "from hyperopt import hp\n",
    "\n",
    "static_params = {'input_channels': 9,\n",
    "                'pred_size': 3,\n",
    "                'window_size': 50,\n",
    "                'rnn_window': 50,\n",
    "                'max_epochs': 2,# 30,\n",
    "                'patience': 3,\n",
    "                'hidden_size': [128, 64, 32, 16],\n",
    "                'optimizer': 'Adam',\n",
    "                'stab_method': 'const'\n",
    "                }\n",
    "\n",
    "grid_params = {'lr': hp.uniform('lr', 1e-4, 1e-2),\n",
    "              's1': hp.uniform('s1', 1e-3, 1e-1)\n",
    "               }\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model_type = 'ARNN'\n",
    "result_df, best_model = hyperopt_search(data_handle, criterion, model_type,\n",
    "                                        static_params, grid_params, max_iterations=4,\n",
    "                                        val_prediction=True, device=device, key='short', \n",
    "                                        print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81583cf9",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "We now visualise the loss over the individual hyperparameters. For 2 hyperparameters it makes sense to display them as even in the 3 dimensional space. For more parameters, other techniques such as scatter plots must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0805b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8,8))\n",
    "X = result_df['lr']\n",
    "stab_list = list(result_df['stabelizer'])\n",
    "Y = [dic['eta'][0] for dic in stab_list]\n",
    "Z = result_df['loss']\n",
    "surf = ax.plot_trisurf(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "ax.set_xlabel('lr')\n",
    "ax.set_ylabel('stabelizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b8f54",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "To Evaluate our model on testing data we use the predefined functions `comp_pred`, which computes the prediction for a defined track in the data_handle class. As track we choose our testing track, which can be accessed using the internal variable `test_names`. Furthermore we compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff65b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.eval_tools import comp_pred, comp_error\n",
    "from softsensor.linear_methods import tf\n",
    "\n",
    "tf_class = tf(window_size=1024,hop=512, fs=10000)\n",
    "tf_class.fit(data_handle.train_df, input_sensors, output_sensors)\n",
    "\n",
    "track = data_handle.test_names[0]\n",
    "models = [best_model, tf_class]\n",
    "pred_df = comp_pred(models, data_handle, track, names=['Neural Network', 'TF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.filter(regex='a_PCB_P10_A1S_Z').plot(figsize=(12,4))\n",
    "pred_df.filter(regex='a_PCB_P10_A1S_Z')[1:1.02].plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad34002",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = comp_error(pred_df, output_sensors, fs, names=['Neural Network', 'TF'], metrics=['MSE', 'MAE'], freq_range=freq_lim)\n",
    "for n in ['MSE', 'MAE']:\n",
    "    error.filter(regex=n, axis=0).plot.bar(ylabel=f'{n}', rot=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
