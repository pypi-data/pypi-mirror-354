{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edee6542",
   "metadata": {},
   "source": [
    "# Hybrid Virtual Sensors\n",
    "\n",
    "In this tutorial we design a hybrid approach using the linear transfer function in frequency domain in combination with autoregressive Neural Networks. This is the fifth tutorial on the SoftSensor Toolbox building upon the other four. Please check out the previous tutorial for more detailed informations concerning the specific models as well as the hyperparameter optimization. Furthermore we utilise advanced techniques for stabelization in the predicition. \n",
    "\n",
    "Hybrid models are extremely useful for systems that have a *strong linear component with slight non-linear deviations*. For these cases, hybrid models can offer a strong benefit in terms of accuracy and computational effort compared to pure deep learning or classical models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b3773",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "The data loading process works similarly to the previous tutorials. A steering system under road conditions is used as the data basis. The linear antecedent of the system is significant, which is why the use of hybrid approaches is appropriate.\n",
    "\n",
    "These are matlab files which require the use of a special function to read the data. The internal function `read_vehicle_data` provides the functionality to read the matfile for the individual sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1809be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ba7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.path.abspath(\"\"), 'data')\n",
    "\n",
    "input_sensors = [f'input_{i}' for i in range(1, 10)]\n",
    "output_sensors = [f'output_{i}' for i in range(1, 4)]\n",
    "\n",
    "file_names = [f for f in os.listdir(data_path) if\n",
    "                    os.path.isfile(os.path.join(data_path, f))]\n",
    "df_list = [pd.read_parquet(os.path.join(data_path, n)) for n in file_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba90417",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "To enable hybrid models, preprocessing is carried out in two steps. In the first step, the data is read into a `Meas_handling` class, filtered and scaled as explained in the previous tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.meas_handling import Meas_handling\n",
    "fs = 1/np.mean(df_list[0].index.diff().dropna())\n",
    "data_handle = Meas_handling(df_list[:2], file_names[:2], input_sensors, output_sensors, fs, df_list[2:], file_names[2:])\n",
    "\n",
    "freq_lim = [12, 1250]\n",
    "data_handle.Resample(fs=4096)\n",
    "data_handle.Filter(freq_lim)\n",
    "data_handle.Scale()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db198c51",
   "metadata": {},
   "source": [
    "In the second step, the linear solution of the system is calculated. If another solution is known, for example from simulations, this can of course also be used. To calculate the linear solution, the Frequency Response Function (FRF) with the class `tf` is used. A detailed introduction to the calculation of linear solutions is described in `tutorial/01_linear_models.ipynb`. \n",
    "\n",
    "Then the difference between the linear solution and the original measurement data must be calculated. The use of the difference is necessary because the subsequent training with stability parameters leads to a simpler training with improved properties. \n",
    "\n",
    "As a last step, the `Meas_handling` class must be adapted so that the linear solution is also used as input in addition to the actual input sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.linear_methods import tf\n",
    "from softsensor.eval_tools import comp_batch\n",
    "import numpy as np\n",
    "\n",
    "# compute linear solution\n",
    "tf_class = tf(window_size=1024, hop=512, fs=data_handle.fs)\n",
    "tf_class.fit(data_handle.train_df, input_sensors, output_sensors)\n",
    "_ = comp_batch([tf_class], data_handle, data_handle.train_names + data_handle.test_names, ['lin'])\n",
    "\n",
    "# compute difference between linear solution and measured output\n",
    "lin_labels = [f'{s}_lin' for s in output_sensors]\n",
    "for df in data_handle.train_df + data_handle.test_df:\n",
    "    for s, l in zip(output_sensors, lin_labels):\n",
    "        df[f'{s}_diff'] = np.array(df[s]) - np.array(df[l])\n",
    "\n",
    "# redefine input sensors for subsequent models\n",
    "data_handle.input_sensors = input_sensors + lin_labels\n",
    "data_handle.output_sensors = [f'{s}_diff' for s in output_sensors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcb7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handle.train_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ae292",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "We define an Autoregressive Neural Network (ARNN) to approximate the functional dependency between input and output as a simple Neural Network. The model is autoregressive as it feeds back in the past outputs into the equation, leading to:\n",
    "\n",
    "$y_i(t+1) = f(\\mathbf{x}_i(t, ..., t-w_x), \\mathbf{y}_i(t, ..., t-w_y))$\n",
    "\n",
    "Formally this model is called NARX (nonlinear autoregressive model with exogenious input) and takes:\n",
    "\n",
    " * number of input channels (`input_channels`)\n",
    " * number of output channels (`pred_size`)\n",
    " * window size (`window_size`)\n",
    " * recurretn window size (`rnn_window`)\n",
    " * neurons in the hidden layers (`hidden_size`)\n",
    " \n",
    "as input. We need to define an extendet input space as `len(input_sensors + output_sensors)` to include the additional input from the linear solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116ef0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.autoreg_models import ARNN\n",
    "\n",
    "ARNN = ARNN(input_channels=len(input_sensors + output_sensors), pred_size=len(output_sensors), window_size=50, rnn_window=50,\n",
    "            hidden_size=[128, 64, 32, 16], activation='leaky_relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5032dd6",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "After we have defined the model, the next step is to adapt the model to the data. The training and validation data can be extracted from the predefined `data_handle` class and then trained using the `train_model` function. The setup for the training works similar to normal non-hybrid methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58260ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "max_epochs = 30 # Max Epochs to train the Model\n",
    "lr = 1e-4 # np.logspace(np.log10(1e-5), np.log10(1e-1), max_epochs, endpoint=True)\n",
    "rel_perm = 5e-4 # gaussian noise added to signals\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ARNN.parameters(), lr=lr)\n",
    "\n",
    "train_loader, val_loader = data_handle.give_torch_loader(window_size=50 , keyword='training', batch_size=256,\n",
    "                                                         rnn_window=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93133ef",
   "metadata": {},
   "source": [
    "The training process is done as in `tutorials/02_ARNN.ipynb`. However, we introduce a change and use a scheduler for the stability parameter. Since we know that a negative stability score (SC) means stable behaviour, we use a high parameter if SC is greater than zero and a low parameter if SC is less than zero. \n",
    "We therefore check the SC inbetween the training process and adjust the stabelizer during training. The scheduling results in a SC close to zero after the training process, thus favouring stable behaviour without interfering too much with the main criterion.\n",
    "\n",
    "We therefore use a log-function for a SC < 0 and a linear function for SC > 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.stab_scheduler import log_lin_stab\n",
    "stab = log_lin_stab(ARNN, s0=1e-7, s1=1e-2, m=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff155b59",
   "metadata": {},
   "source": [
    "We visualize the Stability Parameter to show the specific advantages over simpler methods without scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139cd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "stab.track = False\n",
    "sc_original = np.linspace(-1/np.sqrt(50), 5, 5000)\n",
    "eta_original = [stab.get_stab(ARNN, sc=sco) for sco in sc_original]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(8,4.5))\n",
    "ax[0].plot(sc_original, eta_original, color='grey', alpha=.5)\n",
    "ax[0].set_xlim(-.15, .1)\n",
    "ax[0].set_ylim(-.001, .03)\n",
    "ax[1].plot(sc_original, eta_original, color='grey', alpha=.5)\n",
    "ax[1].set_ylim(-.001, .03)\n",
    "ax[0].set_ylabel('stabelizer')\n",
    "ax[0].set_xlabel('SC')\n",
    "ax[1].set_xlabel('SC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1845a2",
   "metadata": {},
   "source": [
    "Afterwards we train our model similar to the previous tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.train_model import train_model\n",
    "results = train_model(ARNN, train_loader, max_epochs, optimizer,\n",
    "                        device='cpu', criterion=nn.MSELoss(), val_loader=val_loader,\n",
    "                        patience=5, print_results=True, stabelizer=stab,\n",
    "                        give_results=True, rel_perm=rel_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3a4fd",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "To Evaluate our model on testing data we use the predefined functions `comp_pred`, which computes the prediction for a defined track in the data_handle class. As track we choose our testing track, which can be accessed using the internal variable `test_names`. Furthermore we compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc0cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.eval_tools import comp_batch\n",
    "train_pred = comp_batch([ARNN], data_handle, data_handle.train_names, ['ARNN'])\n",
    "test_pred = comp_batch([ARNN], data_handle, data_handle.test_names, ['ARNN'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab75ae8e",
   "metadata": {},
   "source": [
    "Since we have only used one model for the hybrid modelling to calculate the difference between the linear and the measured solution, the linear solution must still be added in post-processing for the model and the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce506911",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(train_pred):\n",
    "    for out_s in output_sensors:\n",
    "        train_pred[i][f'{out_s}_ARNN'] = df[f'{out_s}_diff_ARNN'] + df[f'{out_s}_lin']\n",
    "\n",
    "for i, df in enumerate(test_pred):\n",
    "    for out_s in output_sensors:\n",
    "        test_pred[i][f'{out_s}_ARNN'] = df[f'{out_s}_diff_ARNN'] + df[f'{out_s}_lin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35107726",
   "metadata": {},
   "source": [
    "Afterwards we visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"output_1\", \"output_1_lin\", \"output_1_ARNN\"]\n",
    "test_pred[0][cols].plot(figsize=(12,4))\n",
    "test_pred[0][cols][3:3.02].plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b3018",
   "metadata": {},
   "source": [
    "Lastly we compute a comparison for as a Mean-Squared-Error in Frequency domain (MSLE) as well as Mean Square Error(MSE)  in time domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f846db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.eval_tools import comp_error\n",
    "\n",
    "error = comp_error(test_pred[0], output_sensors, fs, names=['ARNN', 'lin'], metrics=['MSE', 'MAE', 'JSD'], freq_range=freq_lim)\n",
    "for n in ['MSE', 'MAE', 'JSD']:\n",
    "    error.filter(regex=n, axis=0).plot.bar(ylabel=f'{n}', rot=45)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
