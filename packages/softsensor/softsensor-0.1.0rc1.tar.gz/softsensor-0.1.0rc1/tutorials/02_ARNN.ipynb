{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoregressive Neural Networks\n",
    "\n",
    "In this tutorial we will use autoregressive approaches for virtual sensors to calculate the system response of a nonlinear System with external forcing. The tutorial provides an introduction to virtual sensors for dynamic systems. This tutorial defines a starting point introducing the main concepts of autoregressive neural networks and the usage inside the softsensor package. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "As part of Data Science, Deep Learning methods learn from an existing data set. The goal is to learn the principal dynamics behind the duffing oscillation. For the first use case, data is therefore generated from the differential equation using numerical integration. \n",
    "<br>\n",
    "The Duffing equation is a nonlinear second-order differential equation used to model certain damped and driven oscillators. It is expressed as:\n",
    "\n",
    "$$\n",
    "\\ddot{x} + D\\dot{x} + x + c_{\\text{nlin}}x^3 = F(t)\n",
    "$$\n",
    "Where:\n",
    "- $x$ is the displacement,\n",
    "- $D$ is the damping coefficient,\n",
    "- $c_{\\text{nlin}}$ is the nonlinear stiffness coefficient,\n",
    "- $F(t)$ is the external forcing function.\n",
    "\n",
    "<br>\n",
    "A data set must be generated for the training process as well as another one for the later evaluation. In the present case, exchange signals are used as excitation in both the training and the evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.data_gen import white_noise, get_academic_data\n",
    "import numpy as np\n",
    "# Duffing Parameters\n",
    "params = {'D': 0.05,\n",
    "          'c_nlin': 0.1}\n",
    "\n",
    "model = 'Duffing'\n",
    "\n",
    "fs = 10 # sampling rate\n",
    "end_t = 100 # time period of the training data time series\n",
    "n_ts = 50 # number of time series\n",
    "\n",
    "\n",
    "# generate training data\n",
    "time = np.linspace(0, end_t, end_t*fs+1)\n",
    "F = [white_noise(time) for _ in range(n_ts)]\n",
    "\n",
    "training_data = []\n",
    "for f in F:\n",
    "    temp_df = get_academic_data(time, model, params, f, x0=[0, 0], rtol=1e-10)\n",
    "    training_data.append(temp_df)\n",
    "\n",
    "train_names = [f'White Noise {f}' for f in range(n_ts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Data\n",
    "Before we start with the actual preprocessing we want to plot the given data to get a feeling for the properties. \n",
    "For convenience, we just plot the first entry of our list of DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0].plot(subplots=True, sharex=True, figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "For many machine learning models, preprocessing is necessary. Our toolbox uses the class `Meas_handling` for the entire preprocessing. `Meas_handling` can used to transform or scale the data into a normally distributed range. Furthermore, the entire frequency range is not relevant for our analysis. Therefore, we filter between 0,2 Hz to 4 Hz.\n",
    "\n",
    "Furthermore, we split the data in training and evaluation data. We use the first two Measurments as training and the last one as evaluation.\n",
    "\n",
    "Lastly we need to define what will be the inputs and output of our model. For the Duffing oscillation we aim at predicting the response $x(t)$ given the external excitaion $F(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.meas_handling import Meas_handling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "input = ['F(t)']\n",
    "output = ['x']\n",
    "data_handle = Meas_handling(training_data[:40], train_names[:40], input, output, fs,\n",
    "                            training_data[40:], train_names[40:])\n",
    "\n",
    "freq_lim = (.2, 4)\n",
    "data_handle.Filter(freq_lim)\n",
    "data_handle.Scale(StandardScaler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define linear Models\n",
    "We define two different benchmark Models in our example. \n",
    "First we define an Autoregressive Model with exogenious input (ARX) to approximate the functional dependency between input and output as linear combinations. The model is autoregressive as it feeds back in the past outputs into the equation, leading to:\n",
    "\n",
    "$y_i(t+1) = \\mathbf{a} \\cdot \\mathbf{x}_i(t, ..., t-o_x) + \\mathbf{b} \\cdot \\mathbf{y}_i(t, ..., t-o_y))$\n",
    "\n",
    "where $\\mathbf{a}$ and $\\mathbf{b}$ define the different weights for each input as linear combinations, t the time step and $o_x, o_y$ the number of past time steps to include in the computation.\n",
    "\n",
    "We define our ARX model with order = [10, 10] leading to the past 10 time steps of input and output to be included in the computation. The `ARX.fit` function takes a list of `pandas.DataFrames` as well as the input aud output sensor names as input. We can access the `pandas.DataFrames` in our `data_handle` by simply using the internal variable `train_df` which returns a list of DataFrames for training. (`test_df` would return a list of DataFrames for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.arx import ARX\n",
    "arx = ARX(order=[2, 2])\n",
    "arx.fit(data_handle.train_df, input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Congratulation!!!}$ You now have developed your first virtual sensor for acceleration Measurements\n",
    "\n",
    "The ARX is a purely time domain model and is used in a multitude of use cases with different signal types. Since our datasets are acceleration measurements, we define a second linear approach in the frequency domain, namely the linear Transfer function.\n",
    "\n",
    "Setting up the model is quite simple as is takes no input variables. The linear_TF fit function takes a list of `pandas.DataFrames` as well as the input aud output sensor names as input. Furthermore, we need to define a window size and sampling rate for the Fourier transformation conducted in out model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.linear_methods import tf\n",
    "tf_class = tf(window_size=128, hop=64, fs=10)\n",
    "tf_class.fit(data_handle.train_df, input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Congratulation!!!}$ You now have developted your second virtual sensor for acceleration Measurments\n",
    "\n",
    "## Model Evaluation\n",
    "To Evaluate our model on testing data we use the predefined functions `comp_pred`, which computes the prediction for a defined track in the data_handle class. As track we choose our testing track, which can be accessed using the internal variable `test_names`. Furthermore we compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.eval_tools import comp_pred\n",
    "track = data_handle.test_names[0]\n",
    "models = [arx, tf_class]\n",
    "pred_df = comp_pred(models, data_handle, track)\n",
    "pred_df.filter(regex='x').plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The evaluation of the ARX and linear Transfer Function models shows that both models deliver very poor performance in predicting the system response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ARNN Model\n",
    "We define an Autoregressive Neural Network (ARNN) to approximate the functional dependency between input and output as linear combinations. The model is autoregressive as it feeds back in the past outputs into the equation, leading to:\n",
    "\n",
    "$y_i(t+1) = f \\bigl( \\mathbf{x}_i(t, ..., t-w_x), \\mathbf{y}_i(t-1, ..., t-w_y-1) \\bigr)$\n",
    "\n",
    "Formally this model is called NARX (nonlinear autoregressive model with exogenious input) and takes:\n",
    " * number of input channels (`input_channels`)\n",
    " * number of output channels (`pred_size`)\n",
    " * window size $w_x$ (`window_size`)\n",
    " * recurrent window size $w_y$ (`rnn_window`)\n",
    " * neurons in the hidden layers (`hidden_size`)\n",
    " * activation function (`activation`)\n",
    " \n",
    "as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.autoreg_models import ARNN\n",
    "from torchinfo import summary\n",
    "\n",
    "ARNN = ARNN(input_channels=len(input), pred_size=len(output), window_size=10, rnn_window=10,\n",
    "            hidden_size=[32, 16], activation='leaky_relu')\n",
    "summary(ARNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have defined the model, the next step is to adapt the model to the data. The training and validation data can be extracted from the predefined `data_handle` class and then trained using the `train_model` function. The method needs as input.\n",
    "\n",
    "* `window_size`: defines the length of the time window, needs to be the same as for the model\n",
    "* `rnn_window`: defines the length of the recurrent time window, needs to be the same as for the model\n",
    "* `keyword`: either `training` or `short`, training uses the whole training dataset, while short uses only a small subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.train_model import train_model\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "train_loader, val_loader = data_handle.give_torch_loader(window_size=ARNN.window_size,\n",
    "                                                         rnn_window=ARNN.rnn_window, \n",
    "                                                         keyword='training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, some training parameters are needed, which have to be passed to the class `train_model`:\n",
    "\n",
    "* `model`: ARNN to be trained\n",
    "*  `train_loader`: training data\n",
    "* `max_epochs`: maximum number of epochs for training\n",
    "* `optimizer`: optimizer used  for optimization, check [Link](https://pytorch.org/docs/stable/optim.html) for different possible optimizers\n",
    "* `device`: device for training\n",
    "* `criterion`: loss function to train against check [Link](https://pytorch.org/docs/stable/nn.html#loss-functions) for possible criterions\n",
    "* `stabelizer`: stability parameter\\* that is important to get a stable prediction after training. For theory about stability criterion read: [Westmeier et al.](https://onlinelibrary.wiley.com/doi/10.1002/pamm.202200318)\n",
    "* `val_loader`: validation data\n",
    "* `patience`: number of epochs without improvement to stop training process\n",
    "* `print_results`: if True, results are printed\n",
    "\n",
    "\\* The stability parameter defines a relative weight between the actual loss function and a local weight decay. The corresponding stability score (SC) will be printed during training and, if below zero, ensures a stable prediction under all circumstances, however in a lot of cases a SC above zero might also be ok. For more on this read [Westmeier et al.](https://onlinelibrary.wiley.com/doi/10.1002/pamm.202200318)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(ARNN.parameters(), lr=1e-4)\n",
    "crit = nn.MSELoss()\n",
    "results = train_model(model=ARNN, train_loader=train_loader, max_epochs=30, optimizer=opt, device='cpu', criterion=crit,\n",
    "                     stabelizer=5e-3, val_loader=val_loader, patience=3, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Congratulation!!!}$ you have successfully fitted an autoregressive neural network to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track = data_handle.test_names[1]\n",
    "models = [ARNN, arx, tf_class]\n",
    "\n",
    "pred_df = comp_pred(models, data_handle, track, names=['ARNN', 'ARX', 'TF'])\n",
    "pred_df.filter(regex='x').plot(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softsensor.eval_tools import comp_error\n",
    "error = comp_error(pred_df, output, fs, names=['ARNN', 'ARX', 'TF'], metrics=['MSE'])\n",
    "error.filter(regex='MSE', axis=0).plot.bar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
