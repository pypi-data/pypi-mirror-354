import ast
from typing import Dict, List

__all__ = ("DagRenderMixin",)


def _task_id_to_better_name(task_id):
    return task_id.replace("-", "_").replace(" ", "_").replace(".", " ")


class DagRenderMixin:
    def render(self, debug_filename: str = "") -> str:
        """
        Render the DAG to a string representation, suitable for use in a .py file.
        """
        if not self.dag_id:
            raise ValueError("DAG must have a dag_id")
        imports: List[str] = []
        globals_: List[str] = []
        tasks: Dict[str, str] = {}
        task_dependencies: Dict[str, List[str]] = {}

        new_dag = ast.Module(body=[], type_ignores=[])

        for task_id, task in self.tasks.items():
            task_imports, task_globals, task_code = task.render(raw=True, dag_from_context=True)
            imports.extend(task_imports)
            globals_.extend(task_globals)

            # Ensure task_id is a valid Python identifier
            task_id = _task_id_to_better_name(task_id)

            if not task_id.isidentifier():
                raise ValueError(f"Task ID '{task_id}' is not a valid Python identifier")

            # Add task code to dict
            tasks[task_id] = task_code

            # Grab dependencies and add them
            if task.dependencies:
                task_dependencies[task_id] = [_task_id_to_better_name(dependency) for dependency in task.dependencies]

        # Imports
        imports.append(ast.ImportFrom(module="airflow.models", names=[ast.alias(name="DAG")], level=0))

        # remove duplicates
        seen = set()
        unique_imports = []

        for _ in imports:
            unparsed = ast.unparse(_)
            if unparsed not in seen:
                seen.add(unparsed)
                unique_imports.append(_)

        # sort
        imports = [x for _, x in sorted(zip([ast.unparse(_) for _ in unique_imports], unique_imports), key=lambda x: x[0])]
        new_dag.body.extend(imports)

        # Globals
        if globals_:
            globals_ = list(set(globals_))  # Remove duplicates
            new_dag.body.extend(globals_)

        # DAG definition
        dag_block = ast.With(
            items=[
                ast.withitem(
                    context_expr=ast.Call(func=ast.Name(id="DAG", ctx=ast.Load()), args=[ast.Constant(value=self.dag_id)], keywords=[]),
                    optional_vars=ast.Name(id="dag", ctx=ast.Store(), col_offset=0),
                )
            ],
            body=[],
        )

        # Tasks
        # dag_block.body.append(ast.Assign(targets=[ast.Name(id="dag", ctx=ast.Store())], value=dag_block.context_expr))
        for task_id, task in tasks.items():
            dag_block.body.append(
                ast.Assign(
                    targets=[ast.Name(id=task_id, ctx=ast.Store())],
                    value=task,
                )
            )

        # Handle task dependencies
        for task_id, dependencies in task_dependencies.items():
            for dependency in dependencies:
                # NOTE: this should have already been validated in the DAG,
                # but do again here for safety
                if dependency not in tasks:
                    raise ValueError(f"Task Dependency for {task_id} not found: {dependency}")

                dag_block.body.append(
                    ast.Expr(
                        value=ast.BinOp(
                            left=ast.Name(id=dependency),
                            op=ast.RShift(),
                            right=ast.Name(id=task_id),
                        )
                    )
                )

        # Append dag with statement to body
        new_dag.body.append(dag_block)

        # Fix missing locations
        ast.fix_missing_locations(new_dag)

        # Add comment about generation, assemble final dag via unparse
        ret = "# Generated by airflow-config\n" + ast.unparse(new_dag) + "\n"

        # Debug
        if debug_filename:
            with open(debug_filename, "w") as f:
                f.write(ret)
        return ret
