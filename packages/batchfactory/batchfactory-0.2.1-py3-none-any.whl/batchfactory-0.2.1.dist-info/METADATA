Metadata-Version: 2.4
Name: batchfactory
Version: 0.2.1
Summary: Composable, cache-aware batch processing pipelines for LLMs, APIs, and dataset generation.
Project-URL: Homepage, https://github.com/fangzhangmnm/batchfactory
Project-URL: Issues, https://github.com/fangzhangmnm/batchfactory/issues
Author-email: fangzhangmnm <1561797062@qq.com>
License-Expression: MIT
License-File: LICENSE
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.9
Requires-Dist: aiofiles
Requires-Dist: aiolimiter
Requires-Dist: jsonlines
Requires-Dist: openai
Requires-Dist: pydantic
Requires-Dist: tqdm
Description-Content-Type: text/markdown

# BatchFactory

Composable, cacheâ€‘aware pipelines for **parallel LLM workflows**, API calls, and dataset generation.

> **Status â€” `v0.2` alpha.**  Stable enough for prototypes; expect fastâ€‘moving APIs.

---

## Install

```bash
pip install batchfactory            # latest tag
pip install --upgrade batchfactory  # grab the newest patch
```

---

## Quickâ€‘start

```python
import batchfactory as bf
from batchfactory.op import *

project = bf.CacheFolder("quickstart", 1, 0, 0)
broker  = bf.brokers.ConcurrentLLMCallBroker(project["cache/llm_broker.jsonl"])

# Rewrite the first three passages of every *.txt file into fourâ€‘line poems.

g = (
    ReadMarkdownLines("./data/*.txt", key_field="keyword", directory_str_field="directory")
    | Shuffle(42)
    | TakeFirstN(3)
    | GenerateLLMRequest(
        'Rewrite the passage from "{directory}" titled "{keyword}" as a fourâ€‘line poem.',
        model="gpt-4o-mini@openai",
    )
    | ConcurrentLLMCall(project["cache/llm_call.jsonl"], broker)
    | ExtractResponseText()
    | WriteJsonl(project["out/poems.jsonl"], output_fields=["keyword", "text", "directory"])
    | Print()
)

g.compile().execute(dispatch_brokers=True)
```

Run it twice â€“ everything after the first run is served from the onâ€‘disk ledger.

---

## Why BatchFactory?Â Â **Three killer moves**

## Why BatchFactory?  **Three killer moves**

| ğŸ­ Mass data distillation & cleanup | ğŸ­ Multi-agent, multi-round workflows | ğŸŒ² Hierarchical spawning (`ListParallel`) |
|---|---|---|
| Chain `GenerateLLMRequest â†’ ConcurrentLLMCall â†’ ExtractResponseText` behind keyword or file sources to **bulk-create, filter, or refine datasets** (think millions of Q&A rows, code explanations, translation pairs) with caching and cost tracking built-in. | `Repeat` plus chat helpers let you spin up translation swarms, code-review pairs, or tutoring agents in **5 minutes of code** â€“ conversations live in `chat_history`, cost and revisions are automatic. | `ListParallel` explodes complex items into fine-grained subtasks, runs them **in parallel**, then auto-collects results â€“ perfect for beat â†’ scene â†’ arc analysis or any long, messy document pipeline. |


---

### Loop snippet (Roleâ€‘Playing)

```python
Teacher = Character("teacher_name", TEACHER_PROMPT)
Student = Character("student_name", STUDENT_PROMPT)

g = ( ReadMarkdownLines("story.txt", "keyword")
      | SetField({"teacher_name":"Alice", "student_name":"Bob"})
      | Teacher("è€å¸ˆï¼Œè¯·å…ˆè®²è§£è¯¾æ–‡", 0)
      | Repeat( Student("åŒå­¦æé—®æˆ–å›ç­”", 1)
                | Teacher("å›åº”æˆ–ç»§ç»­è®²è§£", 2), 3)
      | Teacher("è¯·æ€»ç»“", 3)
      | ChatHistoryToText() | Print() )
```

### Spawn snippet (chapter â†’ paragraph â†’ synopsis with `ListParallel`)
```python
project = bf.CacheFolder("spawn_demo", 1, 0, 0)
broker  = bf.brokers.ConcurrentLLMCallBroker(project["cache/llm.jsonl"])

def ParaSummary():
    s = GenerateLLMRequest("Summarise:\n{paragraph}", model="gpt-4o-mini@openai")
    s |= ConcurrentLLMCall(project["cache/ps.jsonl"], broker)
    s |= ExtractResponseText()
    return s

g = ( ReadMarkdownLines("novel/*.md", "chapter")         # each entry = a chapter
      | ListParallel(ParaSummary(),                      # spawn per paragraph
                     list_field="paragraphs",
                     item_field="paragraph",
                     collect_field="para_summaries")
      | GenerateLLMRequest("Chapter synopsis:\n{para_summaries}",
                           model="gpt-4o-mini@openai")
      | ConcurrentLLMCall(project["cache/ch_sum.jsonl"], broker) )
```

---

## Core concepts (oneâ€‘liner view)

| Term             | Story in one sentence                                                                                                                               |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Entry**        | Tiny record with immutable `idx`, mutable `data`, autoâ€‘incrementing `rev`.                                                                          |
| **Op**           | Atomic node; compose with `|`or explicit`wire()`.                                                                                                   |
| **GraphSegment** | A lightweight helper: `.to_segment()` just wraps a node so `|`and`wire()` work on itâ€”each actual op still appears exactly once.                     |
| **execute()**    | Highâ€‘level driver that *resumes*, *pumps*, and *dispatches* brokers.                                                                                |
| **Broker**       | Pluggable engine handling expensive / async jobs (LLM, search, human labels).                                                                       |
| **Ledger**       | Appendâ€‘only JSONL cache behind every broker & graph enabling instant restart.                                                                       |

*(You *can* call `pump()` manually, but 99Â % of users stick to `execute()`.)*

---

## Primitive index (short list)

| Family              | Node                                                           | Blurb                             |
| ------------------- | -------------------------------------------------------------- | --------------------------------- |
| **Sources**         | `ReadMarkdownLines`, `FromList`                                | ingest files or raw dicts         |
| **Transforms**      | `Apply`, `Filter`, `SetField`                                  | pythonâ€‘powered field tweaks       |
| **SpawnÂ /Â Collect** | `SpawnFromList`, `CollectAllToList`                            | mapâ€‘reduce with unique child ids  |
| **Control flow**    | `If`, `While`, `Repeat`                                        | branch, loop, iterate             |
| **LLM**             | `GenerateLLMRequest â†’ ConcurrentLLMCall â†’ ExtractResponseText` | prompt, call, harvest             |
| **Utilities**       | `CleanupLLMData`, `PrintTotalCost`                             | tidy temporary fields, audit cost |
| **Parallel helper** | `ListParallel` | one-liner spawn â†’ subgraph â†’ collect |
| **Spawn / Collect (low-level)** | `SpawnFromList`, `CollectAllToList` | manual map-reduce with unique child ids |

*(Sharedâ€‘idx ops `Replicate` / `Collect` are deprecated and vanish inÂ v0.3.)*

---

## Example gallery

| âœ¨Â Example                       | Demonstrates                                          |
| ------------------------------- | ----------------------------------------------------- |
| **01Â â€“ Basic pipeline**         | linear LLM transform & caching                        |
| **02Â â€“ Roleâ€‘playing loop**      | concise multiâ€‘agent RPG using `Repeat` + chat helpers |
| **03Â â€“ SplitÂ & summarise**      | fanâ€‘out/fanâ€‘in summarisation (deprecated style)       |
| **04Â â€“ Longâ€‘text segmentation** | SpawnÂ + CollectAll power pattern                      |
| **05Â â€“ Math ops (unit)**        | loop + conditional logic under pure Python            |

---

## Broker & cache highlights

* Each expensive call is hashed â†’ `job_idx` â€” **duplicate prompts are free**.
* `BrokerFailureBehavior = RETRY | STAY | EMIT` lets you decide how failures propagate.
* On restart, `execute()` reuses cached results and sends *only* the missing jobs.

---

## Roadmap â†’ v0.3

* Enforce **unique `idx`** endâ€‘toâ€‘end â†’ new `JoinByParent` replaces deprecated sharedâ€‘idx ops.
* Builtâ€‘in vectorâ€‘store & semanticâ€‘search nodes.
* Streamlined cost & progress reporting.
* More batteriesâ€‘included tutorials.

---

Â©Â 2025 Â· MIT License
