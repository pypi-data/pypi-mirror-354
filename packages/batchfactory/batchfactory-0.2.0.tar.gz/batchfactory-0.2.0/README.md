# BatchFactory

Composable, cacheâ€‘aware pipelines for **parallel LLM workflows**, API calls, and dataset generation.

> **Status â€” `v0.2` alpha.**  Stable enough for prototypes; expect fastâ€‘moving APIs.

---

## Install

```bash
pip install batchfactory            # latest tag
pip install --upgrade batchfactory  # grab the newest patch
```

---

## Quickâ€‘start

```python
import batchfactory as bf
from batchfactory.op import *

project = bf.CacheFolder("quickstart", 1, 0, 0)
broker  = bf.brokers.ConcurrentLLMCallBroker(project["cache/llm_broker.jsonl"])

# Rewrite the first three passages of every *.txt file into fourâ€‘line poems.

g = (
    ReadMarkdownLines("./data/*.txt", key_field="keyword", directory_str_field="directory")
    | Shuffle(42)
    | TakeFirstN(3)
    | GenerateLLMRequest(
        'Rewrite the passage from "{directory}" titled "{keyword}" as a fourâ€‘line poem.',
        model="gpt-4o-mini@openai",
    )
    | ConcurrentLLMCall(project["cache/llm_call.jsonl"], broker)
    | ExtractResponseText()
    | WriteJsonl(project["out/poems.jsonl"], output_fields=["keyword", "text", "directory"])
    | Print()
)

g.compile().execute(dispatch_brokers=True)
```

Run it twice â€“ everything after the first run is served from the onâ€‘disk ledger.

---

## Why BatchFactory?Â Â **Three killer moves**

| ğŸ“¦Â Mass data distillation & cleanup                                                                                                                                                                                                                             | ğŸ­Â Multiâ€‘agent, multiâ€‘round workflows                                                                                                                                                                   | ğŸ”¥Â Hierarchical spawning for long text                                                                                                                                                                                   |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Chain `GenerateLLMRequest â†’ ConcurrentLLMCall â†’ ExtractResponseText` behind keyword or file sources to **bulkâ€‘create, filter, or refine datasets** (think millions of Q\&A rows, code explanations, translation pairs) with caching and cost tracking builtâ€‘in. | `Repeat` plus chat helpers let you spin up translation swarms, codeâ€‘review pairs, or tutoring agents in **5Â minutes of code** â€“ conversations live in `chat_history`, cost and revisions are automatic. | `SpawnFromList` explodes complex items into fineâ€‘grained subtasks, runs them **in parallel**, then `CollectAllToList` stitches results back â€“ perfect for beatâ†’sceneâ†’arc analyÂ­sis or any long, messy document pipeline. |

---

### Loop snippet (Roleâ€‘Playing)

```python
Teacher = Character("teacher_name", TEACHER_PROMPT)
Student = Character("student_name", STUDENT_PROMPT)

g = ( ReadMarkdownLines("story.txt", "keyword")
      | SetField({"teacher_name":"Alice", "student_name":"Bob"})
      | Teacher("è€å¸ˆï¼Œè¯·å…ˆè®²è§£è¯¾æ–‡", 0)
      | Repeat( Student("åŒå­¦æé—®æˆ–å›ç­”", 1)
                | Teacher("å›åº”æˆ–ç»§ç»­è®²è§£", 2), 3)
      | Teacher("è¯·æ€»ç»“", 3)
      | ChatHistoryToText() | Print() )
```

### Spawn snippet (chapter â†’ paragraph â†’ chapter synopsis)

```python
project = bf.CacheFolder("spawn_demo", 1, 0, 0)
broker  = bf.brokers.ConcurrentLLMCallBroker(project["cache/llm.jsonl"])

g = ( ReadMarkdownLines("novel/*.md", "chapter")                 # each entry = a chapter
      | SpawnFromList("paragraphs", "para")                         # fanâ€‘out per paragraph
      | GenerateLLMRequest("Summarise:\n{para}", model="gpt-4o-mini@openai")
      | ConcurrentLLMCall(project["cache/para_sum.jsonl"], broker)
      | CollectAllToList("text", "chapter_summaries")               # wait until ALL paras done
      | GenerateLLMRequest("Chapter synopsis:\n{chapter_summaries}",
                           model="gpt-4o-mini@openai")
      | ConcurrentLLMCall(project["cache/ch_sum.jsonl"], broker) )
```

*pseudocode, please see examples for implementation detail*

---

## Core concepts (oneâ€‘liner view)

| Term             | Story in one sentence                                                                                                                               |
| ---------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Entry**        | Tiny record with immutable `idx`, mutable `data`, autoâ€‘incrementing `rev`.                                                                          |
| **Op**           | Atomic node; compose with `|`or explicit`wire()`.                                                                                                   |
| **GraphSegment** | A lightweight helper: `.to_segment()` just wraps a node so `|`and`wire()` work on itâ€”each actual op still appears exactly once.                     |
| **execute()**    | Highâ€‘level driver that *resumes*, *pumps*, and *dispatches* brokers.                                                                                |
| **Broker**       | Pluggable engine handling expensive / async jobs (LLM, search, human labels).                                                                       |
| **Ledger**       | Appendâ€‘only JSONL cache behind every broker & graph enabling instant restart.                                                                       |

*(You *can* call `pump()` manually, but 99Â % of users stick to `execute()`.)*

---

## Primitive index (short list)

| Family              | Node                                                           | Blurb                             |
| ------------------- | -------------------------------------------------------------- | --------------------------------- |
| **Sources**         | `ReadMarkdownLines`, `FromList`                                | ingest files or raw dicts         |
| **Transforms**      | `Apply`, `Filter`, `SetField`                                  | pythonâ€‘powered field tweaks       |
| **SpawnÂ /Â Collect** | `SpawnFromList`, `CollectAllToList`                            | mapâ€‘reduce with unique child ids  |
| **Control flow**    | `If`, `While`, `Repeat`                                        | branch, loop, iterate             |
| **LLM**             | `GenerateLLMRequest â†’ ConcurrentLLMCall â†’ ExtractResponseText` | prompt, call, harvest             |
| **Utilities**       | `CleanupLLMData`, `PrintTotalCost`                             | tidy temporary fields, audit cost |

*(Sharedâ€‘idx ops `Replicate` / `Collect` are deprecated and vanish inÂ v0.3.)*

---

## Example gallery

| âœ¨Â Example                       | Demonstrates                                          |
| ------------------------------- | ----------------------------------------------------- |
| **01Â â€“ Basic pipeline**         | linear LLM transform & caching                        |
| **02Â â€“ Roleâ€‘playing loop**      | concise multiâ€‘agent RPG using `Repeat` + chat helpers |
| **03Â â€“ SplitÂ & summarise**      | fanâ€‘out/fanâ€‘in summarisation (deprecated style)       |
| **04Â â€“ Longâ€‘text segmentation** | SpawnÂ + CollectAll power pattern                      |
| **05Â â€“ Math ops (unit)**        | loop + conditional logic under pure Python            |

---

## Broker & cache highlights

* Each expensive call is hashed â†’ `job_idx` â€” **duplicate prompts are free**.
* `BrokerFailureBehavior = RETRY | STAY | EMIT` lets you decide how failures propagate.
* On restart, `execute()` reuses cached results and sends *only* the missing jobs.

---

## Roadmap â†’ v0.3

* Enforce **unique `idx`** endâ€‘toâ€‘end â†’ new `JoinByParent` replaces deprecated sharedâ€‘idx ops.
* Builtâ€‘in vectorâ€‘store & semanticâ€‘search nodes.
* Streamlined cost & progress reporting.
* More batteriesâ€‘included tutorials.

---

Â©Â 2025 Â· MIT License
