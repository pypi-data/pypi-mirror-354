Metadata-Version: 2.4
Name: pyhub-llm
Version: 0.1.0
Summary: Standalone LLM library with support for multiple providers
Project-URL: Homepage, https://github.com/pyhub-team/pyhub-llm
Project-URL: Documentation, https://github.com/pyhub-team/pyhub-llm#readme
Project-URL: Repository, https://github.com/pyhub-team/pyhub-llm
Project-URL: Issues, https://github.com/pyhub-team/pyhub-llm/issues
Author-email: PyHub Team <team@pyhub.dev>
License: MIT
License-File: LICENSE
Keywords: agent,ai,anthropic,google,llm,mcp,ollama,openai,react
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.10
Requires-Dist: aiofiles>=23.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: jinja2>=3.1.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: toml>=0.10.0
Requires-Dist: typer>=0.9.0
Provides-Extra: all
Requires-Dist: anthropic>=0.52.0; extra == 'all'
Requires-Dist: google-genai>=1.19.0; extra == 'all'
Requires-Dist: ollama>=0.5.0; extra == 'all'
Requires-Dist: openai>=1.84.0; extra == 'all'
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.52.0; extra == 'anthropic'
Provides-Extra: build
Requires-Dist: build; extra == 'build'
Requires-Dist: setuptools; extra == 'build'
Requires-Dist: twine; extra == 'build'
Requires-Dist: wheel; extra == 'build'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: docs
Requires-Dist: mkdocs; extra == 'docs'
Requires-Dist: mkdocs-glightbox; extra == 'docs'
Requires-Dist: mkdocs-material; extra == 'docs'
Requires-Dist: pymdown-extensions; extra == 'docs'
Provides-Extra: google
Requires-Dist: google-genai>=1.19.0; extra == 'google'
Provides-Extra: ollama
Requires-Dist: ollama>=0.5.0; extra == 'ollama'
Provides-Extra: openai
Requires-Dist: openai>=1.84.0; extra == 'openai'
Description-Content-Type: text/markdown

# pyhub-llm

Îã§ÏñëÌïú LLM Ï†úÍ≥µÏóÖÏ≤¥Î•º ÏúÑÌïú ÌÜµÌï© Python ÎùºÏù¥Î∏åÎü¨Î¶¨ÏûÖÎãàÎã§. OpenAI, Anthropic, Google, Ollama Îì±Ïùò APIÎ•º ÏùºÍ¥ÄÎêú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.

## Ï£ºÏöî Í∏∞Îä•

- üîå **ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§**: Î™®Îì† LLM Ï†úÍ≥µÏóÖÏ≤¥Î•º ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú ÏÇ¨Ïö©
- üöÄ **Í∞ÑÌé∏Ìïú Ï†ÑÌôò**: ÏΩîÎìú Î≥ÄÍ≤Ω ÏóÜÏù¥ Î™®Îç∏ Ï†ÑÌôò Í∞ÄÎä•
- üíæ **Ï∫êÏã± ÏßÄÏõê**: ÏùëÎãµ Ï∫êÏã±ÏúºÎ°ú ÎπÑÏö© Ï†àÍ∞ê Î∞è ÏÑ±Îä• Ìñ•ÏÉÅ
- üîÑ **Ïä§Ìä∏Î¶¨Î∞ç ÏßÄÏõê**: Ïã§ÏãúÍ∞Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
- üõ†Ô∏è **ÎèÑÍµ¨/Ìï®Ïàò Ìò∏Ï∂ú**: Function calling ÏßÄÏõê
- üì∑ **Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨**: Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö Î∞è Î∂ÑÏÑù Í∏∞Îä•
- ‚ö° **ÎπÑÎèôÍ∏∞ ÏßÄÏõê**: ÎèôÍ∏∞/ÎπÑÎèôÍ∏∞ Î™®Îëê ÏßÄÏõê
- üîó **Ï≤¥Ïù¥Îãù**: Ïó¨Îü¨ LLMÏùÑ Ïó∞Í≤∞ÌïòÏó¨ Î≥µÏû°Ìïú ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨ÏÑ±

## ÏÑ§Ïπò

### Í∏∞Î≥∏ ÏÑ§Ïπò

```bash
pip install pyhub-llm
```

### ÌäπÏ†ï Ï†úÍ≥µÏóÖÏ≤¥Îßå ÏÑ§Ïπò

```bash
# OpenAIÎßå
pip install "pyhub-llm[openai]"

# AnthropicÎßå
pip install "pyhub-llm[anthropic]"

# Î™®Îì† Ï†úÍ≥µÏóÖÏ≤¥
pip install "pyhub-llm[all]"
```

### Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ïπò

```bash
# Ï†ÄÏû•ÏÜå ÌÅ¥Î°†
git clone https://github.com/pyhub-kr/pyhub-llm.git
cd pyhub-llm

# Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ïπò
make install
```

## Îπ†Î•∏ ÏãúÏûë

### Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï

```python
from pyhub.llm import LLMFactory

# LLM Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
llm = LLMFactory.create("gpt-4o-mini")

# ÏßàÎ¨∏ÌïòÍ∏∞
response = llm.ask("PythonÏùò Ïû•Ï†êÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?")
print(response.text)
```

### Î™®Îç∏Î≥Ñ ÏßÅÏ†ë ÏÇ¨Ïö©

```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM

# OpenAI
openai_llm = OpenAILLM(model="gpt-4o-mini")
response = openai_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# Anthropic
claude_llm = AnthropicLLM(model="claude-3-haiku-20240307")
response = claude_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# Google
gemini_llm = GoogleLLM(model="gemini-1.5-flash")
response = gemini_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")
```

## Ï£ºÏöî Í∏∞Îä• ÏòàÏ†ú

### 1. Ïä§Ìä∏Î¶¨Î∞ç ÏùëÎãµ

```python
# ÎèôÍ∏∞ Ïä§Ìä∏Î¶¨Î∞ç
for chunk in llm.ask("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)

# ÎπÑÎèôÍ∏∞ Ïä§Ìä∏Î¶¨Î∞ç
async for chunk in await llm.ask_async("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)
```

### 2. ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Í¥ÄÎ¶¨

```python
# ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ Ïú†ÏßÄ
llm = LLMFactory.create("gpt-4o-mini")

# Ï≤´ Î≤àÏß∏ ÏßàÎ¨∏
llm.ask("Ï†ú Ïù¥Î¶ÑÏùÄ ÍπÄÏ≤†ÏàòÏûÖÎãàÎã§", use_history=True)

# Îëê Î≤àÏß∏ ÏßàÎ¨∏ (Ïù¥Ï†Ñ ÎåÄÌôî Í∏∞Ïñµ)
response = llm.ask("Ï†ú Ïù¥Î¶ÑÏù¥ Î≠êÎùºÍ≥† ÌñàÏ£†?", use_history=True)
print(response.text)  # "ÍπÄÏ≤†ÏàòÎùºÍ≥† ÌïòÏÖ®ÏäµÎãàÎã§"

# ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ï¥àÍ∏∞Ìôî
llm.clear()
```

### 3. Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨

```python
# Îã®Ïùº Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
response = llm.describe_image("photo.jpg")
print(response.text)

# Ïª§Ïä§ÌÖÄ ÌîÑÎ°¨ÌîÑÌä∏Î°ú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù
response = llm.describe_image(
    "photo.jpg",
    prompt="Ïù¥ Ïù¥ÎØ∏ÏßÄÏóêÏÑú Î≥¥Ïù¥Îäî ÏÉâÏÉÅÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?"
)

# Ïó¨Îü¨ Ïù¥ÎØ∏ÏßÄ ÎèôÏãú Ï≤òÎ¶¨
responses = llm.describe_images([
    "image1.jpg",
    "image2.jpg",
    "image3.jpg"
])

# Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
text = llm.extract_text_from_image("document.jpg")
```

### 4. ÏÑ†ÌÉùÏßÄ Ï†úÌïú

```python
# ÏÑ†ÌÉùÏßÄ Ï§ëÏóêÏÑúÎßå ÏùëÎãµ
response = llm.ask(
    "Ïù¥ Î¶¨Î∑∞Ïùò Í∞êÏ†ïÏùÄ?",
    context={"review": "Ï†ïÎßê ÏµúÍ≥†Ïùò Ï†úÌíàÏûÖÎãàÎã§!"},
    choices=["Í∏çÏ†ï", "Î∂ÄÏ†ï", "Ï§ëÎ¶Ω"]
)
print(response.choice)  # "Í∏çÏ†ï"
print(response.confidence)  # 0.95
```

### 5. ÎèÑÍµ¨/Ìï®Ïàò Ìò∏Ï∂ú

```python
from pyhub.llm.tools import Tool

# ÎèÑÍµ¨ Ï†ïÏùò
def get_weather(city: str) -> str:
    return f"{city}Ïùò ÎÇ†Ïî®Îäî ÎßëÏùåÏûÖÎãàÎã§."

weather_tool = Tool(
    name="get_weather",
    description="ÎèÑÏãúÏùò ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏ÏòµÎãàÎã§",
    func=get_weather,
    parameters={
        "type": "object",
        "properties": {
            "city": {"type": "string", "description": "ÎèÑÏãú Ïù¥Î¶Ñ"}
        },
        "required": ["city"]
    }
)

# ÎèÑÍµ¨ÏôÄ Ìï®Íªò LLM ÏÇ¨Ïö©
response = llm.ask(
    "ÏÑúÏö∏Ïùò ÎÇ†Ïî®Îäî Ïñ¥Îïå?",
    tools=[weather_tool]
)
print(response.text)  # "ÏÑúÏö∏Ïùò ÎÇ†Ïî®Îäî ÎßëÏùåÏûÖÎãàÎã§."
```

### 6. LLM Ï≤¥Ïù¥Îãù

```python
# Î≤àÏó≠ Ï≤¥Ïù∏ Íµ¨ÏÑ±
translator = LLMFactory.create(
    "gpt-4o-mini",
    prompt="Îã§Ïùå ÌÖçÏä§Ìä∏Î•º ÏòÅÏñ¥Î°ú Î≤àÏó≠ÌïòÏÑ∏Ïöî: {text}"
)

summarizer = LLMFactory.create(
    "gpt-4o-mini",
    prompt="Îã§Ïùå ÏòÅÏñ¥ ÌÖçÏä§Ìä∏Î•º Ìïú Î¨∏Ïû•ÏúºÎ°ú ÏöîÏïΩÌïòÏÑ∏Ïöî: {text}"
)

# Ï≤¥Ïù∏ Ïó∞Í≤∞
chain = translator | summarizer

# Ïã§Ìñâ
result = chain.ask({"text": "Ïù∏Í≥µÏßÄÎä•ÏùÄ Ïö∞Î¶¨Ïùò ÎØ∏ÎûòÎ•º Î∞îÍøÄ Í≤ÉÏûÖÎãàÎã§..."})
print(result.values["text"])  # Î≤àÏó≠ ÌõÑ ÏöîÏïΩÎêú Í≤∞Í≥º
```

### 7. Ï∫êÏã± ÏÇ¨Ïö©

```python
# Ï∫êÏã± ÌôúÏÑ±Ìôî
response = llm.ask("Î≥µÏû°Ìïú ÏßàÎ¨∏...", enable_cache=True)

# Í∞ôÏùÄ ÏßàÎ¨∏ Ïû¨ÏöîÏ≤≠Ïãú Ï∫êÏãúÏóêÏÑú Î∞òÌôò (Îπ†Î•¥Í≥† ÎπÑÏö© ÏóÜÏùå)
cached_response = llm.ask("Î≥µÏû°Ìïú ÏßàÎ¨∏...", enable_cache=True)
```

### 8. ÌÖúÌîåÎ¶ø ÏÇ¨Ïö©

```python
# ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï
llm = LLMFactory.create(
    "gpt-4o-mini",
    system_prompt="ÎãπÏã†ÏùÄ {role}ÏûÖÎãàÎã§.",
    prompt="ÏßàÎ¨∏: {question}\nÎãµÎ≥Ä:"
)

# ÌÖúÌîåÎ¶ø Î≥ÄÏàòÏôÄ Ìï®Íªò ÏÇ¨Ïö©
response = llm.ask({
    "role": "ÏàòÌïô ÍµêÏÇ¨",
    "question": "ÌîºÌÉÄÍ≥†ÎùºÏä§ Ï†ïÎ¶¨ÎûÄ?"
})
```

## ÌôòÍ≤Ω ÏÑ§Ï†ï

### ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï

```bash
# .env ÌååÏùº
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key
```

### pyproject.toml ÏÑ§Ï†ï

```toml
[tool.pyhub.llm]
# Í∏∞Î≥∏ Î™®Îç∏ ÏÑ§Ï†ï
default_model = "gpt-4o-mini"
default_embedding_model = "text-embedding-3-small"

# Í∏∞Î≥∏ ÌååÎùºÎØ∏ÌÑ∞
temperature = 0.7
max_tokens = 1000

# Ï∫êÏãú ÏÑ§Ï†ï
cache_ttl = 3600
cache_dir = ".cache/llm"
```

## CLI ÏÇ¨Ïö©Î≤ï

### ÎåÄÌôîÌòï Ï±ÑÌåÖ
```bash
# Í∏∞Î≥∏ Î™®Îç∏Î°ú Ï±ÑÌåÖ
pyhub-llm chat

# ÌäπÏ†ï Î™®Îç∏Î°ú Ï±ÑÌåÖ
pyhub-llm chat --model claude-3-haiku-20240307

# ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï
pyhub-llm chat --system "ÎãπÏã†ÏùÄ ÌååÏù¥Ïç¨ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§"
```

### Îã®Ïùº ÏßàÎ¨∏
```bash
# ÏßàÎ¨∏ÌïòÍ≥† ÏùëÎãµ Î∞õÍ∏∞
pyhub-llm ask "PythonÍ≥º GoÏùò Ï∞®Ïù¥Ï†êÏùÄ?"

# ÌååÏùº ÎÇ¥Ïö©Í≥º Ìï®Íªò ÏßàÎ¨∏
pyhub-llm ask "Ïù¥ ÏΩîÎìúÎ•º Î¶¨Î∑∞Ìï¥Ï£ºÏÑ∏Ïöî" --file main.py
```

### Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
```bash
# Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
pyhub-llm describe image.jpg

# Ïó¨Îü¨ Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
pyhub-llm describe *.jpg --output descriptions.json
```

### ÏûÑÎ≤†Îî© ÏÉùÏÑ±
```bash
# ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî©
pyhub-llm embed "ÏûÑÎ≤†Îî©Ìï† ÌÖçÏä§Ìä∏"

# ÌååÏùº ÎÇ¥Ïö© ÏûÑÎ≤†Îî©
pyhub-llm embed --file document.txt
```

## Í≥†Í∏â Í∏∞Îä•

### ÏóêÏù¥Ï†ÑÌä∏ ÌîÑÎ†àÏûÑÏõåÌÅ¨

```python
from pyhub.llm.agents import ReactAgent
from pyhub.llm.tools import WebSearchTool, CalculatorTool

# ÎèÑÍµ¨Î•º Í∞ÄÏßÑ ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±
agent = ReactAgent(
    llm=LLMFactory.create("gpt-4o"),
    tools=[WebSearchTool(), CalculatorTool()],
    max_iterations=5
)

# Î≥µÏû°Ìïú ÏûëÏóÖ ÏàòÌñâ
result = agent.run(
    "2024ÎÖÑ ÌïúÍµ≠Ïùò GDPÎäî ÏñºÎßàÏù¥Í≥†, "
    "Ïù¥Î•º ÏõêÌôîÎ°ú ÌôòÏÇ∞ÌïòÎ©¥ ÏñºÎßàÏù∏Í∞ÄÏöî?"
)
```

### MCP (Model Context Protocol) ÌÜµÌï©

```python
from pyhub.llm.agents.mcp import MCPClient

# MCP ÏÑúÎ≤Ñ Ïó∞Í≤∞
mcp_client = MCPClient("localhost:8080")

# MCP ÎèÑÍµ¨Î•º LLMÍ≥º Ìï®Íªò ÏÇ¨Ïö©
llm = LLMFactory.create("gpt-4o", tools=mcp_client.get_tools())
response = llm.ask("ÌòÑÏû¨ ÏãúÏä§ÌÖú ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî")
```

## Í∞úÎ∞ú

### ÌÖåÏä§Ìä∏ Ïã§Ìñâ

```bash
# Î™®Îì† ÌÖåÏä§Ìä∏
make test

# ÌäπÏ†ï ÌÖåÏä§Ìä∏
pytest tests/test_openai.py -v

# Ïª§Î≤ÑÎ¶¨ÏßÄ Ìè¨Ìï®
pytest --cov=pyhub.llm
```

### ÏΩîÎìú ÌíàÏßà Í≤ÄÏÇ¨

```bash
# Ìè¨Îß∑ÌåÖ Î∞è Î¶∞ÌåÖ
make format
make lint

# ÌÉÄÏûÖ Ï≤¥ÌÅ¨
mypy src/
```

### ÎπåÎìú Î∞è Î∞∞Ìè¨

```bash
# Ìå®ÌÇ§ÏßÄ ÎπåÎìú
make build

# PyPI Î∞∞Ìè¨ (Í∂åÌïú ÌïÑÏöî)
make release
```

## Í∏∞Ïó¨ÌïòÍ∏∞

1. Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Ìè¨ÌÅ¨Ìï©ÎãàÎã§
2. Í∏∞Îä• Î∏åÎûúÏπòÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§ (`git checkout -b feature/amazing-feature`)
3. Î≥ÄÍ≤ΩÏÇ¨Ìï≠ÏùÑ Ïª§Î∞ãÌï©ÎãàÎã§ (`git commit -m 'Add amazing feature'`)
4. Î∏åÎûúÏπòÏóê Ìë∏ÏãúÌï©ÎãàÎã§ (`git push origin feature/amazing-feature`)
5. Pull RequestÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§

### Í∏∞Ïó¨ Í∞ÄÏù¥ÎìúÎùºÏù∏

- Î™®Îì† ÏÉà Í∏∞Îä•ÏóêÎäî ÌÖåÏä§Ìä∏Î•º Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî
- ÏΩîÎìú Ïä§ÌÉÄÏùºÏùÄ BlackÍ≥º RuffÎ•º Îî∞Î¶ÖÎãàÎã§
- ÌÉÄÏûÖ ÌûåÌä∏Î•º ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî
- Î¨∏ÏÑúÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï¥Ï£ºÏÑ∏Ïöî

## ÎùºÏù¥ÏÑ†Ïä§

Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî MIT ÎùºÏù¥ÏÑ†Ïä§Î•º Îî∞Î¶ÖÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ [LICENSE](LICENSE) ÌååÏùºÏùÑ Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.

## Î¨∏Ï†ú Ìï¥Í≤∞

### ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ï†ú

**Q: API ÌÇ§ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§**

```python
# Ìï¥Í≤∞ Î∞©Î≤ï 1: ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
import os
os.environ["OPENAI_API_KEY"] = "your-key"

# Ìï¥Í≤∞ Î∞©Î≤ï 2: ÏßÅÏ†ë Ï†ÑÎã¨
llm = OpenAILLM(api_key="your-key")
```

**Q: ÏÜçÎèÑÍ∞Ä ÎäêÎ¶ΩÎãàÎã§**

```python
# Ï∫êÏã± ÌôúÏÑ±Ìôî
response = llm.ask("...", enable_cache=True)

# Îçî Îπ†Î•∏ Î™®Îç∏ ÏÇ¨Ïö©
llm = LLMFactory.create("gpt-3.5-turbo")
```

**Q: Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ ÎÜíÏäµÎãàÎã§**

```python
# ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ï†úÌïú
llm = LLMFactory.create(
    "gpt-4o-mini",
    initial_messages=[]  # ÌûàÏä§ÌÜ†Î¶¨ ÏóÜÏù¥ ÏãúÏûë
)

# Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÌûàÏä§ÌÜ†Î¶¨ Ï†ïÎ¶¨
if len(llm) > 10:
    llm.clear()
```

## ÎßÅÌÅ¨

- [Î¨∏ÏÑú](https://pyhub-llm.readthedocs.io)
- [PyPI](https://pypi.org/project/pyhub-llm)
- [GitHub](https://github.com/pyhub-kr/pyhub-llm)
- [Ïù¥Ïäà Ìä∏ÎûòÏª§](https://github.com/pyhub-kr/pyhub-llm/issues)

