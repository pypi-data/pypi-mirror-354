Metadata-Version: 2.4
Name: pyhub-llm
Version: 0.4.0
Summary: Standalone LLM library with support for multiple providers
Project-URL: Homepage, https://github.com/pyhub-team/pyhub-llm
Project-URL: Documentation, https://github.com/pyhub-team/pyhub-llm#readme
Project-URL: Repository, https://github.com/pyhub-team/pyhub-llm
Project-URL: Issues, https://github.com/pyhub-team/pyhub-llm/issues
Author-email: PyHub Team <me@pyhub.kr>
License: MIT
License-File: LICENSE
Keywords: agent,ai,anthropic,google,llm,mcp,ollama,openai,react
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.10
Requires-Dist: aiofiles>=23.0.0
Requires-Dist: httpx>=0.24.0
Requires-Dist: jinja2>=3.1.0
Requires-Dist: pillow>=10.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: python-dotenv>=1.0.0
Requires-Dist: rich>=13.0.0
Requires-Dist: toml>=0.10.0
Requires-Dist: typer>=0.9.0
Provides-Extra: all
Requires-Dist: anthropic>=0.52.0; extra == 'all'
Requires-Dist: google-genai>=1.19.0; extra == 'all'
Requires-Dist: ollama>=0.5.0; extra == 'all'
Requires-Dist: openai>=1.84.0; extra == 'all'
Requires-Dist: pymupdf; extra == 'all'
Requires-Dist: pymupdf>=1.23.0; extra == 'all'
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.52.0; extra == 'anthropic'
Provides-Extra: build
Requires-Dist: build; extra == 'build'
Requires-Dist: setuptools; extra == 'build'
Requires-Dist: twine; extra == 'build'
Requires-Dist: wheel; extra == 'build'
Provides-Extra: dev
Requires-Dist: black>=23.0.0; extra == 'dev'
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.21.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: docs
Requires-Dist: mkdocs; extra == 'docs'
Requires-Dist: mkdocs-glightbox; extra == 'docs'
Requires-Dist: mkdocs-material; extra == 'docs'
Requires-Dist: pymdown-extensions; extra == 'docs'
Provides-Extra: google
Requires-Dist: google-genai>=1.19.0; extra == 'google'
Provides-Extra: ollama
Requires-Dist: ollama>=0.5.0; extra == 'ollama'
Requires-Dist: pymupdf; extra == 'ollama'
Provides-Extra: openai
Requires-Dist: openai>=1.84.0; extra == 'openai'
Provides-Extra: pdf
Requires-Dist: pymupdf>=1.23.0; extra == 'pdf'
Description-Content-Type: text/markdown

# pyhub-llm

Îã§ÏñëÌïú LLM Ï†úÍ≥µÏóÖÏ≤¥Î•º ÏúÑÌïú ÌÜµÌï© Python ÎùºÏù¥Î∏åÎü¨Î¶¨ÏûÖÎãàÎã§. OpenAI, Anthropic, Google, Ollama Îì±Ïùò APIÎ•º ÏùºÍ¥ÄÎêú Ïù∏ÌÑ∞ÌéòÏù¥Ïä§Î°ú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.

## Ï£ºÏöî Í∏∞Îä•

- üîå **ÌÜµÌï© Ïù∏ÌÑ∞ÌéòÏù¥Ïä§**: Î™®Îì† LLM Ï†úÍ≥µÏóÖÏ≤¥Î•º ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú ÏÇ¨Ïö©
- üöÄ **Í∞ÑÌé∏Ìïú Ï†ÑÌôò**: ÏΩîÎìú Î≥ÄÍ≤Ω ÏóÜÏù¥ Î™®Îç∏ Ï†ÑÌôò Í∞ÄÎä•
- üíæ **Ï∫êÏã± ÏßÄÏõê**: ÏùëÎãµ Ï∫êÏã±ÏúºÎ°ú ÎπÑÏö© Ï†àÍ∞ê Î∞è ÏÑ±Îä• Ìñ•ÏÉÅ
- üîÑ **Ïä§Ìä∏Î¶¨Î∞ç ÏßÄÏõê**: Ïã§ÏãúÍ∞Ñ ÏùëÎãµ Ïä§Ìä∏Î¶¨Î∞ç
- üõ†Ô∏è **ÎèÑÍµ¨/Ìï®Ïàò Ìò∏Ï∂ú**: Function calling ÏßÄÏõê
- üì∑ **Ïù¥ÎØ∏ÏßÄ Ï≤òÎ¶¨**: Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö Î∞è Î∂ÑÏÑù Í∏∞Îä•
- ‚ö° **ÎπÑÎèôÍ∏∞ ÏßÄÏõê**: ÎèôÍ∏∞/ÎπÑÎèôÍ∏∞ Î™®Îëê ÏßÄÏõê
- üîó **Ï≤¥Ïù¥Îãù**: Ïó¨Îü¨ LLMÏùÑ Ïó∞Í≤∞ÌïòÏó¨ Î≥µÏû°Ìïú ÏõåÌÅ¨ÌîåÎ°úÏö∞ Íµ¨ÏÑ±

## ÏÑ§Ïπò

### Í∏∞Î≥∏ ÏÑ§Ïπò

```bash
pip install pyhub-llm
```

### ÌäπÏ†ï Ï†úÍ≥µÏóÖÏ≤¥Îßå ÏÑ§Ïπò

```bash
# OpenAIÎßå
pip install "pyhub-llm[openai]"

# AnthropicÎßå
pip install "pyhub-llm[anthropic]"

# Î™®Îì† Ï†úÍ≥µÏóÖÏ≤¥
pip install "pyhub-llm[all]"
```

### Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ïπò

```bash
# Ï†ÄÏû•ÏÜå ÌÅ¥Î°†
git clone https://github.com/pyhub-kr/pyhub-llm.git
cd pyhub-llm

# Í∞úÎ∞ú ÌôòÍ≤Ω ÏÑ§Ïπò
pip install -e ".[dev,all]"
# ÌòπÏùÄ make install
```

## Îπ†Î•∏ ÏãúÏûë

### Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï

```python
from pyhub.llm import LLM

# LLM Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
llm = LLM.create("gpt-4o-mini")

# ÏßàÎ¨∏ÌïòÍ∏∞
reply = llm.ask("PythonÏùò Ïû•Ï†êÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?")
print(reply.text)
```

### Î™®Îç∏Î≥Ñ ÏßÅÏ†ë ÏÇ¨Ïö©

Í∞Å ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥ Ìï¥Îãπ ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Î®ºÏ†Ä ÏÑ§ÏπòÌï¥Ïïº Ìï©ÎãàÎã§:

```bash
# OpenAI ÏÇ¨Ïö©Ïãú
pip install "pyhub-llm[openai]"

# Anthropic ÏÇ¨Ïö©Ïãú
pip install "pyhub-llm[anthropic]"

# Google ÏÇ¨Ïö©Ïãú
pip install "pyhub-llm[google]"

# Ollama ÏÇ¨Ïö©Ïãú (Î°úÏª¨ Ïã§Ìñâ)
pip install "pyhub-llm[ollama]"
```

```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM, OllamaLLM

# OpenAI (OPENAI_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàò ÌïÑÏöî)
openai_llm = OpenAILLM(model="gpt-4o-mini")
reply = openai_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# API ÌÇ§ ÏßÅÏ†ë Ï†ÑÎã¨
openai_llm = OpenAILLM(model="gpt-4o-mini", api_key="your-api-key")

# Anthropic (ANTHROPIC_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàò ÌïÑÏöî)
claude_llm = AnthropicLLM(model="claude-3-haiku-20240307")
reply = claude_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# Google (GOOGLE_API_KEY ÌôòÍ≤ΩÎ≥ÄÏàò ÌïÑÏöî)
gemini_llm = GoogleLLM(model="gemini-1.5-flash")
reply = gemini_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")

# Ollama (Î°úÏª¨ Ïã§Ìñâ, API ÌÇ§ Î∂àÌïÑÏöî)
ollama_llm = OllamaLLM(model="mistral")
reply = ollama_llm.ask("ÏïàÎÖïÌïòÏÑ∏Ïöî!")
```

## Ollama Î°úÏª¨ Î™®Îç∏ ÏÇ¨Ïö©

OllamaÎäî Î°úÏª¨ÏóêÏÑú LLMÏùÑ Ïã§ÌñâÌï† Ïàò ÏûàÎäî Ïò§ÌîàÏÜåÏä§ ÎèÑÍµ¨ÏûÖÎãàÎã§. API ÌÇ§Í∞Ä ÌïÑÏöî ÏóÜÍ≥†, Îç∞Ïù¥ÌÑ∞Í∞Ä Ïô∏Î∂ÄÎ°ú Ï†ÑÏÜ°ÎêòÏßÄ ÏïäÏïÑ Í∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏Ïóê Ïú†Î¶¨Ìï©ÎãàÎã§.

### Ollama ÏÑ§Ïπò

#### macOS
```bash
# Homebrew ÏÇ¨Ïö©
brew install ollama

# ÎòêÎäî Í≥µÏãù ÏÑ§Ïπò ÌîÑÎ°úÍ∑∏Îû® Îã§Ïö¥Î°úÎìú
curl -fsSL https://ollama.ai/install.sh | sh
```

#### Linux
```bash
# ÏÑ§Ïπò Ïä§ÌÅ¨Î¶ΩÌä∏ Ïã§Ìñâ
curl -fsSL https://ollama.ai/install.sh | sh

# ÎòêÎäî Docker ÏÇ¨Ïö©
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### Windows
```bash
# PowerShellÏóêÏÑú Ïã§Ìñâ
iex (irm https://ollama.ai/install.ps1)

# ÎòêÎäî Í≥µÏãù ÏõπÏÇ¨Ïù¥Ìä∏ÏóêÏÑú ÏÑ§Ïπò ÌîÑÎ°úÍ∑∏Îû® Îã§Ïö¥Î°úÎìú
# https://ollama.ai/download/windows
```

### Î™®Îç∏ Îã§Ïö¥Î°úÎìú Î∞è Ïã§Ìñâ

```bash
# Ollama ÏÑúÎπÑÏä§ ÏãúÏûë (ÌïÑÏöîÌïú Í≤ΩÏö∞)
ollama serve

# Mistral Î™®Îç∏ Îã§Ïö¥Î°úÎìú
ollama pull mistral

# Îã§Î•∏ Ïù∏Í∏∞ Î™®Îç∏Îì§
ollama pull llama3.3
ollama pull gemma2
ollama pull qwen2

# Î™®Îç∏ Î™©Î°ù ÌôïÏù∏
ollama list

# Î™®Îç∏ ÏßÅÏ†ë Ïã§Ìñâ (ÌÖåÏä§Ìä∏Ïö©)
ollama run mistral
```

### pyhub-llmÏóêÏÑú Ollama ÏÇ¨Ïö©

```python
from pyhub.llm import OllamaLLM

# Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï
llm = OllamaLLM(model="mistral")
reply = llm.ask("PythonÏúºÎ°ú Ïõπ Ïä§ÌÅ¨ÎûòÌïëÌïòÎäî Î∞©Î≤ïÏùÑ ÏïåÎ†§Ï£ºÏÑ∏Ïöî")
print(reply.text)

# Ïä§Ìä∏Î¶¨Î∞çÏúºÎ°ú Ïã§ÏãúÍ∞Ñ ÏùëÎãµ Î∞õÍ∏∞
for chunk in llm.ask("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)

# Ïù¥ÎØ∏ÏßÄÏôÄ Ìï®Íªò ÏßàÎ¨∏ÌïòÍ∏∞
reply = llm.ask(
    "Ïù¥ Ïù¥ÎØ∏ÏßÄÏóê Î¨¥ÏóáÏù¥ Î≥¥Ïù¥ÎÇòÏöî?",
    files=["image.jpg"]
)

# PDF ÌååÏùº Ï≤òÎ¶¨ (ÏûêÎèôÏúºÎ°ú Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôòÎê®)
reply = llm.ask(
    "Ïù¥ PDF Î¨∏ÏÑúÎ•º ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî",
    files=["document.pdf"]  # ÏûêÎèôÏúºÎ°ú Í≥†ÌíàÏßà Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôò
)

# ÎπÑÎèôÍ∏∞ ÏÇ¨Ïö©
async def async_example():
    reply = await llm.ask_async("ÎπÑÎèôÍ∏∞Î°ú ÏßàÎ¨∏Ìï©ÎãàÎã§")
    return reply.text

# Ïª§Ïä§ÌÖÄ ÏÑ§Ï†ï
llm = OllamaLLM(
    model="mistral",
    temperature=0.7,
    max_tokens=2000,
    base_url="http://localhost:11434"  # Ïª§Ïä§ÌÖÄ Ollama ÏÑúÎ≤Ñ
)
```

### Ollama Ïû•Ï†ê

- **üîí Í∞úÏù∏Ï†ïÎ≥¥ Î≥¥Ìò∏**: Î™®Îì† Îç∞Ïù¥ÌÑ∞Í∞Ä Î°úÏª¨ÏóêÏÑú Ï≤òÎ¶¨
- **üí∞ ÎπÑÏö© Ï†àÍ∞ê**: API Ìò∏Ï∂ú ÎπÑÏö© ÏóÜÏùå
- **‚ö° Îπ†Î•∏ ÏùëÎãµ**: ÎÑ§Ìä∏ÏõåÌÅ¨ ÏßÄÏó∞ ÏóÜÏùå  
- **üåê Ïò§ÌîÑÎùºÏù∏ ÏÇ¨Ïö©**: Ïù∏ÌÑ∞ÎÑ∑ Ïó∞Í≤∞ Î∂àÌïÑÏöî
- **üéõÔ∏è ÏôÑÏ†ÑÌïú Ï†úÏñ¥**: Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞ ÏûêÏú† Ï°∞Ï†ï

### ÏßÄÏõê Î™®Îç∏

- **Llama Í≥ÑÏó¥**: llama3.3, llama3.1, llama3.2
- **Mistral**: mistral, mixtral
- **Gemma**: gemma2, gemma3  
- **Qwen**: qwen2, qwen2.5
- **Í∏∞ÌÉÄ**: phi3, codellama, vicuna Îì±

> **Ï∞∏Í≥†**: PDF ÌååÏùº Ï≤òÎ¶¨ Ïãú OllamaÎäî ÏûêÎèôÏúºÎ°ú Í≥†ÌíàÏßà Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôòÌïòÏó¨ Ï≤òÎ¶¨Ìï©ÎãàÎã§. ÌïúÍµ≠Ïñ¥ ÌÖçÏä§Ìä∏ Î≥¥Ï°¥ÏùÑ ÏúÑÌï¥ 600 DPIÎ°ú Î≥ÄÌôòÎê©ÎãàÎã§.

## Ï£ºÏöî Í∏∞Îä• ÏòàÏ†ú

### 1. Ïä§Ìä∏Î¶¨Î∞ç ÏùëÎãµ

```python
# ÎèôÍ∏∞ Ïä§Ìä∏Î¶¨Î∞ç
for chunk in llm.ask("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)

# ÎπÑÎèôÍ∏∞ Ïä§Ìä∏Î¶¨Î∞ç
async for chunk in await llm.ask_async("Í∏¥ Ïù¥ÏïºÍ∏∞Î•º Îì§Î†§Ï£ºÏÑ∏Ïöî", stream=True):
    print(chunk.text, end="", flush=True)
```

### 2. ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Í¥ÄÎ¶¨

```python
# ÎåÄÌôî Ïª®ÌÖçÏä§Ìä∏ Ïú†ÏßÄ
llm = LLM.create("gpt-4o-mini")

# Ï≤´ Î≤àÏß∏ ÏßàÎ¨∏
llm.ask("Ï†ú Ïù¥Î¶ÑÏùÄ ÍπÄÏ≤†ÏàòÏûÖÎãàÎã§", use_history=True)

# Îëê Î≤àÏß∏ ÏßàÎ¨∏ (Ïù¥Ï†Ñ ÎåÄÌôî Í∏∞Ïñµ)
reply = llm.ask("Ï†ú Ïù¥Î¶ÑÏù¥ Î≠êÎùºÍ≥† ÌñàÏ£†?", use_history=True)
print(reply.text)  # "ÍπÄÏ≤†ÏàòÎùºÍ≥† ÌïòÏÖ®ÏäµÎãàÎã§"

# ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ï¥àÍ∏∞Ìôî
llm.clear()
```

### 3. ÌååÏùº Ï≤òÎ¶¨ (Ïù¥ÎØ∏ÏßÄ Î∞è PDF)

```python
# Ïù¥ÎØ∏ÏßÄ ÌååÏùº Ï≤òÎ¶¨
reply = llm.ask(
    "Ïù¥ Ïù¥ÎØ∏ÏßÄÎ•º ÏÑ§Î™ÖÌï¥Ï£ºÏÑ∏Ïöî",
    files=["photo.jpg"]
)

# PDF ÌååÏùº Ï≤òÎ¶¨ (ProviderÎ≥Ñ ÏßÄÏõê ÌòÑÌô©)
# - OpenAI, Anthropic, Google: PDF ÏßÅÏ†ë ÏßÄÏõê
# - Ollama: PDFÎ•º Ïù¥ÎØ∏ÏßÄÎ°ú ÏûêÎèô Î≥ÄÌôòÌïòÏó¨ Ï≤òÎ¶¨
reply = llm.ask(
    "Ïù¥ PDF Î¨∏ÏÑúÎ•º ÏöîÏïΩÌï¥Ï£ºÏÑ∏Ïöî",
    files=["document.pdf"]
)

# Ïó¨Îü¨ ÌååÏùº ÎèôÏãú Ï≤òÎ¶¨
reply = llm.ask(
    "Ïù¥ ÌååÏùºÎì§Ïùò ÎÇ¥Ïö©ÏùÑ ÎπÑÍµêÌï¥Ï£ºÏÑ∏Ïöî",
    files=["doc1.pdf", "image1.jpg", "doc2.pdf"]
)

# Îã®Ïùº Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö (Ìé∏Ïùò Î©îÏÑúÎìú)
reply = llm.describe_image("photo.jpg")
print(reply.text)

# Ïª§Ïä§ÌÖÄ ÌîÑÎ°¨ÌîÑÌä∏Î°ú Ïù¥ÎØ∏ÏßÄ Î∂ÑÏÑù
reply = llm.describe_image(
    "photo.jpg",
    prompt="Ïù¥ Ïù¥ÎØ∏ÏßÄÏóêÏÑú Î≥¥Ïù¥Îäî ÏÉâÏÉÅÏùÄ Î¨¥ÏóáÏù∏Í∞ÄÏöî?"
)

# Ïó¨Îü¨ Ïù¥ÎØ∏ÏßÄ ÎèôÏãú Ï≤òÎ¶¨
responses = llm.describe_images([
    "image1.jpg",
    "image2.jpg",
    "image3.jpg"
])

# Ïù¥ÎØ∏ÏßÄÏóêÏÑú ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú
text = llm.extract_text_from_image("document.jpg")
```

#### ProviderÎ≥Ñ ÌååÏùº ÏßÄÏõê ÌòÑÌô©

| Provider | Ïù¥ÎØ∏ÏßÄ | PDF | ÎπÑÍ≥† |
|----------|--------|-----|------|
| OpenAI | ‚úÖ | ‚úÖ | PDF ÏßÅÏ†ë ÏßÄÏõê |
| Anthropic | ‚úÖ | ‚úÖ | PDF Î≤†ÌÉÄ ÏßÄÏõê |
| Google Gemini | ‚úÖ | ‚úÖ | PDF ÎÑ§Ïù¥Ìã∞Î∏å ÏßÄÏõê |
| Ollama | ‚úÖ | ‚ö†Ô∏è | PDF‚ÜíÏù¥ÎØ∏ÏßÄ ÏûêÎèô Î≥ÄÌôò |

> **Ï∞∏Í≥†**: OllamaÏóêÏÑú PDF ÌååÏùº ÏÇ¨Ïö© Ïãú ÏûêÎèôÏúºÎ°ú Ïù¥ÎØ∏ÏßÄÎ°ú Î≥ÄÌôòÎêòÎ©∞, Í≤ΩÍ≥† Î°úÍ∑∏Í∞Ä Ï∂úÎ†•Îê©ÎãàÎã§.

### 4. ÏÑ†ÌÉùÏßÄ Ï†úÌïú

```python
# ÏÑ†ÌÉùÏßÄ Ï§ëÏóêÏÑúÎßå ÏùëÎãµ
reply = llm.ask(
    "Ïù¥ Î¶¨Î∑∞Ïùò Í∞êÏ†ïÏùÄ?",
    context={"review": "Ï†ïÎßê ÏµúÍ≥†Ïùò Ï†úÌíàÏûÖÎãàÎã§!"},
    choices=["Í∏çÏ†ï", "Î∂ÄÏ†ï", "Ï§ëÎ¶Ω"]
)
print(reply.choice)  # "Í∏çÏ†ï"
print(reply.confidence)  # 0.95
```

### 5. ÎèÑÍµ¨/Ìï®Ïàò Ìò∏Ï∂ú

```python
from pyhub.llm.tools import Tool

# ÎèÑÍµ¨ Ï†ïÏùò
def get_weather(city: str) -> str:
    return f"{city}Ïùò ÎÇ†Ïî®Îäî ÎßëÏùåÏûÖÎãàÎã§."

weather_tool = Tool(
    name="get_weather",
    description="ÎèÑÏãúÏùò ÎÇ†Ïî® Ï†ïÎ≥¥Î•º Í∞ÄÏ†∏ÏòµÎãàÎã§",
    func=get_weather,
    parameters={
        "type": "object",
        "properties": {
            "city": {"type": "string", "description": "ÎèÑÏãú Ïù¥Î¶Ñ"}
        },
        "required": ["city"]
    }
)

# ÎèÑÍµ¨ÏôÄ Ìï®Íªò LLM ÏÇ¨Ïö©
reply = llm.ask(
    "ÏÑúÏö∏Ïùò ÎÇ†Ïî®Îäî Ïñ¥Îïå?",
    tools=[weather_tool]
)
print(reply.text)  # "ÏÑúÏö∏Ïùò ÎÇ†Ïî®Îäî ÎßëÏùåÏûÖÎãàÎã§."
```

### 6. LLM Ï≤¥Ïù¥Îãù

```python
# Î≤àÏó≠ Ï≤¥Ïù∏ Íµ¨ÏÑ±
translator = LLM.create(
    "gpt-4o-mini",
    prompt="Îã§Ïùå ÌÖçÏä§Ìä∏Î•º ÏòÅÏñ¥Î°ú Î≤àÏó≠ÌïòÏÑ∏Ïöî: {text}"
)

summarizer = LLM.create(
    "gpt-4o-mini",
    prompt="Îã§Ïùå ÏòÅÏñ¥ ÌÖçÏä§Ìä∏Î•º Ìïú Î¨∏Ïû•ÏúºÎ°ú ÏöîÏïΩÌïòÏÑ∏Ïöî: {text}"
)

# Ï≤¥Ïù∏ Ïó∞Í≤∞
chain = translator | summarizer

# Ïã§Ìñâ
result = chain.ask({"text": "Ïù∏Í≥µÏßÄÎä•ÏùÄ Ïö∞Î¶¨Ïùò ÎØ∏ÎûòÎ•º Î∞îÍøÄ Í≤ÉÏûÖÎãàÎã§..."})
print(result.values["text"])  # Î≤àÏó≠ ÌõÑ ÏöîÏïΩÎêú Í≤∞Í≥º
```

### 7. Ï∫êÏã± ÏÇ¨Ïö©

#### Ï∫êÏãú Ïù∏Ï†ùÏÖò Ìå®ÌÑ¥

```python
from pyhub.llm.cache import MemoryCache, FileCache

# Î©îÎ™®Î¶¨ Ï∫êÏãú ÏÇ¨Ïö©
memory_cache = MemoryCache(ttl=3600)  # 1ÏãúÍ∞Ñ TTL
llm = LLM.create("gpt-4o-mini", cache=memory_cache)

# ÌååÏùº Ï∫êÏãú ÏÇ¨Ïö©  
file_cache = FileCache(cache_dir=".cache", ttl=7200)  # 2ÏãúÍ∞Ñ TTL
llm = LLM.create("gpt-4o-mini", cache=file_cache)

# Ï∫êÏãúÍ∞Ä ÏÑ§Ï†ïÎêú LLMÏùÄ ÏûêÎèôÏúºÎ°ú Ï∫êÏãú ÏÇ¨Ïö©
reply = llm.ask("ÏßàÎ¨∏")

# Ïª§Ïä§ÌÖÄ Ï∫êÏãú Î∞±ÏóîÎìú Íµ¨ÌòÑ
class CustomCache(BaseCache):
    def get(self, key: str):
        # Redis, Database Îì± Ïª§Ïä§ÌÖÄ Ï∫êÏãú Î°úÏßÅ
        pass
    
    def set(self, key: str, value: Any, ttl: Optional[int] = None):
        # Ïª§Ïä§ÌÖÄ Ï†ÄÏû• Î°úÏßÅ
        pass

custom_cache = CustomCache()
llm = LLM.create("gpt-4o-mini", cache=custom_cache)
```

### 8. ÌÖúÌîåÎ¶ø ÏÇ¨Ïö©

```python
# ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï
llm = LLM.create(
    "gpt-4o-mini",
    system_prompt="ÎãπÏã†ÏùÄ {role}ÏûÖÎãàÎã§.",
    prompt="ÏßàÎ¨∏: {question}\nÎãµÎ≥Ä:"
)

# ÌÖúÌîåÎ¶ø Î≥ÄÏàòÏôÄ Ìï®Íªò ÏÇ¨Ïö©
reply = llm.ask({
    "role": "ÏàòÌïô ÍµêÏÇ¨",
    "question": "ÌîºÌÉÄÍ≥†ÎùºÏä§ Ï†ïÎ¶¨ÎûÄ?"
})
```

## API ÌÇ§ ÏÑ§Ï†ï

### ÌïÑÏöîÌïú API ÌÇ§

Í∞Å ÌîÑÎ°úÎ∞îÏù¥ÎçîÎ•º ÏÇ¨Ïö©ÌïòÎ†§Î©¥ Ìï¥Îãπ API ÌÇ§Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§:

- **OpenAI**: `OPENAI_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://platform.openai.com/api-keys)
- **Anthropic**: `ANTHROPIC_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://console.anthropic.com/settings/keys)
- **Google**: `GOOGLE_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://makersuite.google.com/app/apikey)
- **Upstage**: `UPSTAGE_API_KEY` - [API ÌÇ§ Î∞úÍ∏â](https://console.upstage.ai/)

### ÏÑ§Ï†ï Î∞©Î≤ï

#### 1. ÌôòÍ≤Ω Î≥ÄÏàòÎ°ú ÏÑ§Ï†ï
```bash
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GOOGLE_API_KEY="your-google-key"
```

#### 2. .env ÌååÏùº ÏÇ¨Ïö©
```bash
# .env ÌååÏùº ÏÉùÏÑ±
OPENAI_API_KEY=your-openai-key
ANTHROPIC_API_KEY=your-anthropic-key
GOOGLE_API_KEY=your-google-key
```

#### 3. ÏΩîÎìúÏóêÏÑú ÏßÅÏ†ë Ï†ÑÎã¨
```python
from pyhub.llm import OpenAILLM, AnthropicLLM, GoogleLLM

# API ÌÇ§Î•º ÏßÅÏ†ë Ï†ÑÎã¨
llm = OpenAILLM(api_key="your-api-key")
llm = AnthropicLLM(api_key="your-api-key")
llm = GoogleLLM(api_key="your-api-key")
```

## ÌôòÍ≤Ω ÏÑ§Ï†ï

### pyproject.toml ÏÑ§Ï†ï

```toml
[tool.pyhub.llm]
# Í∏∞Î≥∏ Î™®Îç∏ ÏÑ§Ï†ï
default_model = "gpt-4o-mini"
default_embedding_model = "text-embedding-3-small"

# Í∏∞Î≥∏ ÌååÎùºÎØ∏ÌÑ∞
temperature = 0.7
max_tokens = 1000

# Ï∫êÏãú ÏÑ§Ï†ï
cache_ttl = 3600
cache_dir = ".cache/llm"
```

## CLI ÏÇ¨Ïö©Î≤ï

### ÎåÄÌôîÌòï Ï±ÑÌåÖ
```bash
# Í∏∞Î≥∏ Î™®Îç∏Î°ú Ï±ÑÌåÖ
pyhub-llm chat

# ÌäπÏ†ï Î™®Îç∏Î°ú Ï±ÑÌåÖ
pyhub-llm chat --model claude-3-haiku-20240307

# ÏãúÏä§ÌÖú ÌîÑÎ°¨ÌîÑÌä∏ ÏÑ§Ï†ï
pyhub-llm chat --system "ÎãπÏã†ÏùÄ ÌååÏù¥Ïç¨ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§"
```

### Îã®Ïùº ÏßàÎ¨∏
```bash
# ÏßàÎ¨∏ÌïòÍ≥† ÏùëÎãµ Î∞õÍ∏∞
pyhub-llm ask "PythonÍ≥º GoÏùò Ï∞®Ïù¥Ï†êÏùÄ?"

# ÌååÏùº ÎÇ¥Ïö©Í≥º Ìï®Íªò ÏßàÎ¨∏
pyhub-llm ask "Ïù¥ ÏΩîÎìúÎ•º Î¶¨Î∑∞Ìï¥Ï£ºÏÑ∏Ïöî" --file main.py
```

### Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
```bash
# Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
pyhub-llm describe image.jpg

# Ïó¨Îü¨ Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö
pyhub-llm describe *.jpg --output descriptions.json
```

### ÏûÑÎ≤†Îî© ÏÉùÏÑ±
```bash
# ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî©
pyhub-llm embed "ÏûÑÎ≤†Îî©Ìï† ÌÖçÏä§Ìä∏"

# ÌååÏùº ÎÇ¥Ïö© ÏûÑÎ≤†Îî©
pyhub-llm embed --file document.txt
```

## Í≥†Í∏â Í∏∞Îä•

### ÏóêÏù¥Ï†ÑÌä∏ ÌîÑÎ†àÏûÑÏõåÌÅ¨

```python
from pyhub.llm.agents import ReactAgent
from pyhub.llm.tools import WebSearchTool, CalculatorTool

# ÎèÑÍµ¨Î•º Í∞ÄÏßÑ ÏóêÏù¥Ï†ÑÌä∏ ÏÉùÏÑ±
agent = ReactAgent(
    llm=LLM.create("gpt-4o"),
    tools=[WebSearchTool(), CalculatorTool()],
    max_iterations=5
)

# Î≥µÏû°Ìïú ÏûëÏóÖ ÏàòÌñâ
result = agent.run(
    "2024ÎÖÑ ÌïúÍµ≠Ïùò GDPÎäî ÏñºÎßàÏù¥Í≥†, "
    "Ïù¥Î•º ÏõêÌôîÎ°ú ÌôòÏÇ∞ÌïòÎ©¥ ÏñºÎßàÏù∏Í∞ÄÏöî?"
)
```

### MCP (Model Context Protocol) ÌÜµÌï©

```python
from pyhub.llm.agents.mcp import MCPClient

# MCP ÏÑúÎ≤Ñ Ïó∞Í≤∞
mcp_client = MCPClient("localhost:8080")

# MCP ÎèÑÍµ¨Î•º LLMÍ≥º Ìï®Íªò ÏÇ¨Ïö©
llm = LLM.create("gpt-4o", tools=mcp_client.get_tools())
reply = llm.ask("ÌòÑÏû¨ ÏãúÏä§ÌÖú ÏÉÅÌÉúÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî")
```

## Í∞úÎ∞ú

### ÌÖåÏä§Ìä∏ Ïã§Ìñâ

```bash
# Î™®Îì† ÌÖåÏä§Ìä∏
make test

# ÌäπÏ†ï ÌÖåÏä§Ìä∏
make test tests/test_openai.py

# Ïª§Î≤ÑÎ¶¨ÏßÄ Ìè¨Ìï® ÌÖåÏä§Ìä∏
make test-cov
# ÎòêÎäî
make cov

# Ïª§Î≤ÑÎ¶¨ÏßÄ HTML Î¶¨Ìè¨Ìä∏ Î≥¥Í∏∞
make test-cov-report

# ÌäπÏ†ï ÌååÏùºÎßå Ïª§Î≤ÑÎ¶¨ÏßÄ ÌÖåÏä§Ìä∏
make cov tests/test_optional_dependencies.py

# pytest ÏßÅÏ†ë Ïã§Ìñâ
pytest --cov=src/pyhub/llm --cov-report=term --cov-report=html
```

### ÏΩîÎìú ÌíàÏßà Í≤ÄÏÇ¨

```bash
# Ìè¨Îß∑ÌåÖ Î∞è Î¶∞ÌåÖ
make format
make lint

# ÌÉÄÏûÖ Ï≤¥ÌÅ¨
mypy src/
```

### ÎπåÎìú Î∞è Î∞∞Ìè¨

```bash
# Ìå®ÌÇ§ÏßÄ ÎπåÎìú
make build

# PyPI Î∞∞Ìè¨ (Í∂åÌïú ÌïÑÏöî)
make release
```

## Í∏∞Ïó¨ÌïòÍ∏∞

1. Ïù¥ Ï†ÄÏû•ÏÜåÎ•º Ìè¨ÌÅ¨Ìï©ÎãàÎã§
2. Í∏∞Îä• Î∏åÎûúÏπòÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§ (`git checkout -b feature/amazing-feature`)
3. Î≥ÄÍ≤ΩÏÇ¨Ìï≠ÏùÑ Ïª§Î∞ãÌï©ÎãàÎã§ (`git commit -m 'Add amazing feature'`)
4. Î∏åÎûúÏπòÏóê Ìë∏ÏãúÌï©ÎãàÎã§ (`git push origin feature/amazing-feature`)
5. Pull RequestÎ•º ÏÉùÏÑ±Ìï©ÎãàÎã§

### Í∏∞Ïó¨ Í∞ÄÏù¥ÎìúÎùºÏù∏

- Î™®Îì† ÏÉà Í∏∞Îä•ÏóêÎäî ÌÖåÏä§Ìä∏Î•º Ìè¨Ìï®Ìï¥Ï£ºÏÑ∏Ïöî
- ÏΩîÎìú Ïä§ÌÉÄÏùºÏùÄ BlackÍ≥º RuffÎ•º Îî∞Î¶ÖÎãàÎã§
- ÌÉÄÏûÖ ÌûåÌä∏Î•º ÏÇ¨Ïö©Ìï¥Ï£ºÏÑ∏Ïöî
- Î¨∏ÏÑúÎ•º ÏóÖÎç∞Ïù¥Ìä∏Ìï¥Ï£ºÏÑ∏Ïöî

## ÎùºÏù¥ÏÑ†Ïä§

Ïù¥ ÌîÑÎ°úÏ†ùÌä∏Îäî MIT ÎùºÏù¥ÏÑ†Ïä§Î•º Îî∞Î¶ÖÎãàÎã§. ÏûêÏÑ∏Ìïú ÎÇ¥Ïö©ÏùÄ [LICENSE](LICENSE) ÌååÏùºÏùÑ Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.

## Î¨∏Ï†ú Ìï¥Í≤∞

### ÏùºÎ∞òÏ†ÅÏù∏ Î¨∏Ï†ú

**Q: API ÌÇ§ Ïò§Î•òÍ∞Ä Î∞úÏÉùÌï©ÎãàÎã§**

```python
# Ìï¥Í≤∞ Î∞©Î≤ï 1: ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
import os
os.environ["OPENAI_API_KEY"] = "your-key"

# Ìï¥Í≤∞ Î∞©Î≤ï 2: ÏßÅÏ†ë Ï†ÑÎã¨
llm = OpenAILLM(api_key="your-key")
```

**Q: ÏÜçÎèÑÍ∞Ä ÎäêÎ¶ΩÎãàÎã§**

```python
# Ï∫êÏãú Ïù∏Ï†ùÏÖòÏúºÎ°ú Ï∫êÏã± ÌôúÏÑ±Ìôî
from pyhub.llm.cache import MemoryCache
cache = MemoryCache()
llm = LLM.create("gpt-4o-mini", cache=cache)
reply = llm.ask("...")

# Îçî Îπ†Î•∏ Î™®Îç∏ ÏÇ¨Ïö©
llm = LLM.create("gpt-3.5-turbo")
```

**Q: Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏù¥ ÎÜíÏäµÎãàÎã§**

```python
# ÎåÄÌôî ÌûàÏä§ÌÜ†Î¶¨ Ï†úÌïú
llm = LLM.create(
    "gpt-4o-mini",
    initial_messages=[]  # ÌûàÏä§ÌÜ†Î¶¨ ÏóÜÏù¥ ÏãúÏûë
)

# Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÌûàÏä§ÌÜ†Î¶¨ Ï†ïÎ¶¨
if len(llm) > 10:
    llm.clear()
```

## ÎßÅÌÅ¨

- [Î¨∏ÏÑú](https://pyhub-llm.readthedocs.io)
- [PyPI](https://pypi.org/project/pyhub-llm)
- [GitHub](https://github.com/pyhub-kr/pyhub-llm)
- [Ïù¥Ïäà Ìä∏ÎûòÏª§](https://github.com/pyhub-kr/pyhub-llm/issues)

