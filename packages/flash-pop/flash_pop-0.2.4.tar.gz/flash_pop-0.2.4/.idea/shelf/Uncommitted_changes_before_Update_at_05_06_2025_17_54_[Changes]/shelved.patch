Index: flash_pop/retnet.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+># Based on https://github.com/fkodom/yet-another-retnet/blob/main/yet_another_retnet/retention.py\r\n# Copyright (c) 2022 Frank Odom\r\n# Copyright (c) 2025 Lior Cohen\r\nfrom dataclasses import dataclass\r\nfrom functools import lru_cache\r\nfrom math import ceil, log\r\nfrom typing import Union, Callable, Optional, List, Sequence, Tuple, Literal\r\n\r\nimport numpy as np\r\nfrom loguru import logger\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch._dynamo\r\nfrom einops import rearrange, einsum, repeat\r\nfrom torch import Tensor\r\n\r\nfrom flash_pop.fused_retention import fused_chunk_retention\r\nfrom flash_pop.xpos_emb import XPos\r\n\r\nActivationString = Literal[\"swish\", \"gelu\", \"relu\"]\r\n\r\n\r\ndef _get_activation_fn(activation: str) -> Callable[[Tensor], Tensor]:\r\n    \"\"\"Return an activation function given a string\"\"\"\r\n    if activation == \"swish\":\r\n        return F.silu\r\n    elif activation == \"gelu\":\r\n        return F.gelu\r\n    elif activation == \"relu\":\r\n        return F.relu\r\n    else:\r\n        raise RuntimeError(\r\n            f\"Unsupported activation string '{activation}'. \"\r\n            \"Supported: 'swish', 'gelu', 'relu'\"\r\n        )\r\n\r\n\r\nDECAY_SCALE_MIN_NUM_BLOCKS = 4\r\nDECAY_SCALE_MAX_NUM_BLOCKS = 512\r\n\r\n\r\n@lru_cache(maxsize=1)\r\ndef _build_decay_gammas(\r\n        num_heads: int,\r\n        device: Optional[Union[torch.device, str]] = None,\r\n        dtype: Optional[torch.dtype] = None,\r\n        xmin: Optional[float] = None,\r\n        xmax: Optional[float] = None,\r\n) -> Tensor:\r\n    \"\"\"Decay values are different for each retention head, following the prescribed\r\n    method in the paper.  Conceptually, I think of each head having a different\r\n    \"retention window\", which is the effective number of steps back in time that\r\n    the head can attend to.  Retention windows are effectively determined by\r\n    these decay coefficients.\r\n\r\n    See: https://arxiv.org/pdf/2307.08621v3.pdf, Section 3.1 (Setup)\r\n    \"\"\"\r\n    if xmin is None:\r\n        xmin = log(1 / 32)\r\n    if xmax is None:\r\n        xmax = log(1 / 512)\r\n    x = torch.linspace(xmin, xmax, steps=num_heads, device=device, dtype=dtype)\r\n    return 1 - x.exp_()\r\n\r\n\r\n@lru_cache(maxsize=1)\r\ndef get_decays(num_heads: int, decay_range: Optional[tuple[float, float]] = None) -> np.ndarray:\r\n    if decay_range is None:\r\n        decay_exp = -5 -np.arange(num_heads)\r\n    else:\r\n        decay_exp = -np.linspace(decay_range[0], decay_range[1], num_heads)\r\n    return 1 - np.exp2(decay_exp)\r\n\r\n\r\ndef retention_recurrent(\r\n    query: Tensor,\r\n    key: Tensor,\r\n    value: Tensor,\r\n    head_decays: Tensor,\r\n    prev_state: Optional[Tensor],\r\n    scale: Optional[float] = None,\r\n) -> Tuple[Tensor, Tensor]:\r\n    assert head_decays.dim() == 4 and head_decays.size(0) == 1 and head_decays.size(2) == 1 and head_decays.size(3) == 1\r\n    # einstein notation:\r\n    # - b: batch_size\r\n    # - h: num_heads\r\n    # - d: hidden_dim\r\n    if scale is None:\r\n        scale = key.size(-1) ** 0.5\r\n    key = key / scale\r\n\r\n    state = einsum(key, value, \"b h dk, b h dv -> b h dk dv\")\r\n    if prev_state is not None:\r\n        state = state + prev_state * head_decays\r\n    retention = einsum(query, state, \"b h dk, b h dk dv -> b h dv\")\r\n\r\n    return retention, state\r\n\r\n\r\nclass MultiScaleRetention(nn.Module):\r\n    \"\"\"Multi-scale retention (MSR) layer.  Intended to be (mostly) a drop-in replacement\r\n        for nn.MultiheadAttention, but with the option to use either the parallel or\r\n        recurrent formulation of retention. (Attention only has the parallel formulation.)\r\n\r\n        NOTE: As presented in the paper, Multi-Scale Retention includes an explicit\r\n        position embedding, which is based on xPos.  IMO, this is unnecessary and overly\r\n        specific to language modeling, since other domains (e.g. computer vision,\r\n        heterogeneous graphs) will have different positional semantics.\r\n\r\n        I have made the relational position embedding optional, so that this module\r\n        can (in theory) support more modalities. Setting 'relative_position=False' will\r\n        remove the positional embedding, and instead rely on the query and key\r\n        embeddings to encode positional information ahead of time (if needed at all).\r\n        See: https://github.com/microsoft/torchscale/issues/48\r\n\r\n        Reference:\r\n            \"Retentive Network: A Successor to Transformer for Large Language Models\"\r\n            https://arxiv.org/pdf/2307.08621v3.pdf\r\n        \"\"\"\r\n    @dataclass(kw_only=True)\r\n    class Config:\r\n        num_heads: int\r\n        head_dim_v: int\r\n        head_dim_qk: int = None\r\n        dropout: float = 0.0\r\n        head_decays_range: Optional[Tuple[float, float]] = None\r\n        relative_position: bool = True\r\n        bias: bool = True\r\n        activation: Union[ActivationString, Callable[[Tensor], Tensor]] = \"swish\"\r\n        group_norm_eps: float = 1e-6\r\n        device: Optional[Union[torch.device, str]] = torch.device('cuda')\r\n        dtype: Optional[torch.dtype] = torch.bfloat16\r\n    def __init__(\r\n            self,\r\n            config: Config,\r\n            xpos_embedder: Optional[XPos] = None\r\n    ):\r\n        \"\"\"\"\"\"\r\n        activation = config.activation\r\n        if isinstance(config.activation, str):\r\n            activation = _get_activation_fn(config.activation)\r\n\r\n        super().__init__()\r\n        self.num_heads = config.num_heads\r\n        self.head_dim_v = config.head_dim_v\r\n        self.head_dim_qk = config.head_dim_v if config.head_dim_qk is None else config.head_dim_qk\r\n        embed_dim = self.head_dim_v * self.num_heads\r\n        self.embed_dim = embed_dim\r\n        self.dropout = config.dropout\r\n        self.relative_position = config.relative_position\r\n        self.xpos_embedder = XPos(\r\n            self.head_dim_qk,\r\n            device=config.device,\r\n            dtype=torch.float32\r\n        ) if xpos_embedder is None else xpos_embedder\r\n        self.bias = config.bias\r\n        self.activation = activation\r\n        self.head_decays = tuple(get_decays(self.num_heads, config.head_decays_range).tolist())\r\n        self.head_decays_torch = torch.tensor(self.head_decays, dtype=torch.float32, device=config.device)\r\n        self.head_decays_torch = rearrange(self.head_decays_torch, \"h -> () h () ()\")\r\n\r\n        if embed_dim % self.num_heads != 0:\r\n            raise ValueError(\r\n                f\"embed_dim ({embed_dim}) must be divisible by num_heads ({self.num_heads})\"\r\n            )\r\n\r\n        if not self.head_dim_v % 8 == 0:\r\n            raise ValueError(\r\n                f\"head_dim (embed_dim / num_heads = {self.head_dim_v}) must be divisible by 8\"\r\n            )\r\n\r\n        device, dtype = config.device, config.dtype\r\n        bias = config.bias\r\n\r\n        # The q/k/v projection layers are the same as in vanilla MHA.\r\n        self.q_proj = nn.Linear(\r\n            embed_dim, self.num_heads * self.head_dim_qk, bias=bias, device=device, dtype=dtype\r\n        )\r\n        self.k_proj = nn.Linear(\r\n            embed_dim, self.num_heads * self.head_dim_qk, bias=bias, device=device, dtype=dtype\r\n        )\r\n        self.v_proj = nn.Linear(\r\n            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\r\n        )\r\n        self.group_norm = nn.GroupNorm(\r\n            num_groups=self.num_heads,\r\n            num_channels=embed_dim,\r\n            affine=False,\r\n            eps=config.group_norm_eps,\r\n            device=config.device,\r\n            dtype=config.dtype,\r\n        )\r\n        # The output project is slightly different, due to the gated \"swish\" layer.\r\n        self.g_proj = nn.Linear(\r\n            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\r\n        )\r\n        self.out_proj = nn.Linear(\r\n            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\r\n        )\r\n\r\n        self._reset_parameters()\r\n\r\n    def _reset_parameters(self):\r\n        # TODO: Double-check that we're following the same initialization as in\r\n        # the paper.  This is a generic initialization for MHA linear layers.\r\n        nn.init.xavier_normal_(self.q_proj.weight)\r\n        if self.q_proj.bias is not None:\r\n            nn.init.constant_(self.q_proj.bias, 0)\r\n        nn.init.xavier_normal_(self.k_proj.weight)\r\n        if self.k_proj.bias is not None:\r\n            nn.init.constant_(self.k_proj.bias, 0)\r\n        nn.init.xavier_normal_(self.v_proj.weight)\r\n        if self.v_proj.bias is not None:\r\n            nn.init.constant_(self.v_proj.bias, 0)\r\n        nn.init.xavier_normal_(self.out_proj.weight)\r\n        if self.out_proj.bias is not None:\r\n            nn.init.constant_(self.out_proj.bias, 0)\r\n        nn.init.xavier_normal_(self.g_proj.weight)\r\n        if self.g_proj.bias is not None:\r\n            nn.init.constant_(self.g_proj.bias, 0)\r\n\r\n    def _compute_chunkwise_retention_kernel(\r\n            self,\r\n            q: Tensor,\r\n            k: Tensor,\r\n            v: Tensor,\r\n            prev_state: Tensor,\r\n    ) -> tuple[Tensor, Tensor]:\r\n        retention, state = fused_chunk_retention(\r\n            q, k, v, prev_state, self.head_decays\r\n        )\r\n        return retention, state\r\n\r\n    def _retention_chunkwise(\r\n            self,\r\n            x: Tensor,\r\n            start_idx: Union[int, torch.LongTensor],\r\n            prev_state: Optional[Tensor] = None,\r\n            retention_kernel: Optional[Callable] = None,\r\n            xpos_embedder: Optional[XPos] = None,\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        # einstein notation:\r\n        # b - batch size\r\n        # n - sequence length\r\n        # h - number of heads\r\n        # d - head dimension\r\n        #\r\n        # Input shape: (b, n, dim_v)\r\n        q = self.q_proj(x)\r\n        k = self.k_proj(x)\r\n        v = self.v_proj(x)\r\n\r\n        # Unfold 'd' dimension into 'h' separate retention heads.\r\n        q = rearrange(q, \"b n (h d) -> b n h d\", h=self.num_heads)\r\n        k = rearrange(k, \"b n (h d) -> b n h d\", h=self.num_heads)\r\n        v = rearrange(v, \"b n (h d) -> b n h d\", h=self.num_heads)\r\n\r\n        if self.relative_position:\r\n            # global (cross-chunk) + intra-chunk relative position embedding\r\n            if xpos_embedder is None:\r\n                xpos_embedder = self.xpos_embedder\r\n\r\n            q, k = xpos_embedder(q, k, start_idx)\r\n\r\n        if prev_state is None:\r\n            batch_size, seq_len, num_heads, dim_v = v.shape\r\n            dim_qk = q.shape[-1]\r\n            prev_state = torch.zeros((batch_size, num_heads, dim_qk, dim_v), device=x.device, dtype=x.dtype)\r\n\r\n        # Apply retention then group norm.\r\n        if retention_kernel is None:\r\n            retention_kernel = self._compute_chunkwise_retention_kernel\r\n        retention, state = retention_kernel(\r\n            q, k, v, prev_state,\r\n        )\r\n        # To apply group norm in an equivalent way to the recurrent formulation,\r\n        # we fold the sequence dimension into the batch dimension.  Otherwise,\r\n        # normalization would be applied over the entire input sequence.\r\n        batch_size = retention.size(0)\r\n        retention = rearrange(retention, \"b n h d -> (b n) (h d)\")\r\n        retention = F.dropout(retention, p=self.dropout, training=self.training)\r\n        retention = self.group_norm(retention)\r\n        # Unfold 'n' from the batch dimension, and fold 'h' back into the embed dim.\r\n        retention = rearrange(retention, \"(b n) e -> b n e\", b=batch_size)\r\n\r\n        # NOTE: Unlike multihead attention, the retention paper applies a \"swish\"\r\n        # gate to increase the non-linear capacity of the model.  (IMO this is likely\r\n        # to make up for the lack of \"softmax\" activation in the retention mechanism.)\r\n        #\r\n        # The paper describes the gate as:\r\n        #   g = swish(X * W_g)\r\n        # where X is the input to the layer.\r\n        gate = self.activation(self.g_proj(x))\r\n        retention = self.out_proj(retention * gate)\r\n\r\n        return retention, state\r\n\r\n    def forward_chunkwise(\r\n            self,\r\n            x: Tensor,\r\n            start_idx: Union[int, torch.LongTensor],\r\n            prev_state: Optional[Tensor] = None,\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        return self._retention_chunkwise(\r\n            x,\r\n            start_idx,\r\n            prev_state,\r\n        )\r\n\r\n    def forward_recurrent(\r\n            self,\r\n            x: Tensor,\r\n            seq_idx: int,\r\n            prev_state: Optional[Tensor],\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        # einstein notation:\r\n        # b - batch size\r\n        # h - number of heads\r\n        # d - embedding dimension\r\n        #\r\n        # input shape: (b, d)\r\n        q: Tensor = self.q_proj(x)\r\n        k: Tensor = self.k_proj(x)\r\n        v: Tensor = self.v_proj(x)\r\n\r\n        # Unfold 'd' dimension into 'h' separate retention heads.\r\n        q = rearrange(q, \"b (h d) -> b h d\", h=self.num_heads)\r\n        k = rearrange(k, \"b (h d) -> b h d\", h=self.num_heads)\r\n        v = rearrange(v, \"b (h d) -> b h d\", h=self.num_heads)\r\n\r\n        if self.relative_position:\r\n            q, k = self.xpos_embedder(q, k, seq_idx)\r\n\r\n        # Apply retention then group norm.\r\n        retention, state = retention_recurrent(q, k, v, self.head_decays_torch, prev_state=prev_state)\r\n        retention = F.dropout(retention, p=self.dropout, training=self.training)\r\n        # Fold heads back into the embedding dimension.\r\n        retention = rearrange(retention, \"b h d -> b (h d)\")\r\n        retention = self.group_norm(retention)\r\n\r\n        # NOTE: Unlike multihead attention, the retention paper applies a \"swish\"\r\n        # gate to increase the non-linear capacity of the model.  (IMO this is likely\r\n        # to make up for the lack of \"softmax\" activation in the retention mechanism.)\r\n        #\r\n        # The paper describes the gate as:\r\n        #   g = swish(X * W_g)\r\n        # where X is the input to the layer.\r\n        gate = self.activation(self.g_proj(x))\r\n        retention = self.out_proj(retention * gate)\r\n\r\n        return retention, state\r\n\r\n\r\nclass RetNetDecoderLayer(nn.Module):\r\n\r\n    # NOTE: Mostly pulled from 'nn.TransformerDecoderLayer', but with changes:\r\n    #   - use MultiScaleRetention instead of MultiheadAttention\r\n    #   - no cross-attention layer, since retention doesn't play well with that\r\n\r\n    @dataclass(kw_only=True)\r\n    class Config:\r\n        num_heads: int\r\n        head_dim_v: int\r\n        head_dim_qk: int = None\r\n        dim_feedforward: int = 2048\r\n        dropout: float = 0.1\r\n        head_decays_range: tuple[float, float] = None\r\n        activation: Union[ActivationString, Callable[[Tensor], Tensor]] = \"swish\"\r\n        norm_first: bool = True\r\n        layer_norm_eps: float = 1e-6\r\n        device: Optional[Union[torch.device, str]] = torch.device('cuda')\r\n        dtype: Optional[torch.dtype] = torch.bfloat16\r\n\r\n    def __init__(\r\n        self,\r\n        config: Config,\r\n        xpos_embedder: Optional[XPos] = None\r\n    ) -> None:\r\n        \"\"\"\r\n\r\n        :param num_heads: number of attention heads\r\n        :param head_dim_v: the dimension of each attention head. This defines d_model, i.e., embedding dimension,\r\n        through d_model = num_heads * head_dim_v.\r\n        :param head_dim_qk: the query and key dimension of each attention head. If none, `head_dim_v` is used.\r\n        Lower values (around 0.5-0.75*head_dim_v) were shown to be effective while reducing computational cost.\r\n        :param dim_feedforward: the dimension of feedforward layer (hidden)\r\n        :param dropout:\r\n        :param activation:\r\n        :param norm_first:\r\n        :param layer_norm_eps:\r\n        :param device:\r\n        :param dtype:\r\n        \"\"\"\r\n        self.config = config\r\n        activation = config.activation\r\n        if isinstance(config.activation, str):\r\n            activation = _get_activation_fn(config.activation)\r\n\r\n        super().__init__()\r\n        self.dropout = nn.Dropout(config.dropout)\r\n        self.activation = activation\r\n        self.norm_first = config.norm_first\r\n        d_model = config.num_heads * config.head_dim_v\r\n        # retention block\r\n        self.norm1 = nn.LayerNorm(\r\n            d_model,\r\n            eps=config.layer_norm_eps,\r\n            device=config.device,\r\n            dtype=config.dtype\r\n        )\r\n        self.retention = self._build_multi_scale_retention(xpos_embedder=xpos_embedder)\r\n        # feedforward block\r\n        self.norm2 = nn.LayerNorm(\r\n            d_model,\r\n            eps=config.layer_norm_eps,\r\n            device=config.device,\r\n            dtype=config.dtype\r\n        )\r\n        self.linear1 = nn.Linear(d_model, config.dim_feedforward, device=config.device, dtype=config.dtype)\r\n        self.linear2 = nn.Linear(config.dim_feedforward, d_model, device=config.device, dtype=config.dtype)\r\n\r\n        self._reset_parameters()\r\n\r\n    def _build_multi_scale_retention(self, xpos_embedder: Optional[XPos] = None):\r\n        return MultiScaleRetention(\r\n            MultiScaleRetention.Config(\r\n                num_heads=self.config.num_heads,\r\n                head_dim_v=self.config.head_dim_v,\r\n                head_dim_qk=self.config.head_dim_qk,\r\n                dropout=self.config.dropout,\r\n                head_decays_range=self.config.head_decays_range,\r\n                activation=self.activation,\r\n                device=self.config.device,\r\n                dtype=self.config.dtype,\r\n            ),\r\n            xpos_embedder=xpos_embedder,\r\n        )\r\n\r\n    def _reset_parameters(self):\r\n        # TODO: Check that we're following the same initialization as the paper\r\n        nn.init.xavier_normal_(self.linear1.weight)\r\n        nn.init.constant_(self.linear1.bias, 0)\r\n        nn.init.xavier_normal_(self.linear2.weight)\r\n        nn.init.constant_(self.linear2.bias, 0)\r\n\r\n    def _feedforward_block(self, x: Tensor) -> Tensor:\r\n        x = self.activation(self.linear1(x))\r\n        x = self.dropout(x)\r\n        x = self.linear2(x)\r\n        x = self.dropout(x)\r\n        return x\r\n\r\n    def forward_chunkwise(\r\n            self, x: Tensor, start_idx: int, prev_state: Optional[Tensor] = None\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        # retention block\r\n        if self.norm_first:\r\n            y, state = self.retention.forward_chunkwise(self.norm1(x), start_idx=start_idx, prev_state=prev_state)\r\n            x = x + y\r\n            x = x + self._feedforward_block(self.norm2(x))\r\n        else:\r\n            y, state = self.retention.forward_chunkwise(x, start_idx=start_idx, prev_state=prev_state)\r\n            x = x + self.norm1(y)\r\n            x = x + self.norm2(self._feedforward_block(x))\r\n\r\n        return x, state\r\n\r\n    def forward_recurrent(\r\n            self, x: Tensor, seq_idx: int, prev_state: Optional[Tensor] = None\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        def _retention_block(x: Tensor) -> Tuple[Tensor, Tensor]:\r\n            x, state = self.retention.forward_recurrent(\r\n                x, seq_idx=seq_idx, prev_state=prev_state\r\n            )\r\n            return self.dropout(x), state\r\n\r\n        # retention block\r\n        if self.norm_first:\r\n            y, state = _retention_block(self.norm1(x))\r\n            x = x + y\r\n            x = x + self._feedforward_block(self.norm2(x))\r\n        else:\r\n            y, state = _retention_block(x)\r\n            x = x + self.norm1(y)\r\n            x = x + self.norm2(self._feedforward_block(x))\r\n\r\n        return x, state\r\n\r\n    def forward(self, x: Tensor, start_idx: int = 0, prev_state: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\r\n        return self.forward_chunkwise(x, start_idx, prev_state)\r\n\r\n\r\nclass RetNetDecoder(nn.Module):\r\n    def __init__(self, layer_config: RetNetDecoderLayer.Config, num_layers: int):\r\n        super().__init__()\r\n        self.layer_config = layer_config\r\n        self.num_layers = num_layers\r\n        self.xpos_embedder = XPos(\r\n            layer_config.head_dim_qk,\r\n            device=layer_config.device,\r\n            dtype=layer_config.dtype,\r\n        )\r\n        self.layers = nn.ModuleList(self._build_layers(num_layers))\r\n\r\n    def _build_layers(self, num_layers: int):\r\n        return [RetNetDecoderLayer(self.layer_config, self.xpos_embedder) for _ in range(num_layers)]\r\n\r\n    def forward_recurrent(\r\n            self, x: Tensor, seq_idx: int, prev_states: Sequence[Optional[Tensor]] = ()\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        if prev_states is None or len(prev_states) == 0:\r\n            prev_states = [None] * self.num_layers\r\n        elif len(prev_states) != len(self.layers):\r\n            raise ValueError(\r\n                f\"Expected {len(self.layers)} previous states, got {len(prev_states)}\"\r\n            )\r\n\r\n        states: List[Tensor] = []\r\n        for layer, prev_state in zip(self.layers, prev_states):\r\n            assert isinstance(layer, RetNetDecoderLayer)\r\n            x, state = layer.forward_recurrent(x, seq_idx, prev_state)\r\n            states.append(state)\r\n        return x, torch.stack(states)\r\n\r\n    def forward_chunkwise(\r\n            self, x: Tensor, start_idx: int = 0, prev_states: Sequence[Optional[Tensor]] = ()\r\n    ) -> Tuple[Tensor, Tensor]:\r\n        if prev_states is None or len(prev_states) == 0:\r\n            prev_states = [None] * self.num_layers\r\n        elif len(prev_states) != len(self.layers):\r\n            raise ValueError(\r\n                f\"Expected {len(self.layers)} previous states, got {len(prev_states)}\"\r\n            )\r\n\r\n        states: List[Tensor] = []\r\n        for layer, prev_state in zip(self.layers, prev_states):\r\n            assert isinstance(layer, RetNetDecoderLayer)\r\n            x, state = layer.forward_chunkwise(x, start_idx, prev_state)\r\n            states.append(state)\r\n        return x, torch.stack(states)\r\n\r\n    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor]:\r\n        return self.forward_chunkwise(x)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/flash_pop/retnet.py b/flash_pop/retnet.py
--- a/flash_pop/retnet.py	(revision 2705a8b9d52d1078b603d59fee09eae5bfc3b3e9)
+++ b/flash_pop/retnet.py	(date 1744219263642)
@@ -190,7 +190,7 @@
             affine=False,
             eps=config.group_norm_eps,
             device=config.device,
-            dtype=config.dtype,
+            dtype=torch.float32,
         )
         # The output project is slightly different, due to the gated "swish" layer.
         self.g_proj = nn.Linear(
@@ -273,15 +273,16 @@
         if retention_kernel is None:
             retention_kernel = self._compute_chunkwise_retention_kernel
         retention, state = retention_kernel(
-            q, k, v, prev_state,
+            q.bfloat16(), k.bfloat16(), v.bfloat16(), prev_state.bfloat16(),
         )
+        retention, state = retention.to(dtype=x.dtype), state.to(dtype=x.dtype)
         # To apply group norm in an equivalent way to the recurrent formulation,
         # we fold the sequence dimension into the batch dimension.  Otherwise,
         # normalization would be applied over the entire input sequence.
         batch_size = retention.size(0)
         retention = rearrange(retention, "b n h d -> (b n) (h d)")
         retention = F.dropout(retention, p=self.dropout, training=self.training)
-        retention = self.group_norm(retention)
+        retention = self.group_norm(retention.float()).to(dtype=x.dtype)
         # Unfold 'n' from the batch dimension, and fold 'h' back into the embed dim.
         retention = rearrange(retention, "(b n) e -> b n e", b=batch_size)
 
@@ -292,8 +293,8 @@
         # The paper describes the gate as:
         #   g = swish(X * W_g)
         # where X is the input to the layer.
-        gate = self.activation(self.g_proj(x))
-        retention = self.out_proj(retention * gate)
+        gate = self.activation(self.g_proj(x).float())
+        retention = self.out_proj((retention.float() * gate).to(dtype=x.dtype))
 
         return retention, state
 
@@ -338,7 +339,7 @@
         retention = F.dropout(retention, p=self.dropout, training=self.training)
         # Fold heads back into the embedding dimension.
         retention = rearrange(retention, "b h d -> b (h d)")
-        retention = self.group_norm(retention)
+        retention = self.group_norm(retention.float()).to(dtype=x.dtype)
 
         # NOTE: Unlike multihead attention, the retention paper applies a "swish"
         # gate to increase the non-linear capacity of the model.  (IMO this is likely
@@ -408,7 +409,7 @@
             d_model,
             eps=config.layer_norm_eps,
             device=config.device,
-            dtype=config.dtype
+            dtype=torch.float32  # crucial for maintaining numerical precision!
         )
         self.retention = self._build_multi_scale_retention(xpos_embedder=xpos_embedder)
         # feedforward block
@@ -416,7 +417,7 @@
             d_model,
             eps=config.layer_norm_eps,
             device=config.device,
-            dtype=config.dtype
+            dtype=torch.float32  # crucial for maintaining numerical precision!
         )
         self.linear1 = nn.Linear(d_model, config.dim_feedforward, device=config.device, dtype=config.dtype)
         self.linear2 = nn.Linear(config.dim_feedforward, d_model, device=config.device, dtype=config.dtype)
@@ -456,14 +457,15 @@
             self, x: Tensor, start_idx: int, prev_state: Optional[Tensor] = None
     ) -> Tuple[Tensor, Tensor]:
         # retention block
+        dtype = self.config.dtype
         if self.norm_first:
-            y, state = self.retention.forward_chunkwise(self.norm1(x), start_idx=start_idx, prev_state=prev_state)
+            y, state = self.retention.forward_chunkwise(self.norm1(x.float()).to(dtype=dtype), start_idx=start_idx, prev_state=prev_state)
             x = x + y
-            x = x + self._feedforward_block(self.norm2(x))
+            x = x + self._feedforward_block(self.norm2(x.float()).to(dtype=dtype))
         else:
             y, state = self.retention.forward_chunkwise(x, start_idx=start_idx, prev_state=prev_state)
-            x = x + self.norm1(y)
-            x = x + self.norm2(self._feedforward_block(x))
+            x = x + self.norm1(y.float()).to(dtype=dtype)
+            x = x + self.norm2(self._feedforward_block(x).float()).to(dtype=dtype)
 
         return x, state
 
Index: flash_pop/pop_retnet.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Optional, Union\r\nfrom dataclasses import dataclass\r\n\r\nimport torch\r\nfrom torch import Tensor\r\nfrom einops import rearrange, einsum, repeat\r\n\r\nfrom flash_pop.xpos_emb import XPos\r\nfrom flash_pop.retnet import MultiScaleRetention, RetNetDecoderLayer, RetNetDecoder\r\nfrom flash_pop.pop_retention import flash_pop_retention\r\n\r\n\r\nclass POPMultiScaleRetention(MultiScaleRetention):\r\n    @dataclass(kw_only=True)\r\n    class Config(MultiScaleRetention.Config):\r\n        block_size: int\r\n\r\n    def __init__(self, config: Config, xpos_embedder: Optional[XPos] = None):\r\n        super().__init__(config, xpos_embedder=xpos_embedder)\r\n        self.config = config\r\n\r\n    def _pop_retention_kernel(\r\n            self,\r\n            q: Tensor,\r\n            k: Tensor,\r\n            v: Tensor,\r\n            prev_state: Tensor,\r\n    ) -> tuple[Tensor, Tensor]:\r\n        retention, states = flash_pop_retention(\r\n            q, k, v, prev_state, self.head_decays, self.config.block_size\r\n        )\r\n        return retention, states\r\n\r\n    def pop_chunkwise(\r\n            self,\r\n            x: Tensor,\r\n            start_index: int,\r\n            prev_state: Optional[Tensor]\r\n    ) -> tuple[Tensor, Tensor]:\r\n        return self._retention_chunkwise(\r\n            x,\r\n            start_index,\r\n            prev_state,\r\n            self._pop_retention_kernel\r\n        )\r\n\r\n\r\nclass POPDecoderLayer(RetNetDecoderLayer):\r\n    @dataclass(kw_only=True)\r\n    class Config(RetNetDecoderLayer.Config):\r\n        block_size: int\r\n\r\n    def __init__(\r\n            self,\r\n            config: Config,\r\n            xpos_embedder: Optional[XPos] = None,\r\n            suffixes_xpos_embedder: Optional[XPos] = None,\r\n    ) -> None:\r\n        super().__init__(config, xpos_embedder)\r\n        self.suffixes_xpos_embedder = suffixes_xpos_embedder\r\n        self.config = config\r\n\r\n    def _build_multi_scale_retention(self, xpos_embedder: Optional[XPos] = None):\r\n        return POPMultiScaleRetention(\r\n            POPMultiScaleRetention.Config(\r\n                block_size=self.config.block_size,\r\n                num_heads=self.config.num_heads,\r\n                head_dim_v=self.config.head_dim_v,\r\n                head_dim_qk=self.config.head_dim_qk,\r\n                dropout=self.config.dropout,\r\n                head_decays_range=self.config.head_decays_range,\r\n                activation=self.activation,\r\n                device=self.config.device,\r\n                dtype=self.config.dtype,\r\n            ),\r\n            xpos_embedder=xpos_embedder,\r\n        )\r\n\r\n    def pop_forward(\r\n            self,\r\n            x: Tensor,\r\n            start_index: int,\r\n            prev_state: Optional[Tensor],\r\n            suffixes: Optional[Tensor] = None,\r\n            suffixes_start_indices: Optional[Tensor] = None,\r\n    ):\r\n        \"\"\"\r\n        Retention chunkwise of `x` with state computations every full 'block'.\r\n        If suffixes are provided, another retention chunkwise is computed in a large batch\r\n        form, starting from the computed states and using the suffixes as inputs.\r\n        :param x: Tensor. shape: (batch_size, seq_len, num_heads * dim_v) where seq_len = N * block_size\r\n        for positive integer N.\r\n        :param start_index:\r\n        :param prev_state:\r\n        :param suffixes: Tensor. shape: (batch_size * N+1, sfx_len, num_heads * dim_v).\r\n        Note that given a sequence `x` of N blocks and a previous state we can predict N+1 blocks!\r\n        :return: If suffixes are provided, their corresponding outputs are also returned. Otherwise,\r\n        the retention outputs of `x` and the states are returned.\r\n        \"\"\"\r\n        assert x.dim() == 3, f\"Got {x.dim()}\"  # b t (h d)\r\n\r\n        if self.norm_first:\r\n            y, states = self.retention.pop_chunkwise(self.norm1(x), start_index=start_index, prev_state=prev_state)\r\n            x = x + y\r\n            x = x + self._feedforward_block(self.norm2(x))\r\n        else:\r\n            y, states = self.retention.pop_chunkwise(x, start_index=start_index, prev_state=prev_state)\r\n            x = x + self.norm1(y)\r\n            x = x + self.norm2(self._feedforward_block(x))\r\n\r\n        if suffixes is not None:\r\n            if self.norm_first:\r\n                suffixes_y, last_state = self._suffixes_forward(\r\n                    self.norm1(suffixes),\r\n                    suffixes_start_indices,\r\n                    prev_state,\r\n                    states,\r\n                    x.size(0),\r\n                    self.suffixes_xpos_embedder\r\n                )\r\n                suffixes = suffixes + suffixes_y\r\n                suffixes = suffixes + self._feedforward_block(self.norm2(suffixes))\r\n            else:\r\n                suffixes_y, last_state = self._suffixes_forward(\r\n                    suffixes,\r\n                    suffixes_start_indices,\r\n                    prev_state,\r\n                    states,\r\n                    x.size(0),\r\n                    self.suffixes_xpos_embedder\r\n                )\r\n                suffixes = suffixes + self.norm1(suffixes_y)\r\n                suffixes = suffixes + self.norm2(self._feedforward_block(suffixes))\r\n\r\n            return x, last_state, suffixes\r\n\r\n        else:\r\n            return x, states, None\r\n\r\n    def _suffixes_forward(self, suffixes, start_indices, prev_state, states, batch_size, xpos_embedder=None):\r\n        assert start_indices is not None and isinstance(start_indices, Tensor)\r\n        assert suffixes.dim() == 3, f\"Got {suffixes.dim()}\"  # (b n) t (h d) where n=num blocks, t is sfx length\r\n        num_blocks = suffixes.size(0) // batch_size\r\n\r\n        assert states.size(1) + 1 == num_blocks, (f\"got {states.size(1) + 1} states != {num_blocks} num_blocks. \"\r\n                                                  f\"make sure there is one additional suffix block.\")\r\n\r\n        if prev_state is None:\r\n            prev_state = torch.zeros_like(states[:, 0:1])\r\n        prev_states = torch.cat((prev_state, states), dim=1).flatten(0, 1)\r\n        suffixes, _ = self.retention._retention_chunkwise(\r\n            suffixes,\r\n            start_idx=start_indices,\r\n            prev_state=prev_states,\r\n            xpos_embedder=self.suffixes_xpos_embedder,\r\n        )\r\n        last_state = states[:, -1].clone()\r\n\r\n        return suffixes, last_state\r\n\r\n\r\n\r\ndef _get_suffixes_start_indices(batch_size, num_blocks, start_index: int, block_size: int, device):\r\n    start_idx = start_index + torch.arange(num_blocks, device=device) * block_size\r\n    start_idx = repeat(start_idx, 'n -> (b n)', b=batch_size)\r\n\r\n    return start_idx\r\n\r\n\r\nclass POPRetNetDecoder(RetNetDecoder):\r\n    def __init__(self, layer_config: POPDecoderLayer.Config, num_layers: int):\r\n        self.suffixes_xpos_embedder = XPos(\r\n            layer_config.head_dim_qk,\r\n            device=layer_config.device,\r\n            dtype=layer_config.dtype,\r\n        )\r\n        super().__init__(layer_config, num_layers)\r\n\r\n\r\n    def _build_layers(self, num_layers: int):\r\n        return [\r\n            POPDecoderLayer(\r\n                self.layer_config,\r\n                xpos_embedder=self.xpos_embedder,\r\n                suffixes_xpos_embedder=self.suffixes_xpos_embedder\r\n            ) for _ in range(num_layers)\r\n        ]\r\n\r\n    def pop_forward(\r\n            self,\r\n            x: Tensor,\r\n            start_idx: int = 0,\r\n            prev_states: Optional[tuple[Tensor, ...]] = (),\r\n            suffixes: Optional[Tensor] = None,\r\n    ) -> tuple[Tensor, Tensor, Optional[Tensor]]:\r\n        \"\"\"\r\n\r\n        :param x: Tensor of shape (batch_size, seq_len, num_heads * dim_v).\r\n        :param start_idx:\r\n        :param prev_states:\r\n        :param suffixes: Tensor of shape (batch_size, num_blocks, sfx_seq_len, num_heads * dim_v).\r\n        :return:\r\n        \"\"\"\r\n        if prev_states is None or len(prev_states) == 0:\r\n            prev_states = [None] * self.num_layers\r\n        elif len(prev_states) != len(self.layers):\r\n            raise ValueError(\r\n                f\"Expected {len(self.layers)} previous states, got {len(prev_states)}\"\r\n            )\r\n\r\n        suffixes_start_indices = None\r\n        if suffixes is not None:\r\n            assert suffixes.dim() == 4, f\"Got {suffixes.dim()}\"\r\n            suffixes_start_indices = _get_suffixes_start_indices(\r\n                x.size(0), suffixes.size(1), start_idx, self.layer_config.block_size, x.device\r\n            )\r\n            suffixes = suffixes.flatten(0, 1)\r\n\r\n        states: list[Tensor] = []\r\n        for layer, prev_state in zip(self.layers, prev_states):\r\n            assert isinstance(layer, POPDecoderLayer)\r\n\r\n            x, state, suffixes = layer.pop_forward(\r\n                x,\r\n                start_idx,\r\n                prev_state,\r\n                suffixes,\r\n                suffixes_start_indices=suffixes_start_indices\r\n            )\r\n            states.append(state)\r\n        if suffixes is not None:\r\n            suffixes = rearrange(suffixes, '(b n) t d -> b n t d', b=x.size(0))\r\n        return x, torch.stack(states), suffixes\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/flash_pop/pop_retnet.py b/flash_pop/pop_retnet.py
--- a/flash_pop/pop_retnet.py	(revision 2705a8b9d52d1078b603d59fee09eae5bfc3b3e9)
+++ b/flash_pop/pop_retnet.py	(date 1744219277001)
@@ -27,8 +27,9 @@
             prev_state: Tensor,
     ) -> tuple[Tensor, Tensor]:
         retention, states = flash_pop_retention(
-            q, k, v, prev_state, self.head_decays, self.config.block_size
+            q.bfloat16(), k.bfloat16(), v.bfloat16(), prev_state.bfloat16(), self.head_decays, self.config.block_size
         )
+        retention, states = retention.to(dtype=v.dtype), states.to(dtype=v.dtype)
         return retention, states
 
     def pop_chunkwise(
@@ -98,20 +99,21 @@
         the retention outputs of `x` and the states are returned.
         """
         assert x.dim() == 3, f"Got {x.dim()}"  # b t (h d)
+        dtype = self.config.dtype
 
         if self.norm_first:
-            y, states = self.retention.pop_chunkwise(self.norm1(x), start_index=start_index, prev_state=prev_state)
+            y, states = self.retention.pop_chunkwise(self.norm1(x.float()).to(dtype=dtype), start_index=start_index, prev_state=prev_state)
             x = x + y
-            x = x + self._feedforward_block(self.norm2(x))
+            x = x + self._feedforward_block(self.norm2(x.float()).to(dtype=dtype))
         else:
             y, states = self.retention.pop_chunkwise(x, start_index=start_index, prev_state=prev_state)
-            x = x + self.norm1(y)
-            x = x + self.norm2(self._feedforward_block(x))
+            x = x + self.norm1(y.float()).to(dtype=dtype)
+            x = x + self.norm2(self._feedforward_block(x).float()).to(dtype=dtype)
 
         if suffixes is not None:
             if self.norm_first:
                 suffixes_y, last_state = self._suffixes_forward(
-                    self.norm1(suffixes),
+                    self.norm1(suffixes.float()).to(dtype=dtype),
                     suffixes_start_indices,
                     prev_state,
                     states,
@@ -119,7 +121,7 @@
                     self.suffixes_xpos_embedder
                 )
                 suffixes = suffixes + suffixes_y
-                suffixes = suffixes + self._feedforward_block(self.norm2(suffixes))
+                suffixes = suffixes + self._feedforward_block(self.norm2(suffixes.float()).to(dtype=dtype))
             else:
                 suffixes_y, last_state = self._suffixes_forward(
                     suffixes,
@@ -129,8 +131,8 @@
                     x.size(0),
                     self.suffixes_xpos_embedder
                 )
-                suffixes = suffixes + self.norm1(suffixes_y)
-                suffixes = suffixes + self.norm2(self._feedforward_block(suffixes))
+                suffixes = suffixes + self.norm1(suffixes_y.float()).to(dtype=dtype)
+                suffixes = suffixes + self.norm2(self._feedforward_block(suffixes).float()).to(dtype=dtype)
 
             return x, last_state, suffixes
 
Index: flash_pop/xpos_emb.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from typing import Union, Optional\r\n\r\nfrom functools import lru_cache\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch import Tensor\r\n\r\nfrom einops import rearrange, repeat\r\n\r\n\r\n@lru_cache(maxsize=1)\r\ndef _build_position_thetas(\r\n    head_dim: int,\r\n    scale: float = 10000,\r\n    device: Optional[Union[torch.device, str]] = None,\r\n    dtype: Optional[torch.dtype] = None,\r\n) -> Tensor:\r\n    \"\"\"Positional thetas are different for each value along head_dim, following the\r\n    prescribed method in the paper.  These are used to update the positional\r\n    embeddings in both the parallel and recurrent formulations of retention.\r\n    See: https://arxiv.org/pdf/2307.08621v3.pdf, Section 2.1 (Retention)\r\n\r\n    NOTE: The actual values for thetas are not specified in the paper, so I\r\n    copied these values from the official implementation.\r\n    See: https://github.com/microsoft/torchscale/blob/7d231743f4f96c460b7cf0aa0cf242bb192b34f8/torchscale/architecture/retnet.py#L27C1-L28C59\r\n    \"\"\"\r\n    x = torch.linspace(0, 1, steps=head_dim // 2, device=device)\r\n    thetas = 1 / (scale**x)\r\n    return repeat(thetas.to(dtype=dtype), \"d -> (d n)\", n=2)\r\n\r\n\r\n@torch.compile()\r\ndef _multiply_by_i(x: Tensor) -> Tensor:\r\n    \"\"\"Multiply a complex-valued tensor by the imaginary unit 'i'.\"\"\"\r\n    return torch.stack((-x[..., 1::2], x[..., ::2]), dim=-1).flatten(start_dim=-2)\r\n\r\n\r\n@torch.compile()\r\ndef _theta_shift(x: Tensor, sin: Tensor, cos: Tensor) -> Tensor:\r\n    return (x * cos) + (_multiply_by_i(x) * sin)\r\n\r\n\r\n@torch.compile()\r\ndef _get_sin_cos(\r\n        seq_len: int,\r\n        start_idx: Union[int, torch.Tensor],\r\n        thetas: Tensor,\r\n) -> tuple[Tensor, Tensor]:\r\n    device, dtype = thetas.device, thetas.dtype\r\n    indices = torch.arange(seq_len, device=device)\r\n\r\n    if isinstance(start_idx, int):\r\n        # Combined (cross + intra chunk):\r\n        indices = start_idx + indices\r\n        indices = indices.reshape(1, -1, 1, 1)\r\n\r\n    elif isinstance(start_idx, torch.Tensor):\r\n        # designed for the training phase of POP, where we flatten suffixes into the batch dimension\r\n        # here we assume that start_idx has the (final) batch dimension\r\n        # start_idx entries determine the offsets of individual batch entries.\r\n        assert start_idx.dim() == 1\r\n        indices = start_idx.view(-1, 1) + indices.view(1, -1)\r\n        indices = indices.reshape(start_idx.shape[0], indices.shape[1], 1, 1)\r\n\r\n    else:\r\n        assert False, f\"Unsupported type for start_index. Expected int or LongTensor, got '{type(start_idx)}'.\"\r\n\r\n    thetas = thetas.reshape(1, 1, 1, -1)\r\n    angles = indices * thetas.float()\r\n    sin = torch.sin(angles).to(dtype=dtype)\r\n    cos = torch.cos(angles).to(dtype=dtype)\r\n\r\n    return sin, cos\r\n\r\n\r\nclass XPos:\r\n\r\n    def __init__(\r\n            self,\r\n            head_dim_qk: int,\r\n            seq_len_estimate: int = 2 ** 12,\r\n            start_idx: Union[int, torch.Tensor] = 0,\r\n            device=None,\r\n            dtype=None,\r\n    ) -> None:\r\n        self.cos_cache = None\r\n        self.sin_cache = None\r\n        self.start_idx = start_idx\r\n\r\n        # 'thetas' parameter for updating the relative position embeddings.\r\n        self.thetas = _build_position_thetas(\r\n            head_dim=head_dim_qk, device=device, dtype=dtype\r\n        )\r\n        # self.register_buffer(\"thetas\", self.thetas)\r\n\r\n        self.sin_cache, self.cos_cache = _get_sin_cos(\r\n            seq_len=seq_len_estimate,\r\n            start_idx=start_idx,\r\n            thetas=self.thetas,\r\n        )\r\n        # self.register_buffer(\"cos_cache\", self.cos_cache)\r\n        # self.register_buffer(\"sin_cache\", self.sin_cache)\r\n\r\n        self.seq_len = self.sin_cache.shape[1]\r\n\r\n    def get_sin_cos(self, seq_len, start_idx):\r\n        if isinstance(start_idx, int):\r\n            if start_idx + seq_len > self.seq_len:\r\n                self.seq_len = start_idx + seq_len\r\n                self.sin_cache, self.cos_cache = _get_sin_cos(self.seq_len, 0, self.thetas)\r\n            end_idx = start_idx + seq_len\r\n            return self.sin_cache[:, start_idx:end_idx], self.cos_cache[:, start_idx:end_idx]\r\n        else:\r\n            assert isinstance(start_idx, torch.Tensor)\r\n            if not isinstance(self.start_idx, Tensor) or start_idx.numel() != self.start_idx.numel() or torch.any(start_idx != self.start_idx):\r\n                self.sin_cache, self.cos_cache = _get_sin_cos(seq_len, start_idx, self.thetas)\r\n                self.start_idx = start_idx\r\n            return self.sin_cache, self.cos_cache\r\n\r\n    def __call__(self, q, k, start_idx: Union[int, torch.Tensor], *args, **kwargs):\r\n        dtype = q.dtype\r\n        sin, cos = self.get_sin_cos(seq_len=q.size(1), start_idx=start_idx)\r\n        q = _theta_shift(q, sin, cos).to(dtype=dtype)\r\n        k = _theta_shift(k, sin, cos).to(dtype=dtype)\r\n\r\n        return q, k\r\n\r\n\r\n\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/flash_pop/xpos_emb.py b/flash_pop/xpos_emb.py
--- a/flash_pop/xpos_emb.py	(revision 2705a8b9d52d1078b603d59fee09eae5bfc3b3e9)
+++ b/flash_pop/xpos_emb.py	(date 1744212499844)
@@ -89,7 +89,7 @@
 
         # 'thetas' parameter for updating the relative position embeddings.
         self.thetas = _build_position_thetas(
-            head_dim=head_dim_qk, device=device, dtype=dtype
+            head_dim=head_dim_qk, device=device, dtype=torch.float32
         )
         # self.register_buffer("thetas", self.thetas)
 
@@ -120,8 +120,8 @@
     def __call__(self, q, k, start_idx: Union[int, torch.Tensor], *args, **kwargs):
         dtype = q.dtype
         sin, cos = self.get_sin_cos(seq_len=q.size(1), start_idx=start_idx)
-        q = _theta_shift(q, sin, cos).to(dtype=dtype)
-        k = _theta_shift(k, sin, cos).to(dtype=dtype)
+        q = _theta_shift(q.float(), sin, cos).to(dtype=dtype)
+        k = _theta_shift(k.float(), sin, cos).to(dtype=dtype)
 
         return q, k
 
