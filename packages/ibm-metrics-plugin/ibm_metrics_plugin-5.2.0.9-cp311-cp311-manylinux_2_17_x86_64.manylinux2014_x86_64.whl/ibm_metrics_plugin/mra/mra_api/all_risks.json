[
    {
        "id": "rec06EWwgC0LwKWVV",
        "tag": "toxic-output",
        "title": "Toxic output",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "value alignment",
        "descriptor": "specific",
        "description": "Toxic output occurs when the model produces hateful, abusive, and profane (HAP) or obscene content. This also includes behaviors like bullying.",
        "concern": "Hateful, abusive, and profane (HAP) or obscene content can adversely impact and harm people interacting with the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec1TGsCr5ZaeYtWD",
        "tag": "data-poisoning",
        "title": "Data poisoning",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "robustness",
        "descriptor": "traditional",
        "description": "A type of adversarial attack where an adversary or malicious insider injects intentionally corrupted, false, misleading, or incorrect samples into the training or fine-tuning datasets.",
        "concern": "Poisoning data can make the model sensitive to a malicious data pattern and produce the adversary’s desired output. It can create a security risk where adversaries can force model behavior for their own benefit.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec2UgpRVbzuhmC4O",
        "tag": "unreliable-source-attribution",
        "title": "Unreliable source attribution",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "explainability",
        "descriptor": "specific",
        "description": "Source attribution is the AI system's ability to describe from what training data it generated a portion or all its output. Since current techniques are based on approximations, these attributions might be incorrect.",
        "concern": "Low-quality attributions make it difficult for users, model validators, and auditors to understand and trust the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T18:18:39Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec3jAzou3HYjC2h7",
        "tag": "harmful-output",
        "title": "Harmful output",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "value alignment",
        "descriptor": "specific",
        "description": "A model might generate language that leads to physical harm The language might include overtly violent, covertly dangerous, or otherwise indirectly unsafe statements.",
        "concern": "A model generating harmful output can cause immediate physical harm or create prejudices that might lead to future harm.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec4pXNvyDbd8gCfC",
        "tag": "confidential-information-in-data",
        "title": "Confidential information in data",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "intellectual property",
        "descriptor": "amplified",
        "description": "Confidential information might be included as part of the data that is used to train or tune the model.",
        "concern": "If confidential data is not properly protected, there could be an unwanted disclosure of confidential information. The model might expose confidential information in the generated output or to unauthorized users.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:19:57Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec5PVx0S9AozZ41u",
        "tag": "unrepresentative-data",
        "title": "Unrepresentative data",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "accuracy",
        "descriptor": "traditional",
        "description": "Unrepresentative data occurs when the training or fine-tuning data is not sufficiently representative of the underlying population or does not measure the phenomenon of interest.",
        "concern": "If the data is not representative, then the model will not work as intended.",
        "creation_date": "2024-09-24T15:24:52Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec5pkocbdqgD5BoM",
        "tag": "lack-of-model-transparency",
        "title": "Lack of model transparency",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "traditional",
        "description": "Lack of model transparency is due to insufficient documentation of the model design, development, and evaluation process and the absence of insights into the inner workings of the model.",
        "concern": "Transparency is important for legal compliance, AI ethics, and guiding appropriate use of models. Missing information might make it more difficult to evaluate risks,  change the model, or reuse it.  Knowledge about who built a model can also be an important factor in deciding whether to trust it. Additionally, transparency regarding how the model’s risks were determined, evaluated, and mitigated also play a role in determining model risks, identifying model suitability, and governing model usage.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:29:31Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec67vGdjPt0Urd9D",
        "tag": "personal-information-in-prompt",
        "title": "Personal information in prompt",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "privacy",
        "descriptor": "specific",
        "description": "Personal information or sensitive personal information that is included as a part of a prompt that is sent to the model.",
        "concern": "If personal information or sensitive personal information is included in the prompt, it might be unintentionally disclosed in the models’ output. In addition to accidental disclosure, prompt data might be stored or later used for other purposes like model evaluation and retraining, and might appear in their output if not properly removed. ",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:19:42Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec6s1VL8tgoJ6ls3",
        "tag": "impact-on-human-agency",
        "title": "Impact on human agency",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "amplified",
        "description": "AI might affect the individuals’ ability to make choices and act independently in their best interests.",
        "concern": "AI can generate false or misleading information that looks real.  It may simplify the ability of nefarious actors to generate realistically looking false or misleading content with intention to manipulate human thoughts and behavior. When false or misleading content that is generated by AI is spread, people might not recognize it as false information leading to a distorted understanding of the truth. People might experience reduced agency when exposed to false or misleading information since they may use false assumptions in their decision process.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec8BkLPI7jzVH8qe",
        "tag": "exposing-personal-information",
        "title": "Exposing personal information",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "privacy",
        "descriptor": "amplified",
        "description": "When personal identifiable information (PII) or sensitive personal information (SPI) are used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing personal information is a type of data leakage.",
        "concern": "Sharing people’s PI impacts their rights and make them more vulnerable.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:21:50Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec8dnQs4FfJv0t8X",
        "tag": "nonconsensual-use",
        "title": "Nonconsensual use",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "misuse",
        "descriptor": "amplified",
        "description": "Generative AI models might be intentionally used to imitate people through deepfakes by using video, images, audio, or other modalities without their consent.",
        "concern": "Deepfakes can spread disinformation about a person, possibly resulting in a negative impact on the person’s reputation. A model that has this potential must be properly governed.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec8fOMm7N0zY3qbR",
        "tag": "decision-bias",
        "title": "Decision bias",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "fairness",
        "descriptor": "traditional",
        "description": "Decision bias occurs when one group is unfairly advantaged over another due to decisions of the model. This might be caused by biases in the data and also amplified as a result of the model’s training.",
        "concern": "Bias can harm persons affected by the decisions of the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rec8heqKHk9QOOWhC",
        "tag": "lack-of-testing-diversity",
        "title": "Lack of testing diversity",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "amplified",
        "description": "AI model risks are socio-technical, so their testing needs input from a broad set of disciplines and diverse testing practices.",
        "concern": "Without diversity and the relevant experience, an organization might not correctly or completely identify and test for AI risks.",
        "creation_date": "2024-09-24T17:33:14Z",
        "last_update_date": "2024-09-24T17:35:42Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recA3wWXB5dW1mZKI",
        "tag": "data-privacy-rights",
        "title": "Data privacy rights alignment",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "privacy",
        "descriptor": "amplified",
        "description": "Existing laws could include providing data subject rights such as opt-out, right to access, and right to be forgotten.",
        "concern": "Improper usage or a request for data removal could force organizations to retrain the model, which is expensive.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recAR5M3933aWcxwi",
        "tag": "prompt-leaking",
        "title": "Prompt leaking",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "robustness",
        "descriptor": "specific",
        "description": "A prompt leak attack attempts to extract a model's system prompt (also known as the system message).",
        "concern": "A successful attack copies the system prompt used in the model. Depending on the content of that prompt, the attacker might gain access to valuable information, such as sensitive personal information or intellectual property, and might be able to replicate some of the functionality of the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:24:42Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recAfS0bbE000JxFW",
        "tag": "ip-information-in-prompt",
        "title": "IP information in prompt",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "intellectual property",
        "descriptor": "specific",
        "description": "Copyrighted information or other intellectual property might be included as a part of the prompt that is sent to the model.",
        "concern": "Inclusion of such data might result in it being disclosed in the model output. In addition to accidental disclosure, prompt data might be used for other purposes like model evaluation and retraining, and might appear in their output if not properly removed.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recB3qKqPrtam4ikz",
        "tag": "hallucination",
        "title": "Hallucination",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "robustness",
        "descriptor": "specific",
        "description": "Hallucinations generate factually inaccurate or untruthful content with respect to the model’s training data or input. This is also sometimes referred to lack of faithfulness or lack of groundedness.",
        "concern": "Hallucinations can be misleading. These false outputs can mislead users and be incorporated into downstream artifacts, further spreading misinformation. False output can harm both owners and users of the AI models. In some uses, hallucinations can be particularly consequential.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recBThvliJQcDwHaf",
        "tag": "legal-accountability",
        "title": "Legal accountability",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "legal compliance",
        "descriptor": "amplified",
        "description": "Determining who is responsible for an AI model is challenging without good documentation and governance processes.",
        "concern": "If ownership for development of the model is uncertain, regulators and others might have concerns about the model. It would not be clear who would be liable and responsible for the problems with it or can answer questions about it. Users of models without clear ownership might find challenges with compliance with future AI regulation.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recDW7oAjYGGojpXT",
        "tag": "data-transparency",
        "title": "Lack of training data transparency",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "transparency",
        "descriptor": "amplified",
        "description": "Without accurate documentation on how a model's data was collected, curated, and used to train a model, it might be harder to satisfactorily explain the behavior of the model with respect to the data.",
        "concern": "A lack of data documentation limits the ability to evaluate risks associated with the data. Having access to the training data is not enough. Without recording how the data was cleaned, modified, or generated, the model behavior is more difficult to understand and to fix. Lack of data transparency also impacts model reuse as it is difficult to determine data representativeness for the new use without such documentation.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recDYGHESlXc65EwK",
        "tag": "model-usage-rights",
        "title": "Model usage rights restrictions",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "legal compliance",
        "descriptor": "traditional",
        "description": "Terms of service, licenses, or other rules restrict the use of certain models.",
        "concern": "Laws and regulations that concern the use of AI are in place and vary from country to country. Additionally, the usage of models might be dictated by licensing terms or agreements.",
        "creation_date": "2024-09-24T17:47:20Z",
        "last_update_date": "2024-11-18T03:30:46Z",
        "created_by_user": null,
        "access_groups": []
    },
    {
        "id": "recDnhzIrxSwPo7lM",
        "tag": "non-disclosure",
        "title": "Non-disclosure",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "misuse",
        "descriptor": "specific",
        "description": "Content might not be clearly disclosed as AI generated.",
        "concern": "Users must be notified when they are interacting with an AI system. Not disclosing the AI-authored content can result in a lack of transparency.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:57:12Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recFezEmaVz1CoCyB",
        "tag": "prompt-injection",
        "title": "Prompt injection attack",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "robustness",
        "descriptor": "specific",
        "description": "A prompt injection attack forces a generative model that takes a prompt as input to produce unexpected output by manipulating the structure, instructions, or information contained in its prompt.",
        "concern": "Injection attacks can be used to alter model behavior and benefit the attacker.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T19:24:15Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recFjlZDy87b81A61",
        "tag": "incomplete-advice",
        "title": "Incomplete advice",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "value alignment",
        "descriptor": "specific",
        "description": "When a model provides advice without having enough information, resulting in possible harm if the advice is followed.",
        "concern": "A person might act on incomplete advice or worry about a situation that is not applicable to them due to the overgeneralized nature of the content generated. For example, a model might provide incorrect medical, financial, and legal advice or recommendations that the end user might act on, resulting in harmful actions.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recGP7TDqEbIsQxzy",
        "tag": "data-usage",
        "title": "Data usage restrictions",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "data laws",
        "descriptor": "traditional",
        "description": "Laws and other restrictions can limit or prohibit the use of some data for specific AI use cases.",
        "concern": "Data usage restrictions can impact the availability of the data required for training an AI model and can lead to poorly represented data.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:29:04Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recGghXH7fVFw4uq3",
        "tag": "lack-of-system-transparency",
        "title": "Lack of system transparency",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "traditional",
        "description": "Insufficient documentation of the system that uses the model and the model’s purpose within the system in which it is used.",
        "concern": "A lack of documentation makes it difficult to understand how the model’s outcomes contribute to the system’s or application’s functionality.",
        "creation_date": "2024-09-24T17:32:01Z",
        "last_update_date": "2024-11-18T03:29:22Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recHlgNl0VVWj7yf7",
        "tag": "impact-on-cultural-diversity",
        "title": "Impact on cultural diversity",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "specific",
        "description": "AI systems might overly represent certain cultures that result in a homogenization of culture and thoughts.",
        "concern": "Underrepresented groups' languages, viewpoints, and institutions might be suppressed by that means reducing diversity of thought and culture.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T19:28:59Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recIRDjOlaUjvTqjr",
        "tag": "plagiarism",
        "title": "Impact on education: plagiarism",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "specific",
        "description": "Easy access to high-quality generative models might result in students that use AI models to plagiarize existing work intentionally or unintentionally.",
        "concern": "AI models can be used to claim the authorship or originality of works that were created by other people in doing so by engaging in plagiarism. Claiming others’ work as your own is both unethical and often illegal.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:53:20Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recJP6JsJLkB0N9fB",
        "tag": "improper-usage",
        "title": "Improper usage",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "misuse",
        "descriptor": "amplified",
        "description": "Improper usage occurs when a model is used for a purpose that it was not originally designed for.",
        "concern": "Reusing a model without understanding its original data, design intent, and goals might result in unexpected and unwanted model behaviors.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recJtEIGjp0QQRipZ",
        "tag": "personal-information-in-data",
        "title": "Personal information in data",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "privacy",
        "descriptor": "traditional",
        "description": "Inclusion or presence of personal identifiable information (PII) and sensitive personal information (SPI) in the data used for training or fine tuning the model might result in unwanted disclosure of that information.",
        "concern": "If not properly developed to protect sensitive data, the model might expose personal information in the generated output.  Additionally, personal, or sensitive data must be reviewed and handled in accordance with privacy laws and regulations.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:20:22Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recLFLLanHGwb5BK1",
        "tag": "jailbreaking",
        "title": "Jailbreaking",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "multi-category",
        "descriptor": "specific",
        "description": "A jailbreaking attack attempts to break through the guardrails that are established in the model to perform restricted actions.",
        "concern": "Jailbreaking attacks can be used to alter model behavior and benefit the attacker. If not properly controlled, business entities can face fines, reputational harm, and other legal consequences.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:26:29Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recLfs8ZhShkHjxIK",
        "tag": "extraction-attack",
        "title": "Extraction attack",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "robustness",
        "descriptor": "amplified",
        "description": "An extraction attack attempts to copy or steal an AI model by appropriately sampling the input space and observing outputs to build a surrogate model that behaves similarly.",
        "concern": "With a successful extraction attack, the attacker can perform further adversarial attacks to gain valuable information such as sensitive personal information or intellectual property.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2025-01-08T16:22:34Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recLkWh7uNAzR2Vnp",
        "tag": "job-loss",
        "title": "Impact on Jobs",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "amplified",
        "description": "Widespread adoption of foundation model-based AI systems might lead to people's job loss as their work is automated if they are not reskilled.",
        "concern": "Job loss might lead to a loss of income and thus might negatively impact the society and human welfare. Reskilling might be challenging given the pace of the technology evolution.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:50:50Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recMeLwnicT2vPpvl",
        "tag": "data-aquisition",
        "title": "Data acquisition restrictions",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "data laws",
        "descriptor": "amplified",
        "description": "Laws and other regulations might limit the collection of certain types of data for specific AI use cases.",
        "concern": "There are several ways of collecting data for building a foundation models: web scraping, web crawling, crowdsourcing, and curating public datasets. Data acquisition restrictions can also impact the availability of the data that is required for training an AI model and can lead to poorly represented data.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:30:05Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recP91zThJQSrOSus",
        "tag": "prompt-priming",
        "title": "Prompt priming",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "multi-category",
        "descriptor": "specific",
        "description": "Because generative models tend to produce output like the input provided, the model can be prompted to reveal specific kinds of information. For example, adding personal information in the prompt increases its likelihood of generating similar kinds of personal information in its output. If personal data was included as part of the model’s training, there is a possibility it could be revealed.",
        "concern": "Jailbreaking attacks can be used to alter model behavior and benefit the attacker. ",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:26:26Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recQ2j65fkuGb8XCc",
        "tag": "reidentification",
        "title": "Reidentification",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "privacy",
        "descriptor": "traditional",
        "description": "Even with the removal or personal identifiable information (PII) and sensitive personal information (SPI) from data, it might be possible to identify persons due to correlations to other features available in the data.",
        "concern": "Including irrelevant but highly correlated features to personal information for model training can increase the risk of reidentification.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recS54iZ8Nomp4XK8",
        "tag": "attribute-inference-attack",
        "title": "Attribute inference attack",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "privacy",
        "descriptor": "amplified",
        "description": "An attribute inference attack repeatedly queries a model to detect whether certain sensitive features can be inferred about individuals who participated in training a model. These attacks occur when an adversary has some prior knowledge about the training data and uses that knowledge to infer the sensitive data.",
        "concern": "With a successful attack, the attacker can gain valuable information such as sensitive personal information or intellectual property.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recTbP5f2QN2HTw8O",
        "tag": "poor-model-accuracy",
        "title": "Poor model accuracy",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "accuracy",
        "descriptor": "amplified",
        "description": "Poor model accuracy occurs when a model’s performance is insufficient to the task it was designed for. Low accuracy might occur if the model is not correctly engineered, or there are changes to the model’s expected inputs.",
        "concern": "Inadequate model performance can adversely affect end users and downstream systems that are relying on correct output. In cases where model output is consequential, this might result in societal, reputational, or financial harm.",
        "creation_date": "2024-09-24T17:26:46Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recU85em58cBeL5yH",
        "tag": "data-transfer",
        "title": "Data transfer restrictions",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "data laws",
        "descriptor": "traditional",
        "description": "Laws and other restrictions can limit or prohibit transferring data.",
        "concern": "Data transfer restrictions can also impact the availability of the data that is required for training an AI model and can lead to poorly represented data.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recUcDObrpjKDwOo6",
        "tag": "generated-content-ownership",
        "title": "Generated content ownership and IP",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "legal compliance",
        "descriptor": "specific",
        "description": "Legal uncertainty about the ownership and intellectual property rights of AI-generated content.",
        "concern": "Laws and regulations that relate to the ownership of AI-generated content are largely unsettled and can vary from country to country. Not being able to identify the owner of an AI-generated content might negatively impact AI-supported creative tasks.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recWMOZFMxO3LxWSm",
        "tag": "output-bias",
        "title": "Output bias",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "fairness",
        "descriptor": "specific",
        "description": "Generated content might unfairly represent certain groups or individuals.",
        "concern": "Bias can harm users of the AI models and magnify existing discriminatory behaviors.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recWQLnjTkiAzNEIy",
        "tag": "dangerous-use",
        "title": "Dangerous use",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "misuse",
        "descriptor": "specific",
        "description": "Generative AI models might be used with the sole intention of harming people.",
        "concern": "Large language models are often trained on vast amounts of publicly-available information that may include information on harming others. A model that has this potential must be carefully evaluated for such content and properly governed.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recX57W5grEGCdjw9",
        "tag": "unexplainable-output",
        "title": "Unexplainable output",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "explainability",
        "descriptor": "amplified",
        "description": "Explanations for model output decisions might be difficult, imprecise, or not possible to obtain.",
        "concern": "Foundation models are based on complex deep learning architectures, making explanations for their outputs difficult. Inaccessible training data could limit the types of explanations a model can provide. Without clear explanations for model output, it is difficult for users, model validators, and auditors to understand and trust the model. Wrong explanations might lead to over-trust.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recXBsNK7nSnfkwLB",
        "tag": "human-exploitation",
        "title": "Human exploitation",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "amplified",
        "description": "When workers who train AI models such as ghost workers are not provided with adequate working conditions, fair compensation, and good health care benefits that also include mental health.",
        "concern": "Foundation models still depend on human labor to source, manage, and program the data that is used to train the model. Human exploitation for these activities might negatively impact the society and human welfare. ",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:51:04Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recaXCfwlxyrHMVdK",
        "tag": "data-curation",
        "title": "Improper data curation",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "value alignment",
        "descriptor": "amplified",
        "description": "Improper collection and preparation of training or tuning data includes data label errors and by using data with conflicting information or misinformation.",
        "concern": "Improper data curation can adversely affect how a model is trained, resulting in a model that does not behave in accordance with the intended values. Correcting problems after the model is trained and deployed might be insufficient for guaranteeing proper behavior. ",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recbUeRhKYbzfh9cZ",
        "tag": "revealing-confidential-information",
        "title": "Revealing confidential information",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "intellectual property",
        "descriptor": "amplified",
        "description": "When confidential information is used in training data, fine-tuning data, or as part of the prompt, models might reveal that data in the generated output. Revealing confidential information is a type of data leakage.",
        "concern": "If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output and reveal information that was meant to be secret.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:20:00Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "reccCdEg6MYiGPWDk",
        "tag": "spreading-disinformation",
        "title": "Spreading disinformation",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "misuse",
        "descriptor": "specific",
        "description": "Generative AI models might be used to intentionally create misleading or false information to deceive or influence a targeted audience.",
        "concern": "Spreading disinformation might affect human’s ability to make informed decisions. A model that has this potential must be properly governed.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recdUtph9LqBgpIxW",
        "tag": "unrepresentative-risk-testing",
        "title": "Unrepresentative risk testing",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "amplified",
        "description": "Testing is unrepresentative when the test inputs are mismatched with the inputs that are expected during deployment.",
        "concern": "If the model is evaluated in a use, context, or setting that is not the same as the one expected for deployment, the evaluations might not accurately reflect the risks of the model.",
        "creation_date": "2024-09-24T17:33:12Z",
        "last_update_date": "2024-09-24T17:35:03Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recddHU17uuEbdHEs",
        "tag": "data-bias",
        "title": "Data bias",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "fairness",
        "descriptor": "amplified",
        "description": "Historical and societal biases that are present in the data are used to train and fine-tune the model.",
        "concern": "Training an AI system on data with bias, such as historical or societal bias, can lead to biased or skewed outputs that can unfairly represent or otherwise discriminate against certain groups or individuals.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recdh1pfpqFpnVmLU",
        "tag": "data-provenance",
        "title": "Uncertain data provenance",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "transparency",
        "descriptor": "amplified",
        "description": "Data provenance refers to tracing history of data, which includes its ownership, origin, and transformations. Without standardized and established methods for verifying where the data came from, there are no guarantees that the data is the same as the original source and has the correct usage terms.",
        "concern": "Not all data sources are trustworthy. Data might be unethically collected, manipulated, or falsified. Verifying that data provenance is challenging due to factors such as data volume, data complexity, data source varieties, and poor data management. Using such data can result in undesirable behaviors in the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recgjP2PkJZobtvsW",
        "tag": "data-usage-rights",
        "title": "Data usage rights restrictions",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "intellectual property",
        "descriptor": "amplified",
        "description": "Terms of service, license compliance, or other IP issues may restrict the ability to use certain data for building models.",
        "concern": "Laws and regulations concerning the use of data to train AI are unsettled and can vary from country to country, which creates challenges in the development of models.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:29:58Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "reciYzvHUJGD3JT9W",
        "tag": "harmful-code-generation",
        "title": "Harmful code generation",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "harmful code generation",
        "descriptor": "specific",
        "description": "Models might generate code that causes harm or unintentionally affects other systems.",
        "concern": "The execution of harmful code might open vulnerabilities in IT systems.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recii8HHOAnzMPPAZ",
        "tag": "data-contamination",
        "title": "Data contamination",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "accuracy",
        "descriptor": "amplified",
        "description": "Data contamination occurs when incorrect data is used for training. For example, data that is not aligned with model’s purpose or data that is already set aside for other development tasks such as testing and evaluation.",
        "concern": "Data that differs from the intended training data might skew model accuracy and affect model outcomes.",
        "creation_date": "2024-09-24T15:26:57Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recjRtaUSIuhf1Bgh",
        "tag": "incomplete-usage-definition",
        "title": "Incomplete usage definition",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "specific",
        "description": "Since foundation models can be used for many purposes, a model’s intended use is important for defining the relevant risks of that model. As the use changes, the relevant risks might correspondingly change.",
        "concern": "It might be difficult to accurately determine and mitigate the relevant risks for a model when its intended use is insufficiently specified. Such as how a model is going to be used, where it is going to be used and what it is going to be used for.",
        "creation_date": "2024-09-24T17:33:16Z",
        "last_update_date": "2024-09-25T18:14:40Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "reckU2g3xOSfIX1fw",
        "tag": "lack-of-data-transparency",
        "title": "Lack of data transparency",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "amplified",
        "description": "Lack of data transparency is due to insufficient documentation of training or tuning dataset details. ",
        "concern": "Transparency is important for legal compliance and AI ethics. Information on the collection and preparation of training data, including how it was labeled and by who are necessary to understand model behavior and suitability. Details about how the data risks were determined, measured, and mitigated are important for evaluating both data and model trustworthiness. Missing details about the data might make it more difficult to evaluate representational harms, data ownership, provenance, and other data-oriented risks. The lack of standardized requirements might limit disclosure as organizations protect trade secrets and try to limit others from copying their models.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-11-18T03:29:27Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "reckx3aQ9Q9Q3mDyJ",
        "tag": "copyright-infringement",
        "title": "Copyright infringement",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "intellectual property",
        "descriptor": "specific",
        "description": "A model might generate content that is similar or identical to existing work protected by copyright or covered by open-source license agreement.",
        "concern": "Laws and regulations concerning the use of content that looks the same or closely similar to other copyrighted data are largely unsettled and can vary from country to country, providing challenges in determining and implementing compliance.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:47:05Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recmkrt4GWtO1FSau",
        "tag": "impact-on-affected-communities",
        "title": "Impact on affected communities",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "traditional",
        "description": "It is important to include the perspectives or concerns of communities that are affected by model outcomes when designing and building models. Failing to include these perspectives makes it difficult to understand the relevant context for the model and to engender trust within these communities.",
        "concern": "Failing to engage with communities that are affected by a model’s outcomes might result in harms to those communities and societal backlash.",
        "creation_date": "2024-09-24T17:49:36Z",
        "last_update_date": "2024-09-24T17:52:24Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recn7FcNrI8Py4QoX",
        "tag": "spreading-toxicity",
        "title": "Spreading toxicity",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "misuse",
        "descriptor": "specific",
        "description": "Generative AI models might be used intentionally to generate hateful, abusive, and profane (HAP) or obscene content.",
        "concern": "Toxic content might negatively affect the well-being of its recipients. A model that has this potential must be properly governed.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recnX97h27OCQl4Yn",
        "tag": "downstream-retraining",
        "title": "Improper retraining",
        "type": "input",
        "phase": "training-tuning",
        "category": "",
        "group": "value alignment",
        "descriptor": "amplified",
        "description": "Using undesirable output (for example, inaccurate, inappropriate, and user content) for retraining purposes can result in unexpected model behavior.",
        "concern": "Repurposing generated output for retraining a model without implementing proper human vetting increases the chances of undesirable outputs to be incorporated into the training or tuning data of the model. In turn, this model can generate even more undesirable output.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T15:12:56Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recrZBFYxzSuXFj0y",
        "tag": "inaccessible-training-data",
        "title": "Inaccessible training data",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "explainability",
        "descriptor": "amplified",
        "description": "Without access to the training data, the types of explanations a model can provide are limited and more likely to be incorrect.",
        "concern": "Low quality explanations without source data make it difficult for users, model validators, and auditors to understand and trust the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-05-03T17:23:41Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recrayIjLZ8ojrmqN",
        "tag": "bypassing-learning",
        "title": "Impact on education: bypassing learning",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "specific",
        "description": "Easy access to high-quality generative models might result in students that use AI models to bypass the learning process.",
        "concern": "AI models are quick to find solutions or solve complex problems. These systems can be misused by students to bypass the learning process. The ease of access to these models results in students having a superficial understanding of concepts and hampers further education that might rely on understanding those concepts.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:53:16Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recsJZiKhk9OBSVuI",
        "tag": "untraceable-attribution",
        "title": "Untraceable attribution",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "explainability",
        "descriptor": "amplified",
        "description": "The content of the training data used for generating the model’s output is not accessible.",
        "concern": "Without the ability to access training data content, the possibility of using source attribution techniques can be severely limited or impossible. This makes it difficult for users, model validators, and auditors to understand and trust the model.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T18:26:32Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "rectXfNOXEP3HqMkE",
        "tag": "evasion-attack",
        "title": "Evasion attack",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "robustness",
        "descriptor": "amplified",
        "description": "Evasion attacks attempt to make a model output incorrect results by slightly perturbing the input data that is sent to the trained model.",
        "concern": "Evasion attacks alter model behavior, usually to benefit the attacker.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:25:09Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recuMkKGWjfU5s92G",
        "tag": "impact-on-the-environment",
        "title": "Impact on the environment",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "societal impact",
        "descriptor": "amplified",
        "description": "AI, and large generative models in particular, might produce increased carbon emissions and increase water usage for their training and operation.",
        "concern": "Training and operating large AI models, building data centers, and manufacturing specialized hardware for AI can consume large amounts of water and energy, which contributes to carbon emissions. Additionally, water resources that are used for cooling AI data center servers can no longer be allocated for other necessary uses. If not managed, these could exacerbate climate change. ",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T19:27:45Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recvU6z6mxr8RfBKI",
        "tag": "incorrect-risk-testing",
        "title": "Incorrect risk testing",
        "type": "non-technical",
        "phase": null,
        "category": "",
        "group": "governance",
        "descriptor": "amplified",
        "description": "A metric selected to measure or track a risk is incorrectly selected, incompletely measuring the risk, or measuring the wrong risk for the given context.",
        "concern": "If the metrics do not measure the risk as intended, then the understanding of that risk will be incorrect and mitigations might not be applied. If the model’s output is consequential, this might result in societal, reputational, or financial harm.",
        "creation_date": "2024-09-24T17:33:11Z",
        "last_update_date": "2024-09-24T17:34:12Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recvWmkB4hRuc2qB7",
        "tag": "over-or-under-reliance",
        "title": "Over- or Under-reliance",
        "type": "output",
        "phase": null,
        "category": "",
        "group": "value alignment",
        "descriptor": "amplified",
        "description": "In AI-assisted decision-making tasks, reliance measures how much a person trusts (and potentially acts on) a model’s output. Over-reliance occurs when a person puts too much trust in a model, accepting a model’s output when the model’s output is likely incorrect. Under-reliance is the opposite, where the person doesn’t trust the model but should.",
        "concern": "In tasks where humans make choices based on AI-based suggestions, over/under reliance can lead to poor decision making because of the misplaced trust in the AI system, with negative consequences that increase with the importance of the decision.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "recw5GfO2ZZchlI47",
        "tag": "membership-inference-attack",
        "title": "Membership inference attack",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "privacy",
        "descriptor": "amplified",
        "description": "A membership inference attack repeatedly queries a model to determine whether a given input was part of the model’s training. More specifically, given a trained model and a data sample, an attacker samples the input space, observing outputs to deduce whether that sample was part of the model's training. ",
        "concern": "Identifying whether a data sample was used for training data can reveal what data was used to train a model. Possibly giving competitors insight into how a model was trained and the opportunity to replicate the model or tamper with it. Models that include publicly-available data are at higher risk of such attacks.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-10-22T17:33:01Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    },
    {
        "id": "reczKTPlB3slAd0ef",
        "tag": "confidential-data-in-prompt",
        "title": "Confidential data in prompt",
        "type": "input",
        "phase": "inference",
        "category": "",
        "group": "intellectual property",
        "descriptor": "specific",
        "description": "Confidential information might be included as a part of the prompt that is sent to the model.",
        "concern": "If not properly developed to secure confidential data, the model might reveal confidential information or IP in the generated output. Additionally, end users' confidential information might be unintentionally collected and stored.",
        "creation_date": "2024-03-06T16:33:08Z",
        "last_update_date": "2024-09-24T17:23:39Z",
        "created_by_user": "default_user",
        "access_groups": [
            "DEFAULT"
        ]
    }
]
