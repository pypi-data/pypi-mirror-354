import argparse
import json
import os
import pickle
import shutil

import litdata
import numpy as np
import polars as pl

import deeptan.constants as const
from deeptan.utils.data import DeepTANDataModule, read_nmic_npz


def deeptan_litdata():
    parser = argparse.ArgumentParser(description="Optimize data for Deeptan model")
    parser.add_argument("--labels", type=str, default="", help="Path to label data in .parquet format")
    parser.add_argument("--bs", type=int, default=const.default.bs, help="Batch size for training")
    parser.add_argument("--trn_npz", type=str, required=True, help="Path to training data in .npz format (generated by mi2graph)")
    parser.add_argument("--val_parquet", type=str, required=True, help="Path to validation data in .parquet format")
    parser.add_argument("--tst_parquet", type=str, required=True, help="Path to test data in .parquet format")
    parser.add_argument("--output_dir", type=str, default=".tmp_data_optimized", help="Directory for logging")
    parser.add_argument("--thre_mi", type=float, default=const.default.threshold_nmic, help="Threshold for edge attribute")
    parser.add_argument("--in_feat", type=str, default="", help="Path to a .csv with header, containing a list of features to specify. If None, all features are used")
    parser.add_argument("--in_obs", type=str, default="", help="")
    parser.add_argument("--onlytest", action="store_true", help="Only run the test phase")
    parser.add_argument("--n_workers", type=int, default=const.default.n_threads, help="Number of workers for data loading")
    args = parser.parse_args()

    if len(args.labels) < 2:
        labels = None
    else:
        labels = args.labels

    if len(args.in_feat) < 2:
        specify_features = None
    else:
        specify_features = args.in_feat

    if len(args.in_obs) < 2:
        specify_trn_obs = None
    else:
        specify_trn_obs = args.in_obs

    files_fit = {
        const.dkey.abbr_train: args.trn_npz,
        const.dkey.abbr_val: args.val_parquet,
        const.dkey.abbr_test: args.tst_parquet,
    }
    datamodule = DeepTANDataModule(
        files_fit,
        labels,
        batch_size=args.bs,
        edge_attr_threshold=args.thre_mi,
        specify_features=specify_features,
    )
    datamodule.setup()

    # Copy original training data to output directory for saving node_names and g_label_dim
    # shutil.copy(args.trn_npz, os.path.join(args.output_dir, "trn.npz"))
    others2save = {
        "dict_node_names": datamodule.dict_node_names,
        "output_g_label_dim": datamodule.label_dim,
    }
    os.makedirs(args.output_dir, exist_ok=True)
    # Save as json
    with open(os.path.join(args.output_dir, const.fname.litdata_others2save_json), "w") as f:
        json.dump(others2save, f)
    # Save as pickle
    with open(os.path.join(args.output_dir, const.fname.litdata_others2save_pkl), "wb") as f:
        pickle.dump(others2save, f)

    if labels is not None:
        shutil.copy(labels, os.path.join(args.output_dir, const.fname.label_class_onehot))

    # Check obs_names filter
    if specify_trn_obs is not None:
        _edge_attr, _edge_index, _mat, _mat_feat_indices, _obs_names, _node_names = read_nmic_npz(args.trn_npz)
        _obs_names_goal = pl.read_parquet(specify_trn_obs)["obs_names"].to_list()
        # Get intersection of _obs_names and _obs_names_goal
        _obs_names_filtered = list(set(_obs_names) & set(_obs_names_goal))
        # Get available indices in _obs_names for the following litdata getting indices
        _obs_names_indices = np.where(np.isin(_obs_names, _obs_names_filtered))[0]
        _trn_indices = _obs_names_indices.tolist()
    else:
        _trn_indices = list(range(datamodule.train.len()))

    # Optimize
    if not args.onlytest:
        litdata.optimize(
            fn=datamodule.train.get,
            inputs=_trn_indices,
            output_dir=os.path.join(args.output_dir, const.dkey.abbr_train),
            chunk_bytes=const.default.lit_chunk_bytes,
            compression=const.default.lit_compression,
            num_workers=min(args.n_workers, const.default.n_threads),
        )
        litdata.optimize(
            fn=datamodule.val.get,
            inputs=list(range(datamodule.val.len())),
            output_dir=os.path.join(args.output_dir, const.dkey.abbr_val),
            chunk_bytes=const.default.lit_chunk_bytes,
            compression=const.default.lit_compression,
            num_workers=min(args.n_workers, const.default.n_threads),
        )
    litdata.optimize(
        fn=datamodule.test.get,
        inputs=list(range(datamodule.test.len())),
        output_dir=os.path.join(args.output_dir, const.dkey.abbr_test),
        chunk_bytes=const.default.lit_chunk_bytes,
        compression=const.default.lit_compression,
        num_workers=min(args.n_workers, const.default.n_threads),
    )
