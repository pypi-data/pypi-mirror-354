{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125eefaa-65a3-4caf-b0e7-1d2a3dcf3bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent-kafka in /opt/conda/lib/python3.11/site-packages (2.8.2)\n",
      "Requirement already satisfied: autogen in /opt/conda/lib/python3.11/site-packages (0.8.1)\n",
      "Requirement already satisfied: pyautogen==0.8.1 in /opt/conda/lib/python3.11/site-packages (from autogen) (0.8.1)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (0.0.8)\n",
      "Requirement already satisfied: diskcache in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (5.6.3)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (7.1.0)\n",
      "Requirement already satisfied: fast-depends<3,>=2.4.12 in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (2.4.12)\n",
      "Requirement already satisfied: httpx<1,>=0.28.1 in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (0.28.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=2.6.1 in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (2.10.6)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (1.0.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (2.5.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.11/site-packages (from pyautogen==0.8.1->autogen) (0.9.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.4.0 in /opt/conda/lib/python3.11/site-packages (from asyncer==0.0.8->pyautogen==0.8.1->autogen) (4.0.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.28.1->pyautogen==0.8.1->autogen) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.28.1->pyautogen==0.8.1->autogen) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.28.1->pyautogen==0.8.1->autogen) (3.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen==0.8.1->autogen) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.1->autogen) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.1->autogen) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=2.6.1->pyautogen==0.8.1->autogen) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from docker->pyautogen==0.8.1->autogen) (2.31.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from docker->pyautogen==0.8.1->autogen) (2.0.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken->pyautogen==0.8.1->autogen) (2024.11.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen==0.8.1->autogen) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->docker->pyautogen==0.8.1->autogen) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install confluent-kafka autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8306c10a-63eb-40ef-8e0f-0a1db574531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import Annotated\n",
    "\n",
    "from autogen import (\n",
    "    AssistantAgent,\n",
    "    UserProxyAgent,\n",
    "    register_function,\n",
    ")\n",
    "from autogen.cache import Cache\n",
    "\n",
    "from confluent_kafka.admin import AdminClient\n",
    "from confluent_kafka import Consumer, TopicPartition\n",
    "\n",
    "\n",
    "config_list = [\n",
    "    {\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]},\n",
    "]\n",
    "\n",
    "broker = \"redpanda:9092\"\n",
    "\n",
    "\n",
    "def list_kafka_topics() -> Annotated[list, \"The name of tables in the system\"]:\n",
    "    \"\"\"List topics from a Kafka broker using confluent-kafka.\"\"\"\n",
    "    admin_client = AdminClient({'bootstrap.servers': broker})\n",
    "    metadata = admin_client.list_topics(timeout=5)\n",
    "    return list(metadata.topics.keys())\n",
    "\n",
    "def get_topic_details(topic: Annotated[str, \"the name of the topic\"]\n",
    "                     )-> Annotated[dict, \"the information of the topic\"]:\n",
    "    \"\"\"Fetch partitions and replication details for a topic.\"\"\"\n",
    "    admin_client = AdminClient({'bootstrap.servers': broker})\n",
    "    metadata = admin_client.list_topics(topic, timeout=5)\n",
    "\n",
    "    if topic not in metadata.topics:\n",
    "        return f\"Topic '{topic}' not found\"\n",
    "\n",
    "    topic_info = metadata.topics[topic]\n",
    "    return {p.id: {\"replicas\": p.replicas} for p in topic_info.partitions.values()}\n",
    "\n",
    "\n",
    "def get_topic_offsets(topic: Annotated[str, \"the name of the topic\"]\n",
    "                     )-> Annotated[dict, \"the json object with earliest and latest offset\"]:\n",
    "    \"\"\"Retrieve earliest and latest offsets for each partition.\"\"\"\n",
    "    consumer = Consumer({'bootstrap.servers': broker, 'group.id': 'offset_checker', 'auto.offset.reset': 'earliest'})\n",
    "\n",
    "    metadata = consumer.list_topics(topic, timeout=5)\n",
    "    partitions = [p.id for p in metadata.topics[topic].partitions.values()]\n",
    "\n",
    "    offsets = {}\n",
    "    for partition in partitions:\n",
    "        tp = TopicPartition(topic, partition)\n",
    "        low, high = consumer.get_watermark_offsets(tp)\n",
    "        offsets[partition] = {\"earliest\": low, \"latest\": high}\n",
    "\n",
    "    consumer.close()\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def get_latest_message(\n",
    "    topic: Annotated[str, \"the name of the topic\"]\n",
    ") -> Annotated[str, \"the latest event in string format\"]:\n",
    "    # Generate a unique consumer group ID\n",
    "    random_group_id = f\"consumer-{uuid.uuid4()}\"  \n",
    "    conf = {\n",
    "        'bootstrap.servers': broker,\n",
    "        'group.id': random_group_id,  # Use a unique group ID to avoid offset tracking\n",
    "        'auto.offset.reset': 'latest',  # Start from the latest message\n",
    "    }\n",
    "\n",
    "    consumer = Consumer(conf)\n",
    "\n",
    "    # Get partition info\n",
    "    metadata = consumer.list_topics(topic, timeout=5)\n",
    "    partitions = [p.id for p in metadata.topics[topic].partitions.values()]\n",
    "\n",
    "    # Find the partition with the latest message\n",
    "    latest_msg = None\n",
    "    latest_offset = -1\n",
    "\n",
    "    for partition in partitions:\n",
    "        # Get the latest offset for the partition\n",
    "        low, high = consumer.get_watermark_offsets(TopicPartition(topic, partition))\n",
    "        last_offset = high - 1  # Last available message offset\n",
    "\n",
    "        if last_offset < 0:\n",
    "            continue  # No messages in this partition\n",
    "\n",
    "        # Seek to the last offset\n",
    "        tp = TopicPartition(topic, partition, last_offset)\n",
    "        consumer.assign([tp])\n",
    "        consumer.seek(tp)\n",
    "\n",
    "        # Poll for the last message\n",
    "        msg = consumer.poll(timeout=2.0)\n",
    "        if msg and not msg.error():\n",
    "            latest_msg = msg.value().decode('utf-8')\n",
    "            latest_offset = last_offset\n",
    "\n",
    "    consumer.close()\n",
    "\n",
    "    return latest_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "647b6238-f951-4a33-a9cf-0c013d86fd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__consumer_offsets',\n",
       " 'debezium_config',\n",
       " 'debezium_status',\n",
       " 'postgres.public.credit_history',\n",
       " 'postgres.public.customers',\n",
       " 'mongodb.lumi_data.unstructured_data',\n",
       " 'debezium_offset']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_kafka_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35952879-7b89-48b6-b5d0-9012972ccb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latest_message('debezium_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bdce8e90-ae2e-4274-93bf-392a1b3b2ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this ReAct prompt is adapted from Langchain's ReAct agent: https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/agents/react/agent.py#L79\n",
    "ReAct_prompt = \"\"\"\n",
    "You are a asistent help explore Apachy Kafka Broker based on input questions.\n",
    "\n",
    "You have access to tools provided.\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this process can repeat multiple times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "# Define the ReAct prompt message. Assuming a \"question\" field is present in the context\n",
    "\n",
    "\n",
    "def react_prompt_message(sender, recipient, context):\n",
    "    return ReAct_prompt.format(input=context[\"question\"])\n",
    "\n",
    "\n",
    "class KafkaExplorerAgent:\n",
    "    def __init__(self):\n",
    "        self.user_proxy = UserProxyAgent(\n",
    "            name=\"User\",\n",
    "            is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "            human_input_mode=\"NEVER\",\n",
    "            max_consecutive_auto_reply=10,\n",
    "        )\n",
    "\n",
    "        self.assistant = AssistantAgent(\n",
    "            name=\"Assistant\",\n",
    "            system_message=(\n",
    "                \"Only use the tools you have been provided with. \"\n",
    "                \"When you have determined the Final Answer to the question, \"\n",
    "                \"provide it in the format 'Final Answer: [your answer]' and \"\n",
    "                \"then end your message with the word TERMINATE. \"\n",
    "                \"Do not continue the conversation after providing the Final Answer.\"\n",
    "            ),\n",
    "            llm_config={\"config_list\": config_list, \"cache_seed\": None},\n",
    "        )\n",
    "\n",
    "        # Register the timeplus tool.\n",
    "        register_function(\n",
    "            list_kafka_topics,\n",
    "            caller=self.assistant,\n",
    "            executor=self.user_proxy,\n",
    "            name=\"list_kafka_topics\",\n",
    "            description=\"list available kafka topics in the system\",\n",
    "        )\n",
    "\n",
    "        register_function(\n",
    "            get_latest_message,\n",
    "            caller=self.assistant,\n",
    "            executor=self.user_proxy,\n",
    "            name=\"get_latest_message\",\n",
    "            description=\"return the latest messages from kafka topic\",\n",
    "        )\n",
    "\n",
    "        register_function(\n",
    "            get_topic_details,\n",
    "            caller=self.assistant,\n",
    "            executor=self.user_proxy,\n",
    "            name=\"get_topic_details\",\n",
    "            description=\"return the detailed information of a kafka topic\",\n",
    "        )\n",
    "\n",
    "        register_function(\n",
    "            get_topic_offsets,\n",
    "            caller=self.assistant,\n",
    "            executor=self.user_proxy,\n",
    "            name=\"get_topic_offsets\",\n",
    "            description=\"return the latest and earliest offset of a kafka topic\",\n",
    "        )\n",
    "\n",
    "    def ask(self, question: str):\n",
    "        with Cache.disk(cache_seed=43) as cache:\n",
    "            self.user_proxy.initiate_chat(\n",
    "                self.assistant,\n",
    "                message=react_prompt_message,\n",
    "                question=question,\n",
    "                cache=cache,\n",
    "            )\n",
    "\n",
    "        # Get the conversation history\n",
    "        chat_history = self.user_proxy.chat_messages[self.assistant]\n",
    "        # Look through all messages for the \"Final Answer:\" pattern\n",
    "        for message in reversed(chat_history):  # Start from the most recent\n",
    "            content = message.get(\"content\", \"\")\n",
    "            if \"Final Answer:\" in content:\n",
    "                final_answer = content.split(\"Final Answer:\")[1].strip()\n",
    "                return final_answer\n",
    "                \n",
    "        # If no \"Final Answer:\" pattern is found\n",
    "        return \"No final answer found in the conversation.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e563d16-8064-47b8-824a-47d12b0f1f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "You are a asistent help explore Apachy Kafka Broker based on input questions.\n",
      "\n",
      "You have access to tools provided.\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this process can repeat multiple times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "Question: how many messages are in topic topics postgres.public.credit_history\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_suSvOIPWSVNDLKTEBvsQb6C7): get_topic_offsets *****\u001b[0m\n",
      "Arguments: \n",
      "{\"topic\":\"postgres.public.credit_history\"}\n",
      "\u001b[32m**********************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION get_topic_offsets...\n",
      "Call ID: call_suSvOIPWSVNDLKTEBvsQb6C7\n",
      "Input arguments: {'topic': 'postgres.public.credit_history'}\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_suSvOIPWSVNDLKTEBvsQb6C7) *****\u001b[0m\n",
      "{0: {'earliest': 0, 'latest': 220}}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "Thought: The offset provides the number of messages in the topic. The difference between the latest and earliest offset indicates the number of messages in the topic.\n",
      "\n",
      "Final Answer: There are 220 messages in the topic \"postgres.public.credit_history\". TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "the answer is There are 220 messages in the topic \"postgres.public.credit_history\". TERMINATE\n"
     ]
    }
   ],
   "source": [
    "agent = KafkaExplorerAgent()\n",
    "result = agent.ask('how many messages are in topic topics postgres.public.credit_history')\n",
    "print(f'the answer is {result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
