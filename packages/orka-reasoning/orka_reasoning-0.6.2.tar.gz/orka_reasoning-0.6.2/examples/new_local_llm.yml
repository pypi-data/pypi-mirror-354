orchestrator:
  id: orka-ui
  strategy: parallel
  queue: orka:generated
  agents:
    - answer_17
    - duckduckgo_18
    - fork_0
    - join_3
    - eval_4
    - fork_5
    - join_8
    - eval_9
    - fork_10
    - join_13
    - eval_14
    - answer_16
    - answer_16
agents:
  - id: answer_17
    type: openai-answer
    queue: orka:answer_17
    prompt: "based on the input build a sentence to introduce in an internet search engine.INPUT: {{ input }} **constrains**ONLY RETURN THE SEARCH SENTENCE. not other wording around"
  - id: duckduckgo_18
    type: duckduckgo
    queue: orka:duckduckgo_18
    prompt: "{{ previous_outputs.answer_17 }}"
    depends_on:
      - answer_17
  - id: fork_0
    type: fork
    targets:
      - - local_llm_1
      - - gpt4o_2
    depends_on:
      - duckduckgo_18
  - id: local_llm_1
    type: local_llm
    queue: orka:local_llm_1
    prompt: "Clean and structure this research document for analysis. Remove formatting artifacts, standardize section headers, and return a clear markdown version: {{ input }}Use also search data from internet:{{ previous_outputs['duckduckgo_18'] }}"
    model: deepseek-r1:7b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7
    depends_on:
      - fork_0
  - id: gpt4o_2
    type: openai-answer
    queue: orka:gpt4o_2
    prompt: "Clean and structure this research document for analysis. Remove formatting artifacts, standardize section headers, and return a clear markdown version: {{ input }}Use also search data from internet:{{ previous_outputs['duckduckgo_18'] }}"
    model: gpt-4o
    temperature: 0.7
    depends_on:
      - fork_0
  - id: join_3
    type: join
    group: fork_0
  - id: eval_4
    type: openai-classification
    queue: orka:eval_4
    prompt: 'Two agents processed the same research document. Evaluate which output is better in terms of clarity, formatting, and readiness for analysis. Respond with "deepseek" or "gpt4o" based on which version you prefer. Use this structure: --- deepseek_output: {{ previous_outputs[''local_llm_1''] }} gpt4o_output: {{ previous_outputs[''gpt4o_2''] }} ---'
    options:
      - deepseek
      - gpt-4o-mini
    model: gpt-3.5-turbo
    depends_on:
      - join_3
  - id: fork_5
    type: fork
    targets:
      - - local_llm_6
      - - gpt4o_7
    depends_on:
      - eval_4
  - id: local_llm_6
    type: local_llm
    queue: orka:local_llm_6
    prompt: "Provide a comprehensive 3-paragraph summary of this research paper, focusing on objectives, methods, and conclusions: {{ input }}Use also search data from internet:{{ previous_outputs['duckduckgo_18'] }}"
    model: deepseek-r1:7b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7
    depends_on:
      - fork_5
  - id: gpt4o_7
    type: openai-answer
    queue: orka:gpt4o_7
    prompt: "Provide a comprehensive 3-paragraph summary of this research paper, focusing on objectives, methods, and conclusions: {{ input }}Use also search data from internet:{{ previous_outputs['duckduckgo_18'] }}"
    model: gpt-4o
    temperature: 0.7
    depends_on:
      - fork_5
  - id: join_8
    type: join
    group: fork_5
  - id: eval_9
    type: openai-classification
    queue: orka:eval_9
    prompt: 'Compare two summaries of a research paper. Which one is clearer, more informative, and better structured? Return "deepseek" or "gpt4o". --- deepseek_summary: {{ previous_outputs[''local_llm_6''] }} gpt4o_summary: {{ previous_outputs[''gpt4o_7''] }} ---'
    options:
      - deepseek
      - gpt-4o-mini
    model: gpt-3.5-turbo
    depends_on:
      - join_8
  - id: fork_10
    type: fork
    targets:
      - - local_llm_11
      - - gpt4o_12
    depends_on:
      - eval_9
  - id: local_llm_11
    type: local_llm
    queue: orka:local_llm_11
    prompt: "Generate constructive feedback for the author of this research document, focusing on clarity, scientific rigor, and formatting: {{ input }}Use also search data from internet:{{ previous_outputs['duckduckgo_18'] }}"
    model: deepseek-r1:7b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7
    depends_on:
      - fork_10
  - id: gpt4o_12
    type: openai-answer
    queue: orka:gpt4o_12
    prompt: "Generate constructive feedback for the author of this research document, focusing on clarity, scientific rigor, and formatting: {{ input }}Use also search data from internet:{{ previous_outputs['duckduckgo_18'] }}"
    model: gpt-4o
    temperature: 0.7
    depends_on:
      - fork_10
  - id: join_13
    type: join
    group: fork_10
  - id: eval_14
    type: openai-classification
    queue: orka:eval_14
    prompt: 'Which feedback is more useful, specific, and actionable for improving the research paper? Output "deepseek" or "gpt4o". --- deepseek_feedback: {{ previous_outputs[''local_llm_11''] }} gpt4o_feedback: {{ previous_outputs[''gpt4o_12''] }} ---'
    options:
      - deepseek
      - gpt-4o-mini
    model: gpt-3.5-turbo
    depends_on:
      - join_13
  - id: answer_16
    type: openai-answer
    queue: orka:answer_16
    prompt: "Based on three evaluations (cleaning, summarization, feedback), determine the overall better model: DeepSeek or GPT-4o. Count wins, explain why each was chosen, and declare the overall winner.Inputs: - eval_1: {{ previous_outputs['eval_4'] }} - eval_2: {{ previous_outputs['eval_9'] }} - eval_3: {{ previous_outputs['eval_14'] }}Also compare consistency of answer between 2 models across all the executions: {{ previous_outputs}}"
    depends_on:
      - eval_14
  - id: answer_16
    type: openai-answer
    queue: orka:answer_16
    prompt: "Based on three evaluations (cleaning, summarization, feedback), determine the overall better model: DeepSeek or GPT-4o. Count wins, explain why each was chosen, and declare the overall winner.Inputs: - eval_1: {{ previous_outputs['eval_4'] }} - eval_2: {{ previous_outputs['eval_9'] }} - eval_3: {{ previous_outputs['eval_14'] }}Also compare consistency of answer between 2 models across all the executions: {{ previous_outputs}}"
    depends_on:
      - eval_14
